---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

Setup & Import

```{r}
library(tidyverse)
library(lme4)
library(lmerTest)
library(sjPlot)
library(data.table)
library(dplyr)
library(plyr)
library(tidyr)
library(ggplot2)
library(stringr)
library(readr)
library(psych)
library(performance)
library(corrr)
library(purrr)
library(corrplot)
library(multilevel)
library(emmeans)
library(tibble)

data <- read.csv("otree_cleaned1.csv")
data_old <- read.csv("flow_reports_exp1.csv")
```

Data quality check

```{r}
# Datenqualitäts-Check für unaufmerksame Antworten
# ================================================================================

# 1. Straight-Lining Detection (immer gleiche Antwort)
# ================================================================================

# Funktion zur Berechnung der Antwort-Variabilität
calculate_response_variability <- function(row_data) {
  # Nur numerische Spalten nehmen
  numeric_data <- row_data[sapply(row_data, is.numeric)]
  
  # Standardabweichung der Antworten pro Person
  sd_responses <- sd(unlist(numeric_data), na.rm = TRUE)
  
  # Anzahl unterschiedlicher Werte
  n_unique <- length(unique(unlist(numeric_data)))
  
  print(list(sd = sd_responses, n_unique = n_unique))
}

# Für Flow-Items (FSS)
fss_columns <- names(data)[grep("fss\\d+", names(data))]

if(length(fss_columns) > 0) {
  fss_variability <- data %>%
    select(participant.code, all_of(fss_columns)) %>%
    group_by(participant.code) %>%
    summarise(
      n_fss_items = sum(!is.na(across(all_of(fss_columns)))),
      fss_sd = sd(c_across(all_of(fss_columns)), na.rm = TRUE),
      fss_n_unique = n_distinct(c_across(all_of(fss_columns)), na.rm = TRUE),
      fss_straightline = fss_n_unique == 1  # Nur eine Antwortoption verwendet
    )
  
  print("Teilnehmer mit Straight-Lining bei Flow-Items:")
  print(filter(fss_variability, fss_straightline))
}

# 2. Inkonsistenz-Prüfung
# ================================================================================

# Beispiel: Prüfen ob Flow Proneness Items konsistent beantwortet wurden
# (Item 1 ist invers kodiert in jeder Dimension)
if(all(c("Outro.1.player.fpw1", "Outro.1.player.fpw2") %in% names(data))) {
  consistency_check <- data %>%
    select(participant.code, 
           fpw1 = Outro.1.player.fpw1, fpw2 = Outro.1.player.fpw2,
           fph1 = Outro.1.player.fph1, fph2 = Outro.1.player.fph2,
           fpl1 = Outro.1.player.fpl1, fpl2 = Outro.1.player.fpl2) %>%
    mutate(
      # Inverse Items umkehren für Konsistenzprüfung
      fpw1_rev = 6 - fpw1,
      fph1_rev = 6 - fph1,
      fpl1_rev = 6 - fpl1,
      
      # Große Differenzen zwischen verwandten Items?
      fpw_diff = abs(fpw1_rev - fpw2),
      fph_diff = abs(fph1_rev - fph2),
      fpl_diff = abs(fpl1_rev - fpl2),
      
      max_diff = pmax(fpw_diff, fph_diff, fpl_diff, na.rm = TRUE),
      inconsistent = max_diff >= 4  # Differenz von 4+ auf 5-Punkt Skala
    )
  
  print("\nTeilnehmer mit inkonsistenten Antworten:")
  print(filter(consistency_check, inconsistent))
}
```

Descriptive statistics

```{r}
# Demografische Übersicht
demographics <- data %>%
  summarise(
    # Geschlecht
    female_pct = mean(Intro.1.player.gender == "Female", na.rm = TRUE) * 100,
    male_pct = mean(Intro.1.player.gender == "Male", na.rm = TRUE) * 100,
    # Alter
    age_mean = mean(Intro.1.player.age, na.rm = TRUE),
    age_sd = sd(Intro.1.player.age, na.rm = TRUE),
    # Händigkeit
    right_handed_pct = mean(Intro.1.player.dominant_hand == "Right", na.rm = TRUE) * 100
  )

print(round(demographics, 1))

# Englischkenntnisse detailliert
cat("\nEnglischkenntnisse:\n")
english_table <- prop.table(table(data$Intro.1.player.english)) * 100
print(round(english_table, 1))

# Occupation und Field of Study - nur Top 5
cat("\nTop 5 Occupations:\n")
head(sort(table(data$Intro.1.player.occupation), decreasing = TRUE), 5)

cat("\nTop 5 Fields of Study:\n")
head(sort(table(data$Intro.1.player.field_of_study), decreasing = TRUE), 5)
```

Manipulation checks for team goal, team member independence, and ability to coordinate work

```{r}
# mathChat - Interdependence
int_mathChat <- data %>%
  select(mathChat.6.player.int1, mathChat.6.player.int2, mathChat.6.player.int3)
alpha(int_mathChat, check.keys=TRUE)

# mathChat - Common Goal
cg_mathChat <- data %>%
  select(mathChat.6.player.cg1, mathChat.6.player.cg2, mathChat.6.player.cg3, 
         mathChat.6.player.cg4, mathChat.6.player.cg5, mathChat.6.player.cg6)
alpha(cg_mathChat, check.keys=TRUE)

# mathChat - Means for Coordination
mc_mathChat <- data %>%
  select(mathChat.6.player.mc1, mathChat.6.player.mc2)
alpha(mc_mathChat, check.keys=TRUE) 

# mathJitsi - Interdependence
int_mathJitsi <- data %>%
  select(mathJitsi.6.player.int1, mathJitsi.6.player.int2, mathJitsi.6.player.int3)
alpha(int_mathJitsi, check.keys=TRUE)

# mathJitsi - Common Goal
cg_mathJitsi <- data %>%
  select(mathJitsi.6.player.cg1, mathJitsi.6.player.cg2, mathJitsi.6.player.cg3, 
         mathJitsi.6.player.cg4, mathJitsi.6.player.cg5, mathJitsi.6.player.cg6)
alpha(cg_mathJitsi, check.keys=TRUE)

# mathJitsi - Means for Coordination
mc_mathJitsi <- data %>%
  select(mathJitsi.6.player.mc1, mathJitsi.6.player.mc2)
alpha(mc_mathJitsi, check.keys=TRUE) 

# HiddenProfile_Chat - Interdependence
int_HiddenProfile_Chat <- data %>%
  select(HiddenProfile_Chat.3.player.int1, HiddenProfile_Chat.3.player.int2, HiddenProfile_Chat.3.player.int3)
alpha(int_HiddenProfile_Chat, check.keys=TRUE)

# HiddenProfile_Chat - Common Goal
cg_HiddenProfile_Chat <- data %>%
  select(HiddenProfile_Chat.3.player.cg1, HiddenProfile_Chat.3.player.cg2, HiddenProfile_Chat.3.player.cg3, 
         HiddenProfile_Chat.3.player.cg4, HiddenProfile_Chat.3.player.cg5, HiddenProfile_Chat.3.player.cg6)
alpha(cg_HiddenProfile_Chat, check.keys=TRUE)

# HiddenProfile_Chat - Means for Coordination
mc_HiddenProfile_Chat <- data %>%
  select(HiddenProfile_Chat.3.player.mc1, HiddenProfile_Chat.3.player.mc2)
alpha(mc_HiddenProfile_Chat, check.keys=TRUE) 

# HiddenProfile_Jitsi - Interdependence
int_HiddenProfile_Jitsi <- data %>%
  select(HiddenProfile_Jitsi.3.player.int1, HiddenProfile_Jitsi.3.player.int2, HiddenProfile_Jitsi.3.player.int3)
alpha(int_HiddenProfile_Jitsi, check.keys=TRUE)

# HiddenProfile_Jitsi - Common Goal
cg_HiddenProfile_Jitsi <- data %>%
  select(HiddenProfile_Jitsi.3.player.cg1, HiddenProfile_Jitsi.3.player.cg2, HiddenProfile_Jitsi.3.player.cg3, 
         HiddenProfile_Jitsi.3.player.cg4, HiddenProfile_Jitsi.3.player.cg5, HiddenProfile_Jitsi.3.player.cg6)
alpha(cg_HiddenProfile_Jitsi, check.keys=TRUE)

# HiddenProfile_Jitsi - Means for Coordination
mc_HiddenProfile_Jitsi <- data %>%
  select(HiddenProfile_Jitsi.3.player.mc1, HiddenProfile_Jitsi.3.player.mc2)
alpha(mc_HiddenProfile_Jitsi, check.keys=TRUE) 

# Items umpolen (nur die mit negativem Vorzeichen aus der Alpha-Analyse)
data <- data %>%
  mutate(
    # mathChat Items umpolen:
    mathChat.6.player.int1_rev = 8 - mathChat.6.player.int1,
    mathChat.6.player.cg1_rev = 8 - mathChat.6.player.cg1,
    mathChat.6.player.cg3_rev = 8 - mathChat.6.player.cg3,
    mathChat.6.player.cg5_rev = 8 - mathChat.6.player.cg5,
    mathChat.6.player.mc1_rev = 8 - mathChat.6.player.mc1
  )

# Skalenmittelwerte je Konstrukt pro Treatment
data <- data %>%
  mutate(
    mc_mathChat = rowMeans(select(., mathChat.6.player.mc1_rev, mathChat.6.player.mc2), na.rm = TRUE),
    mc_mathJitsi = rowMeans(select(., mathJitsi.6.player.mc1, mathJitsi.6.player.mc2), na.rm = TRUE),
    mc_hpChat = rowMeans(select(., HiddenProfile_Chat.3.player.mc1, HiddenProfile_Chat.3.player.mc2), na.rm = TRUE),
    mc_hpJitsi = rowMeans(select(., HiddenProfile_Jitsi.3.player.mc1, HiddenProfile_Jitsi.3.player.mc2), na.rm = TRUE),

    int_mathChat = rowMeans(select(., mathChat.6.player.int1_rev, mathChat.6.player.int2, mathChat.6.player.int3), na.rm = TRUE),
    int_mathJitsi = rowMeans(select(., mathJitsi.6.player.int1, mathJitsi.6.player.int2, mathJitsi.6.player.int3), na.rm = TRUE),
    int_hpChat = rowMeans(select(., HiddenProfile_Chat.3.player.int1, HiddenProfile_Chat.3.player.int2, HiddenProfile_Chat.3.player.int3), na.rm = TRUE),
    int_hpJitsi = rowMeans(select(., HiddenProfile_Jitsi.3.player.int1, HiddenProfile_Jitsi.3.player.int2, HiddenProfile_Jitsi.3.player.int3), na.rm = TRUE),

    cg_mathChat = rowMeans(select(., mathChat.6.player.cg1_rev, mathChat.6.player.cg2, mathChat.6.player.cg3_rev, 
                                  mathChat.6.player.cg4, mathChat.6.player.cg5_rev, mathChat.6.player.cg6), na.rm = TRUE),
    cg_mathJitsi = rowMeans(select(., starts_with("mathJitsi.6.player.cg")), na.rm = TRUE),
    cg_hpChat = rowMeans(select(., starts_with("HiddenProfile_Chat.3.player.cg")), na.rm = TRUE),
    cg_hpJitsi = rowMeans(select(., starts_with("HiddenProfile_Jitsi.3.player.cg")), na.rm = TRUE)
  )

mc_long <- data %>%
  select(mc_mathChat, mc_mathJitsi, mc_hpChat, mc_hpJitsi) %>%
  pivot_longer(cols = everything(), names_to = "Treatment", values_to = "Score") %>%
  mutate(Treatment = recode(Treatment,
                            mc_mathChat = "Math – Chat",
                            mc_mathJitsi = "Math – Jitsi",
                            mc_hpChat = "HiddenProfile – Chat",
                            mc_hpJitsi = "HiddenProfile – Jitsi"))

ggplot(mc_long, aes(x = Treatment, y = Score)) +
  geom_boxplot(fill = "skyblue", alpha = 0.7) +
  theme_minimal() +
  labs(title = "Means for Coordination (MC) – nach Treatment",
       y = "Skalenwert (1–7)", x = NULL) +
  ylim(1, 7)

int_long <- data %>%
  select(int_mathChat, int_mathJitsi, int_hpChat, int_hpJitsi) %>%
  pivot_longer(cols = everything(), names_to = "Treatment", values_to = "Score") %>%
  mutate(Treatment = recode(Treatment,
                            int_mathChat = "Math – Chat",
                            int_mathJitsi = "Math – Jitsi",
                            int_hpChat = "HiddenProfile – Chat",
                            int_hpJitsi = "HiddenProfile – Jitsi"))

ggplot(int_long, aes(x = Treatment, y = Score)) +
  geom_boxplot(fill = "orchid", alpha = 0.7) +
  theme_minimal() +
  labs(title = "Team Member Interdependence (INT) – nach Treatment",
       y = "Skalenwert (1–7)", x = NULL) +
  ylim(1, 7)

cg_long <- data %>%
  select(cg_mathChat, cg_mathJitsi, cg_hpChat, cg_hpJitsi) %>%
  pivot_longer(cols = everything(), names_to = "Treatment", values_to = "Score") %>%
  mutate(Treatment = recode(Treatment,
                            cg_mathChat = "Math – Chat",
                            cg_mathJitsi = "Math – Jitsi",
                            cg_hpChat = "HiddenProfile – Chat",
                            cg_hpJitsi = "HiddenProfile – Jitsi"))

ggplot(cg_long, aes(x = Treatment, y = Score)) +
  geom_boxplot(fill = "seagreen3", alpha = 0.7) +
  theme_minimal() +
  labs(title = "Common Goal (CG) – nach Treatment",
       y = "Skalenwert (1–7)", x = NULL) +
  ylim(1, 7)

```

Manipulation check for difficulty

```{r}
# Funktion um z.B. "['A', 'O', 'F', 'B']" in echten Vektor zu verwandeln
parse_order_string <- function(s) {
  s %>%
    str_remove_all("\\[|\\]|'|\"") %>%
    str_split(",\\s*") %>%
    unlist()
}

# Mapping Math-Codes zu Labels
map_math_difficulty <- function(code) {
  recode(code,
         "B" = "Easy",
         "A" = "Optimal_Selected",
         "F" = "Optimal_Calibrated",
         "O" = "Hard")
}

data <- data %>%
  mutate(
    math_order = map(as.character(participant.condition_order), parse_order_string) %>%
                   map(~ map_chr(.x, map_math_difficulty)),
    hp_order   = map(as.character(participant.hp_condition_order), parse_order_string) %>%
                   map(str_to_title)
  )

# MATH TASK
math_difficulty_long <- data %>%
  select(participant.code, math_order,
         starts_with("mathChat.3.player.csb"),
         starts_with("mathChat.4.player.csb"),
         starts_with("mathChat.5.player.csb"),
         starts_with("mathChat.6.player.csb"),
         starts_with("mathJitsi.3.player.csb"),
         starts_with("mathJitsi.4.player.csb"),
         starts_with("mathJitsi.5.player.csb"),
         starts_with("mathJitsi.6.player.csb")) %>%
  pivot_longer(cols = -c(participant.code, math_order),
               names_to = "var", values_to = "csb") %>%
  mutate(
    round = str_extract(var, "\\d+"),
    comm = ifelse(str_detect(var, "Chat"), "Chat", "Jitsi"),
    index = as.integer(round) - 2,  # weil math.3 = erste relevante Runde
    difficulty = map2_chr(math_order, index, ~ .x[.y])
  ) %>%
  drop_na(csb)

# HP TASK
hp_difficulty_long <- data %>%
  select(participant.code, hp_order,
         starts_with("HiddenProfile_Chat."),
         starts_with("HiddenProfile_Jitsi.")) %>%
  pivot_longer(cols = matches("HiddenProfile_.*\\.player\\.csb[12]"),
               names_to = "var", values_to = "csb") %>%
  mutate(
    round = str_extract(var, "(?<=\\.)\\d+"),
    comm = ifelse(str_detect(var, "Chat"), "Chat", "Jitsi"),
    index = as.integer(round),  # hier ist Runde = Index
    difficulty = map2_chr(hp_order, index, ~ .x[.y])
  ) %>%
  drop_na(csb)

# X-Achse splitten in zwei ästhetischere Achsen:
ggplot(math_difficulty_long, aes(x = difficulty, y = csb, fill = comm)) +
  geom_boxplot(position = position_dodge(width = 0.75)) +
  labs(title = "Math Task", x = "Difficulty", y = "Subjective Difficulty") +
  theme_minimal()

ggplot(hp_difficulty_long, aes(x = difficulty, y = csb, fill = comm)) +
  geom_boxplot(position = position_dodge(width = 0.75)) +
  labs(title = "Hidden Profile Task", x = "Difficulty", y = "Subjective Difficulty") +
  theme_minimal()

```

Internal consistency check for flow construct and flow score calculation

```{r}
# Reihenfolge-Parsing-Helfer
parse_order_string <- function(s) {
  s %>%
    str_remove_all("\\[|\\]|'|\"") %>%
    str_split(",\\s*") %>%
    unlist()
}

# Reihenfolgen extrahieren
data <- data %>%
  mutate(
    math_order = map(participant.condition_order, parse_order_string),
    hp_order   = map(participant.hp_condition_order, parse_order_string)
  )

# Funktion zum Remappen der Items von Runde -> Schwierigkeitsstufe
remap_fss_items <- function(df, prefix, rounds, difficulty_map) {
  out <- list()
  for (i in seq_along(rounds)) {
    r <- rounds[i]
    diff_code <- difficulty_map[i]
    for (j in 1:9) {
      old_name <- sprintf("%s.%d.player.fss%02d", prefix, r, j)
      new_name <- sprintf("%s.%s.player.fss%02d", prefix, diff_code, j)
      out[[new_name]] <- if (old_name %in% names(df)) df[[old_name]][1] else NA
    }
  }
  # Eine Zeile mit vielen Spalten zurückgeben
  return(as_tibble(out))
}

# Spaltennamen für Mathe-Items extrahieren
math_cols <- names(data)[startsWith(names(data), "math")]

# Mathe-Daten remappen
math_data <- data %>%
  mutate(math_items = pmap(
    c(list(math_order), select(., all_of(math_cols))),
    function(order, ...) {
      df <- tibble(...)
      bind_cols(
        remap_fss_items(df, "mathJitsi", 3:6, order),
        remap_fss_items(df, "mathChat", 3:6, order)
      )
    }
  ))

# Spaltennamen für Hidden Profile-Items extrahieren
hp_cols <- names(data)[startsWith(names(data), "HiddenProfile_")]

# HP-Daten remappen
hp_data <- data %>%
  mutate(hp_items = pmap(
    c(list(hp_order), select(., all_of(hp_cols))),
    function(order, ...) {
      df <- tibble(...)
      bind_cols(
        remap_fss_items(df, "HiddenProfile_Jitsi", 1:3, order),
        remap_fss_items(df, "HiddenProfile_Chat", 1:3, order)
      )
    }
  ))

# Funktion für Cronbach's Alpha und Mittelwert
aggregate_flow <- function(df, prefix, difficulties) {
  results <- list()
  
  # NEU: Alpha-Ausgabe Header
  cat("\n=== Cronbach's Alpha für", prefix, "===\n")
  
  for (d in difficulties) {
    for (comm in c("Chat", "Jitsi")) {
      # Prefix korrekt setzen – für HiddenProfile mit Unterstrich
      full_prefix <- if (prefix == "HiddenProfile") {
        paste0(prefix, "_", comm)
      } else {
        paste0(prefix, comm)
      }
      
      items <- sprintf("%s.%s.player.fss%02d", full_prefix, d, 1:10)
      valid_items <- items[items %in% names(df)]
      
      if (length(valid_items) >= 2) {
        item_df <- df[valid_items]
        alpha_val <- tryCatch(psych::alpha(item_df)$total$raw_alpha, error = function(e) NA)
        scale_mean <- rowMeans(item_df, na.rm = TRUE)
        
        # NEU: Alpha-Wert ausgeben
        scale_name <- paste0(prefix, "_", d, "_", comm)
        if (!is.na(alpha_val)) {
          cat(sprintf("%-20s: α = %.3f\n", scale_name, alpha_val))
        } else {
          cat(sprintf("%-20s: α = NA\n", scale_name))
        }
        
      } else {
        alpha_val <- NA
        scale_mean <- rep(NA, nrow(df))
        
        # NEU: Ausgabe für zu wenige Items
        scale_name <- paste0(prefix, "_", d, "_", comm)
        cat(sprintf("%-20s: α = NA (nur %d Items)\n", scale_name, length(valid_items)))
      }
      
      col_name <- paste0("fss_", prefix, "_", d, "_", comm)
      results[[col_name]] <- scale_mean
    }
  }
  as.data.frame(results)
}


full_items <- bind_cols(
  data["participant.code"],
  map_dfr(math_data$math_items, identity),
  map_dfr(hp_data$hp_items, identity)
)

# Skalen berechnen
math_scores <- aggregate_flow(full_items, "math", c("A", "O", "F", "B"))
hp_scores   <- aggregate_flow(full_items, "HiddenProfile", c("EASY", "MED", "HARD"))

# Enddatensatz mit allen Skalen
flow_scores <- bind_cols(full_items["participant.code"], math_scores, hp_scores)

# Wide → Long
flow_scores_long <- flow_scores %>%
  pivot_longer(
    cols = -participant.code,
    names_to = "scale_name",
    values_to = "flow_score"
  )

# Zerlegen von scale_name in task, difficulty, comm
flow_scores_long <- flow_scores_long %>%
  separate(scale_name, into = c("fss", "task", "difficulty", "comm"), sep = "_", remove = TRUE) %>%
  select(-fss)

# NaN-Zeilen rausfiltern: nur tatsächliche Kommunikationsbedingung behalten
flow_scores_long <- flow_scores_long %>%
  filter(!is.na(flow_score))

data <- data %>%
  mutate(
    math_order = map(participant.condition_order, parse_order_string),
    hp_order   = map(participant.hp_condition_order, parse_order_string)
  )

# condition_order ergänzen
flow_scores_long <- flow_scores_long %>%
  left_join(data %>% select(participant.code, participant.condition_order), by = "participant.code") %>%
  mutate(condition_order = participant.condition_order) %>%
  select(-participant.condition_order)

# condition_order ergänzen
flow_scores_long <- flow_scores_long %>%
  left_join(data %>% select(participant.code, participant.hp_condition_order), by = "participant.code") %>%
  mutate(hp_condition_order = participant.hp_condition_order) %>%
  select(-participant.hp_condition_order)

# Team-ID erzeugen (falls noch nicht erfolgt)
data <- data %>%
  mutate(team_id = paste(session.code, Intro.1.group.custom_group_id, sep = "_"))

# Team-ID ergänzen
flow_scores_long <- flow_scores_long %>%
  left_join(data %>% select(participant.code, team_id), by = "participant.code")

# Einheitliche Schreibweise für spätere Filter
flow_scores_long <- flow_scores_long %>%
  mutate(
    task = recode(task,
                  "math" = "Math",
                  "HiddenProfile" = "HP"),
    difficulty = recode(difficulty,
                        "B" = "Easy",
                        "A" = "Optimal_Selected",
                        "F" = "Optimal_Calibrated",
                        "O" = "Hard",
                        "EASY" = "Easy",
                        "MED" = "Medium",
                        "HARD" = "Hard")
  )

# Jetzt den Long-Datensatz als neuen flow_scores verwenden
flow_scores <- flow_scores_long

flow_scores <- flow_scores %>%
  mutate(
    order = case_when(
      task == "Math" ~ condition_order,
      task == "HP"   ~ hp_condition_order,
      TRUE           ~ NA_character_
    )
  ) %>%
  select(-condition_order, -hp_condition_order)

# Boxplot erstellen
ggplot(flow_scores, aes(x = interaction(task, comm, sep = " - "), y = flow_score, fill = task)) +
  geom_boxplot(outlier.shape = NA, alpha = 0.7) +
  geom_jitter(width = 0.2, alpha = 0.4, color = "black", size = 1) +
  labs(
    title = "Flow-Scores nach Experimentalbedingung",
    x = "Bedingung (Task - Medium)",
    y = "Flow-Score"
  ) +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")

```

Outlier check

```{r}
# Ausreißer-Analyse für Flow-Scores
# ================================================================================

# 1. Ausreißer identifizieren (gruppenweise)
flow_scores_outlier <- flow_scores %>%
  group_by(task, comm) %>%
  mutate(
    mean_flow = mean(flow_score, na.rm = TRUE),
    sd_flow = sd(flow_score, na.rm = TRUE),
    z_score = (flow_score - mean_flow) / sd_flow,
    is_outlier = abs(z_score) > 2,
    outlier_direction = case_when(
      z_score > 2 ~ "high",
      z_score < -2 ~ "low",
      TRUE ~ "normal"
    )
  ) %>%
  ungroup()

# 2. Zusammenfassung der Ausreißer
outlier_summary <- flow_scores_outlier %>%
  group_by(task, comm) %>%
  summarise(
    n_total = n(),
    n_outliers = sum(is_outlier),
    n_high = sum(outlier_direction == "high"),
    n_low = sum(outlier_direction == "low"),
    pct_outliers = round(mean(is_outlier) * 100, 2),
    mean_flow = round(mean(flow_score, na.rm = TRUE), 3),
    sd_flow = round(sd(flow_score, na.rm = TRUE), 3),
    .groups = "drop"
  )

print("Ausreißer-Zusammenfassung nach Bedingung:")
print(outlier_summary)

# 3. Gesamtübersicht
total_outliers <- flow_scores_outlier %>%
  summarise(
    total_observations = n(),
    total_outliers = sum(is_outlier),
    pct_outliers = round(mean(is_outlier) * 100, 2)
  )

print("\nGesamtanzahl Ausreißer:")
print(total_outliers)

# 4. Details zu den Ausreißern
outlier_details <- flow_scores_outlier %>%
  filter(is_outlier) %>%
  select(participant.code, task, comm, difficulty, flow_score, z_score, outlier_direction) %>%
  arrange(desc(abs(z_score)))

print("\nTop 10 extremste Ausreißer:")
print(head(outlier_details, 10))

# 5. Visualisierung der Ausreißer
library(ggplot2)

# Boxplot mit Ausreißern markiert
p1 <- ggplot(flow_scores_outlier, aes(x = interaction(task, comm), y = flow_score)) +
  geom_boxplot(aes(fill = task), alpha = 0.7) +
  geom_point(data = filter(flow_scores_outlier, is_outlier), 
             aes(color = outlier_direction), size = 3) +
  scale_color_manual(values = c("high" = "red", "low" = "blue")) +
  labs(title = "Flow-Scores mit markierten Ausreißern (>2 SD)",
       x = "Bedingung", y = "Flow Score",
       color = "Ausreißer-Typ") +
  theme_minimal()

print(p1)

# Z-Score Verteilung
p2 <- ggplot(flow_scores_outlier, aes(x = z_score)) +
  geom_histogram(bins = 30, fill = "lightblue", color = "black", alpha = 0.7) +
  geom_vline(xintercept = c(-2, 2), color = "red", linetype = "dashed") +
  facet_wrap(~ interaction(task, comm)) +
  labs(title = "Z-Score Verteilung der Flow-Werte",
       subtitle = "Rote Linien = ±2 SD Grenze",
       x = "Z-Score", y = "Häufigkeit") +
  theme_minimal()

print(p2)

# 6. Ausreißer nach Schwierigkeit untersuchen
outlier_by_difficulty <- flow_scores_outlier %>%
  group_by(difficulty, task) %>%
  summarise(
    n_total = n(),
    n_outliers = sum(is_outlier),
    pct_outliers = round(mean(is_outlier) * 100, 2),
    outlier_types = paste(
      "High:", sum(outlier_direction == "high"),
      "Low:", sum(outlier_direction == "low")
    ),
    .groups = "drop"
  )

print("\nAusreißer nach Schwierigkeitsstufe:")
print(outlier_by_difficulty)

# 7. Prüfen ob bestimmte Teilnehmer häufig Ausreißer sind
participant_outlier_freq <- flow_scores_outlier %>%
  group_by(participant.code) %>%
  summarise(
    n_measurements = n(),
    n_outlier = sum(is_outlier),
    pct_outlier = round(mean(is_outlier) * 100, 2)
  ) %>%
  filter(n_outlier > 0) %>%
  arrange(desc(n_outlier))

print("\nTeilnehmer mit Ausreißer-Messungen:")
print(head(participant_outlier_freq, 10))

# 8. Team-Level Ausreißer prüfen (für Shared Flow)
team_flow_outliers <- flow_scores_outlier %>%
  group_by(team_id, task, comm) %>%
  summarise(
    team_mean_flow = mean(flow_score, na.rm = TRUE),
    team_sd_flow = sd(flow_score, na.rm = TRUE),
    n_members = n(),
    n_outlier_members = sum(is_outlier),
    .groups = "drop"
  ) %>%
  group_by(task, comm) %>%
  mutate(
    grand_mean = mean(team_mean_flow, na.rm = TRUE),
    grand_sd = sd(team_mean_flow, na.rm = TRUE),
    team_z_score = (team_mean_flow - grand_mean) / grand_sd,
    is_team_outlier = abs(team_z_score) > 2
  ) %>%
  filter(is_team_outlier)

print("\nTeams mit extremen Flow-Werten:")
print(team_flow_outliers)
```


Flow proneness consistency check and score calculation

```{r}
# Flow Proneness Items
flowp_items <- data %>%
  select(participant.code,
         starts_with("Outro.1.player.fpw"),
         starts_with("Outro.1.player.fph"),
         starts_with("Outro.1.player.fpl"))

flowp_items <- flowp_items %>%
  mutate(
    `Outro.1.player.fpl1` = 6 - `Outro.1.player.fpl1`,
    `Outro.1.player.fph1` = 6 - `Outro.1.player.fph1`,
    `Outro.1.player.fpw1` = 6 - `Outro.1.player.fpw1`
  )

# Prüfe interne Konsistenz pro Dimension
alpha_work <- psych::alpha(flowp_items %>% select(starts_with("Outro.1.player.fpw")))
alpha_household <- psych::alpha(flowp_items %>% select(starts_with("Outro.1.player.fph")))
alpha_leisure <- psych::alpha(flowp_items %>% select(starts_with("Outro.1.player.fpl")))

# Aggregiere zu drei Scores + Gesamtwert
flow_proneness_scores <- flowp_items %>%
  mutate(
    fp_work = rowMeans(select(., starts_with("Outro.1.player.fpw")), na.rm = TRUE),
    fp_household = rowMeans(select(., starts_with("Outro.1.player.fph")), na.rm = TRUE),
    fp_leisure = rowMeans(select(., starts_with("Outro.1.player.fpl")), na.rm = TRUE)
  ) %>%
  mutate(fp_total = rowMeans(select(., fp_work, fp_household, fp_leisure), na.rm = TRUE)) %>%
  select(participant.code, fp_total)

flow_scores <- flow_scores %>%
  left_join(flow_proneness_scores, by = "participant.code")

flow_clean <- flow_clean %>%
  left_join(flow_proneness_scores, by = "participant.code")
```

Linear mixed model for flow with individual-level variables (level-1) nested within teams (level-2), Calculation of Goodness-of-Fit criteria AIC, BIC, Marginal R2 and Conditional R2

```{r}
# Erweiterte Regressionsanalyse mit emmeans Post-Hoc Tests
# Basierend auf Betreuer-Vorgaben

library(lme4)
library(performance)
library(emmeans)
library(dplyr)
library(tibble)

# ================================================================================
# TEIL 1: MATH TASK ANALYSE
# ================================================================================

print("=== MATH TASK REGRESSIONSANALYSE ===")

# Filter: Nur Mathe-Task
flow_scores_math <- flow_scores %>% 
  filter(task == "Math")

# Modell-Datensatz vorbereiten
model_data_math <- flow_scores_math %>%
  mutate(
    synchronicity = ifelse(comm == "Jitsi", "High", "Low"),
    synchronicity = factor(synchronicity, levels = c("Low", "High")),
    difficulty = factor(difficulty, levels = c("Easy", "Optimal_Selected", "Optimal_Calibrated", "Hard"))
  )

print(sprintf("Math Task: %d Beobachtungen, %d Teams, %d Teilnehmer", 
              nrow(model_data_math),
              length(unique(model_data_math$team_id)),
              length(unique(model_data_math$participant.code))))

# Modell 1: Synchronicity + Difficulty
print("\n--- MATH MODELL 1: Haupteffekte ---")
model_math_1 <- lmer(
  flow_score ~ synchronicity + difficulty + 
    (1 | team_id) + (1 | participant.code),
  data = model_data_math
)
summary(model_math_1)

# Modell 2: Interaktion hinzufügen
print("\n--- MATH MODELL 2: Mit Interaktion ---")
model_math_2 <- lmer(
  flow_score ~ synchronicity * difficulty + 
    (1 | team_id) + (1 | participant.code),
  data = model_data_math
)
summary(model_math_2)

# Modell 3: Zusätzlich Flow Proneness & Condition Order
print("\n--- MATH MODELL 3: Vollständiges Modell ---")
model_math_3 <- lmer(
  flow_score ~ synchronicity * difficulty + 
    fp_total + order +
    (1 | team_id) + (1 | participant.code),
  data = model_data_math
)
summary(model_math_3)

# Modellvergleich
print("\n--- MATH MODELLVERGLEICH ---")
aic_comparison_math <- AIC(model_math_1, model_math_2, model_math_3)
bic_comparison_math <- BIC(model_math_1, model_math_2, model_math_3)
print("AIC Vergleich:")
print(aic_comparison_math)
print("BIC Vergleich:")
print(bic_comparison_math)

# R² Vergleich
r2_math_1 <- r2(model_math_1)
r2_math_2 <- r2(model_math_2)
r2_math_3 <- r2(model_math_3)

print("\nR² Vergleich:")
print(paste("Modell 1 - Marginal R²:", round(r2_math_1$R2_marginal, 4), 
            "Conditional R²:", round(r2_math_1$R2_conditional, 4)))
print(paste("Modell 2 - Marginal R²:", round(r2_math_2$R2_marginal, 4), 
            "Conditional R²:", round(r2_math_2$R2_conditional, 4)))
print(paste("Modell 3 - Marginal R²:", round(r2_math_3$R2_marginal, 4), 
            "Conditional R²:", round(r2_math_3$R2_conditional, 4)))

# ================================================================================
# TEIL 1b: MATH TASK - EMMEANS POST-HOC TESTS
# ================================================================================

print("\n=== MATH TASK - EMMEANS POST-HOC ANALYSEN ===")

# Wähle das beste Modell für Post-Hoc Tests (basierend auf AIC/BIC)
best_math_model <- model_math_3  # Anpassbar je nach Ergebnissen

# Between-Subjects Tests: Synchronicity Unterschiede für jede Difficulty
print("\n--- Between-Subjects Tests: Synchronicity|Difficulty ---")
bs_tests_math <- emmeans(best_math_model, specs = pairwise ~ synchronicity|difficulty, adjust = "none")$contrasts %>%
  as_tibble() %>%
  # Manuelle BH Korrektur
  mutate(p.adj = p.adjust(p.value, "BH")) %>%
  # Difficulty Labels anpassen (falls gewünscht)
  mutate(difficulty = dplyr::recode(difficulty, 
                                   "Easy" = "Easy",
                                   "Optimal_Selected" = "Optimal\n(Selected)",
                                   "Optimal_Calibrated" = "Optimal\n(Calibrated)",
                                   "Hard" = "Hard")) %>%
  # Numerische Werte runden (neue Syntax)
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Between-Subjects Tests (Synchronicity Vergleiche pro Difficulty):")
print(bs_tests_math)

# Within-Subjects Tests: Difficulty Unterschiede für jede Synchronicity
print("\n--- Within-Subjects Tests: Difficulty|Synchronicity ---")
ws_tests_math <- emmeans(best_math_model, specs = pairwise ~ difficulty|synchronicity, adjust = "none")$contrasts %>%
  as_tibble() %>%
  # Manuelle BH Korrektur
  mutate(p.adj = p.adjust(p.value, "BH")) %>%
  # Numerische Werte runden (neue Syntax)
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Within-Subjects Tests (Difficulty Vergleiche pro Synchronicity):")
print(ws_tests_math)

# Marginal Means zur Interpretation
print("\n--- Marginal Means für Math Task ---")
marginal_means_math <- emmeans(best_math_model, ~ synchronicity * difficulty) %>%
  as_tibble() %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))
print(marginal_means_math)

# ================================================================================
# TEIL 2: HIDDEN PROFILE TASK ANALYSE
# ================================================================================

print("\n\n=== HIDDEN PROFILE TASK REGRESSIONSANALYSE ===")

# Filter: Nur HP-Task
flow_scores_hp <- flow_scores %>% 
  filter(task == "HP")

# Modell-Datensatz vorbereiten
model_data_hp <- flow_scores_hp %>%
  mutate(
    synchronicity = ifelse(comm == "Jitsi", "High", "Low"),
    synchronicity = factor(synchronicity, levels = c("Low", "High")),
    difficulty = factor(difficulty, levels = c("Easy", "Medium", "Hard"))  # HP hat nur 3 Stufen
  )

print(sprintf("HP Task: %d Beobachtungen, %d Teams, %d Teilnehmer", 
              nrow(model_data_hp),
              length(unique(model_data_hp$team_id)),
              length(unique(model_data_hp$participant.code))))

# Modell 1: Synchronicity + Difficulty
print("\n--- HP MODELL 1: Haupteffekte ---")
model_hp_1 <- lmer(
  flow_score ~ synchronicity + difficulty + 
    (1 | team_id) + (1 | participant.code),
  data = model_data_hp
)
summary(model_hp_1)

# Modell 2: Interaktion hinzufügen
print("\n--- HP MODELL 2: Mit Interaktion ---")
model_hp_2 <- lmer(
  flow_score ~ synchronicity * difficulty + 
    (1 | team_id) + (1 | participant.code),
  data = model_data_hp
)
summary(model_hp_2)

# Modell 3: Zusätzlich Flow Proneness & Condition Order
print("\n--- HP MODELL 3: Vollständiges Modell ---")
model_hp_3 <- lmer(
  flow_score ~ synchronicity * difficulty + 
    fp_total + order +
    (1 | team_id) + (1 | participant.code),
  data = model_data_hp
)
summary(model_hp_3)

# Modellvergleich
print("\n--- HP MODELLVERGLEICH ---")
aic_comparison_hp <- AIC(model_hp_1, model_hp_2, model_hp_3)
bic_comparison_hp <- BIC(model_hp_1, model_hp_2, model_hp_3)
print("AIC Vergleich:")
print(aic_comparison_hp)
print("BIC Vergleich:")
print(bic_comparison_hp)

# R² Vergleich
r2_hp_1 <- r2(model_hp_1)
r2_hp_2 <- r2(model_hp_2)
r2_hp_3 <- r2(model_hp_3)

print("\nR² Vergleich:")
print(paste("Modell 1 - Marginal R²:", round(r2_hp_1$R2_marginal, 4), 
            "Conditional R²:", round(r2_hp_1$R2_conditional, 4)))
print(paste("Modell 2 - Marginal R²:", round(r2_hp_2$R2_marginal, 4), 
            "Conditional R²:", round(r2_hp_2$R2_conditional, 4)))
print(paste("Modell 3 - Marginal R²:", round(r2_hp_3$R2_marginal, 4), 
            "Conditional R²:", round(r2_hp_3$R2_conditional, 4)))

# ================================================================================
# TEIL 2b: HIDDEN PROFILE TASK - EMMEANS POST-HOC TESTS
# ================================================================================

print("\n=== HIDDEN PROFILE TASK - EMMEANS POST-HOC ANALYSEN ===")

# Wähle das beste Modell für Post-Hoc Tests
best_hp_model <- model_hp_3  # Anpassbar je nach Ergebnissen

# Between-Subjects Tests: Synchronicity Unterschiede für jede Difficulty
print("\n--- Between-Subjects Tests: Synchronicity|Difficulty ---")
bs_tests_hp <- emmeans(best_hp_model, specs = pairwise ~ synchronicity|difficulty, adjust = "none")$contrasts %>%
  as_tibble() %>%
  # Manuelle BH Korrektur
  mutate(p.adj = p.adjust(p.value, "BH")) %>%
  # Numerische Werte runden (neue Syntax)
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Between-Subjects Tests (Synchronicity Vergleiche pro Difficulty):")
print(bs_tests_hp)

# Within-Subjects Tests: Difficulty Unterschiede für jede Synchronicity
print("\n--- Within-Subjects Tests: Difficulty|Synchronicity ---")
ws_tests_hp <- emmeans(best_hp_model, specs = pairwise ~ difficulty|synchronicity, adjust = "none")$contrasts %>%
  as_tibble() %>%
  # Manuelle BH Korrektur
  mutate(p.adj = p.adjust(p.value, "BH")) %>%
  # Numerische Werte runden (neue Syntax)
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Within-Subjects Tests (Difficulty Vergleiche pro Synchronicity):")
print(ws_tests_hp)

# Marginal Means zur Interpretation
print("\n--- Marginal Means für HP Task ---")
marginal_means_hp <- emmeans(best_hp_model, ~ synchronicity * difficulty) %>%
  as_tibble() %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))
print(marginal_means_hp)

# ================================================================================
# TEIL 3: ÜBERGREIFENDE ZUSAMMENFASSUNG
# ================================================================================

print("\n\n=== ÜBERGREIFENDE ZUSAMMENFASSUNG ===")

# Sammle signifikante Ergebnisse
print("\n--- Signifikante Between-Subjects Effekte (Synchronicity) ---")

# Math Task signifikante BS Effekte
math_significant_bs <- bs_tests_math %>%
  filter(p.adj < 0.05) %>%
  mutate(Task = "Math")

# HP Task signifikante BS Effekte  
hp_significant_bs <- bs_tests_hp %>%
  filter(p.adj < 0.05) %>%
  mutate(Task = "HP")

# Kombiniere signifikante Ergebnisse
all_significant_bs <- bind_rows(math_significant_bs, hp_significant_bs)

if (nrow(all_significant_bs) > 0) {
  print("Signifikante Synchronicity Unterschiede:")
  print(all_significant_bs %>% select(Task, difficulty, estimate, p.value, p.adj))
} else {
  print("Keine signifikanten Synchronicity Unterschiede gefunden.")
}

print("\n--- Signifikante Within-Subjects Effekte (Difficulty) ---")

# Math Task signifikante WS Effekte
math_significant_ws <- ws_tests_math %>%
  filter(p.adj < 0.05) %>%
  mutate(Task = "Math")

# HP Task signifikante WS Effekte
hp_significant_ws <- ws_tests_hp %>%
  filter(p.adj < 0.05) %>%
  mutate(Task = "HP")

# Kombiniere signifikante Ergebnisse
all_significant_ws <- bind_rows(math_significant_ws, hp_significant_ws)

if (nrow(all_significant_ws) > 0) {
  print("Signifikante Difficulty Unterschiede:")
  print(all_significant_ws %>% 
         select(Task, synchronicity, contrast, estimate, p.value, p.adj) %>%
         head(10))  # Zeige nur die ersten 10 wegen der Vielzahl an Paarvergleichen
} else {
  print("Keine signifikanten Difficulty Unterschiede gefunden.")
}

# ================================================================================
# TEIL 4: EXPORTIERBARE ERGEBNISTABELLEN
# ================================================================================

print("\n=== VERFÜGBARE OBJEKTE FÜR WEITERE ANALYSEN ===")
cat("MODELLE:\n")
cat("- model_math_1, model_math_2, model_math_3: Math Task Modelle\n")
cat("- model_hp_1, model_hp_2, model_hp_3: Hidden Profile Task Modelle\n")
cat("\nPOST-HOC TESTS:\n")
cat("- bs_tests_math, ws_tests_math: Math Task emmeans Tests\n")
cat("- bs_tests_hp, ws_tests_hp: Hidden Profile Task emmeans Tests\n")
cat("- marginal_means_math, marginal_means_hp: Geschätzte Randmittel\n")
cat("\nZUSAMMENFASSUNGEN:\n")
cat("- all_significant_bs: Alle signifikanten Between-Subjects Effekte\n")
cat("- all_significant_ws: Alle signifikanten Within-Subjects Effekte\n")

# Optional: Exportiere Ergebnisse
# write.csv(bs_tests_math, "math_between_subjects_tests.csv", row.names = FALSE)
# write.csv(bs_tests_hp, "hp_between_subjects_tests.csv", row.names = FALSE)
# write.csv(marginal_means_math, "math_marginal_means.csv", row.names = FALSE)
# write.csv(marginal_means_hp, "hp_marginal_means.csv", row.names = FALSE)

print("\n=== INTERPRETATION GUIDELINES ===")
cat("Between-Subjects Tests zeigen:\n")
cat("- Unterschiede zwischen Synchronicity (Jitsi vs Chat) für jede Difficulty\n")
cat("- Negative estimates: Chat > Jitsi Flow\n")
cat("- Positive estimates: Jitsi > Chat Flow\n\n")
cat("Within-Subjects Tests zeigen:\n")
cat("- Unterschiede zwischen Difficulty Levels für jede Synchronicity\n")
cat("- Wichtig für die Interpretation von Difficulty-Effekten\n\n")
cat("Marginal Means geben dir die geschätzten Mittelwerte für jede Bedingung.\n")
```

Central LMMs without Outliers

```{r}
library(emmeans)
library(dplyr)
library(tibble)

# ================================================================================
# MATH TASK - OUTLIER-BEREINIGTER DATENSATZ
# ================================================================================

print("=== MATH TASK - OUTLIER-BEREINIGTE ANALYSE ===")

# Filter: Nur Mathe-Task
flow_clean_math <- flow_clean %>% 
  filter(task == "Math")

# Modell-Datensatz vorbereiten
model_data_math_clean <- flow_clean_math %>%
  mutate(
    synchronicity = ifelse(comm == "Jitsi", "High", "Low"),
    synchronicity = factor(synchronicity, levels = c("Low", "High")),
    difficulty = factor(difficulty, levels = c("Easy", "Optimal_Selected", "Optimal_Calibrated", "Hard"))
  )

print(sprintf("Math Task (clean): %d Beobachtungen, %d Teams, %d Teilnehmer", 
              nrow(model_data_math_clean),
              length(unique(model_data_math_clean$team_id)),
              length(unique(model_data_math_clean$participant.code))))

# Modell 3 (bestes Modell)
print("\n--- MATH MODELL 3 (OUTLIER-BEREINIGT) ---")
model_math_3_clean <- lmer(
  flow_score ~ synchronicity * difficulty + 
    fp_total + order +
    (1 | team_id) + (1 | participant.code),
  data = model_data_math_clean
)
summary(model_math_3_clean)

# ================================================================================
# MATH TASK - EMMEANS POST-HOC TESTS (OUTLIER-BEREINIGT)
# ================================================================================

print("\n=== MATH TASK - EMMEANS POST-HOC ANALYSEN (OUTLIER-BEREINIGT) ===")

# Between-Subjects Tests: Synchronicity Unterschiede für jede Difficulty
print("\n--- Between-Subjects Tests: Synchronicity|Difficulty (clean) ---")
bs_tests_math_clean <- emmeans(model_math_3_clean, specs = pairwise ~ synchronicity|difficulty, adjust = "none")$contrasts %>%
  as_tibble() %>%
  # Manuelle BH Korrektur
  mutate(p.adj = p.adjust(p.value, "BH")) %>%
  # Difficulty Labels anpassen (falls gewünscht)
  mutate(difficulty = dplyr::recode(difficulty, 
                                   "Easy" = "Easy",
                                   "Optimal_Selected" = "Optimal\n(Selected)",
                                   "Optimal_Calibrated" = "Optimal\n(Calibrated)",
                                   "Hard" = "Hard")) %>%
  # Numerische Werte runden (neue Syntax)
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Between-Subjects Tests (Synchronicity Vergleiche pro Difficulty - clean):")
print(bs_tests_math_clean)

# Within-Subjects Tests: Difficulty Unterschiede für jede Synchronicity
print("\n--- Within-Subjects Tests: Difficulty|Synchronicity (clean) ---")
ws_tests_math_clean <- emmeans(model_math_3_clean, specs = pairwise ~ difficulty|synchronicity, adjust = "none")$contrasts %>%
  as_tibble() %>%
  # Manuelle BH Korrektur
  mutate(p.adj = p.adjust(p.value, "BH")) %>%
  # Numerische Werte runden (neue Syntax)
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Within-Subjects Tests (Difficulty Vergleiche pro Synchronicity - clean):")
print(ws_tests_math_clean)

# Marginal Means zur Interpretation
print("\n--- Marginal Means für Math Task (clean) ---")
marginal_means_math_clean <- emmeans(model_math_3_clean, ~ synchronicity * difficulty) %>%
  as_tibble() %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))
print(marginal_means_math_clean)

# ================================================================================
# HIDDEN PROFILE TASK - OUTLIER-BEREINIGTER DATENSATZ
# ================================================================================

print("\n\n=== HIDDEN PROFILE TASK - OUTLIER-BEREINIGTE ANALYSE ===")

# Filter: Nur HP-Task
flow_clean_hp <- flow_clean %>% 
  filter(task == "HP")

# Modell-Datensatz vorbereiten
model_data_hp_clean <- flow_clean_hp %>%
  mutate(
    synchronicity = ifelse(comm == "Jitsi", "High", "Low"),
    synchronicity = factor(synchronicity, levels = c("Low", "High")),
    difficulty = factor(difficulty, levels = c("Easy", "Medium", "Hard"))  # HP hat nur 3 Stufen
  )

print(sprintf("HP Task (clean): %d Beobachtungen, %d Teams, %d Teilnehmer", 
              nrow(model_data_hp_clean),
              length(unique(model_data_hp_clean$team_id)),
              length(unique(model_data_hp_clean$participant.code))))

# Modell 3 (bestes Modell)
print("\n--- HP MODELL 3 (OUTLIER-BEREINIGT) ---")
model_hp_3_clean <- lmer(
  flow_score ~ synchronicity * difficulty + 
    fp_total + order +
    (1 | team_id) + (1 | participant.code),
  data = model_data_hp_clean
)
summary(model_hp_3_clean)

# ================================================================================
# HIDDEN PROFILE TASK - EMMEANS POST-HOC TESTS (OUTLIER-BEREINIGT)
# ================================================================================

print("\n=== HIDDEN PROFILE TASK - EMMEANS POST-HOC ANALYSEN (OUTLIER-BEREINIGT) ===")

# Between-Subjects Tests: Synchronicity Unterschiede für jede Difficulty
print("\n--- Between-Subjects Tests: Synchronicity|Difficulty (clean) ---")
bs_tests_hp_clean <- emmeans(model_hp_3_clean, specs = pairwise ~ synchronicity|difficulty, adjust = "none")$contrasts %>%
  as_tibble() %>%
  # Manuelle BH Korrektur
  mutate(p.adj = p.adjust(p.value, "BH")) %>%
  # Numerische Werte runden (neue Syntax)
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Between-Subjects Tests (Synchronicity Vergleiche pro Difficulty - clean):")
print(bs_tests_hp_clean)

# Within-Subjects Tests: Difficulty Unterschiede für jede Synchronicity
print("\n--- Within-Subjects Tests: Difficulty|Synchronicity (clean) ---")
ws_tests_hp_clean <- emmeans(model_hp_3_clean, specs = pairwise ~ difficulty|synchronicity, adjust = "none")$contrasts %>%
  as_tibble() %>%
  # Manuelle BH Korrektur
  mutate(p.adj = p.adjust(p.value, "BH")) %>%
  # Numerische Werte runden (neue Syntax)
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Within-Subjects Tests (Difficulty Vergleiche pro Synchronicity - clean):")
print(ws_tests_hp_clean)

# Marginal Means zur Interpretation
print("\n--- Marginal Means für HP Task (clean) ---")
marginal_means_hp_clean <- emmeans(model_hp_3_clean, ~ synchronicity * difficulty) %>%
  as_tibble() %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))
print(marginal_means_hp_clean)

# ================================================================================
# VERGLEICH: ORIGINAL vs OUTLIER-BEREINIGT
# ================================================================================

print("\n\n=== VERGLEICH: ORIGINAL vs OUTLIER-BEREINIGT ===")

# Funktion zum Vergleich von Between-Subjects Effekten
compare_bs_effects <- function(original, clean, task_name) {
  cat(sprintf("\n--- %s Task: BS-Effekte Vergleich ---\n", task_name))
  
  # Kombiniere Datensätze mit korrigierter rename Syntax
  comparison <- original %>%
    select(difficulty, estimate, p.value, p.adj) %>%
    dplyr::rename(original_estimate = estimate, original_p = p.value, original_p.adj = p.adj) %>%
    left_join(
      clean %>%
        select(difficulty, estimate, p.value, p.adj) %>%
        dplyr::rename(clean_estimate = estimate, clean_p = p.value, clean_p.adj = p.adj),
      by = "difficulty"
    ) %>%
    mutate(
      estimate_diff = clean_estimate - original_estimate,
      p_change = clean_p.adj < 0.05 & original_p.adj >= 0.05 | clean_p.adj >= 0.05 & original_p.adj < 0.05
    )
  
  print("Vergleich Between-Subjects Effekte (Original vs Clean):")
  print(comparison)
  
  # Zeige bedeutsame Änderungen
  if (any(comparison$p_change, na.rm = TRUE)) {
    cat("ACHTUNG: Signifikanzänderungen nach Outlier-Bereinigung!\n")
  }
  
  return(comparison)
}

# Vergleiche die Ergebnisse (falls Original-Ergebnisse verfügbar)
if (exists("bs_tests_math") && exists("bs_tests_hp")) {
  math_comparison <- compare_bs_effects(bs_tests_math, bs_tests_math_clean, "Math")
  hp_comparison <- compare_bs_effects(bs_tests_hp, bs_tests_hp_clean, "HP")
}

# ================================================================================
# ZUSAMMENFASSUNG FÜR OUTLIER-BEREINIGTE ANALYSEN
# ================================================================================

print("\n=== ZUSAMMENFASSUNG - OUTLIER-BEREINIGTE ANALYSEN ===")

# Sammle signifikante Ergebnisse für outlier-bereinigte Daten
print("\n--- Signifikante Between-Subjects Effekte (CLEAN) ---")

# Math Task signifikante BS Effekte
math_significant_bs_clean <- bs_tests_math_clean %>%
  filter(p.adj < 0.05) %>%
  mutate(Task = "Math")

# HP Task signifikante BS Effekte  
hp_significant_bs_clean <- bs_tests_hp_clean %>%
  filter(p.adj < 0.05) %>%
  mutate(Task = "HP")

# Kombiniere signifikante Ergebnisse
all_significant_bs_clean <- bind_rows(math_significant_bs_clean, hp_significant_bs_clean)

if (nrow(all_significant_bs_clean) > 0) {
  print("Signifikante Synchronicity Unterschiede (clean data):")
  print(all_significant_bs_clean %>% select(Task, difficulty, estimate, p.value, p.adj))
} else {
  print("Keine signifikanten Synchronicity Unterschiede in bereinigten Daten gefunden.")
}

```

Comparison of communication media (NONE/CHAT/JITSI)

```{r}
# ================================================================================
# TEIL 1: DATENAUFBEREITUNG UND -ZUSAMMENFÜHRUNG
# ================================================================================

# Aktuelle Daten: Nur Math Task
current_data <- flow_scores %>%
  filter(task == "Math") %>%
  select(participant.code, team_id, difficulty, comm, flow_score, order, fp_total) %>%
  # Spaltennamen für Konsistenz anpassen
  dplyr::rename(
    participant_id = participant.code,
    session_id = team_id
  ) %>%
  # Communication Medium als kategoriale Variable
  mutate(
    comm_type = case_when(
      comm == "Jitsi" ~ "Video",
      comm == "Chat" ~ "Chat"
    ),
    data_source = "Current"
  ) %>%
  select(-comm)  # Original comm Spalte entfernen

print(sprintf("Aktuelle Daten: %d Beobachtungen, %d Teams, %d Teilnehmer",
              nrow(current_data),
              length(unique(current_data$session_id)),
              length(unique(current_data$participant_id))))

# Historische Daten: Nur MP Treatment
historical_data_prep <- data_old %>%
  filter(Treatment == "MP") %>%  # Nur Multi-Player
  select(SessionID, SubjectID, Condition, Order, flowFKS_9, flowProne.General)

# Erstelle Order-Strings für historische Daten (analog zu flow_scores)
print("Erstelle Order-Strings für historische Daten...")
historical_order_mapping <- historical_data_prep %>%
  select(SessionID, Condition, Order) %>%
  distinct() %>%
  # Condition zu Difficulty-Buchstaben mapping (entsprechend deinem Recode)
  mutate(
    difficulty_code = case_when(
      Condition == 1 ~ "B",  # Easy
      Condition == 2 ~ "A",  # Optimal_Selected
      Condition == 3 ~ "F",  # Optimal_Calibrated  
      Condition == 4 ~ "O"   # Hard
    )
  ) %>%
  # Sortiere nach Order um die richtige Reihenfolge zu bekommen
  arrange(SessionID, Order) %>%
  # Erstelle Order-String für jedes Team
  group_by(SessionID) %>%
  summarise(
    order_string = paste0("['", paste(difficulty_code, collapse = "', '"), "']"),
    .groups = "drop"
  )

print("Beispiel Order-Mappings:")
print(head(historical_order_mapping, 5))

# Jetzt die historischen Daten mit Order-Strings verknüpfen
historical_data <- historical_data_prep %>%
  # Spaltennamen für Konsistenz anpassen
  dplyr::rename(
    session_id = SessionID,
    participant_id = SubjectID,
    condition_num = Condition,
    order_position = Order,
    flow_score = flowFKS_9,
    fp_total = flowProne.General
  ) %>%
  # Order-String hinzufügen
  left_join(historical_order_mapping, by = c("session_id" = "SessionID")) %>%
  # Condition Numbers zu Difficulty Labels konvertieren (entsprechend deinem Mapping)
  mutate(
    difficulty = case_when(
      condition_num == 1 ~ "Easy",           # B
      condition_num == 2 ~ "Optimal_Selected",  # A
      condition_num == 3 ~ "Optimal_Calibrated", # F
      condition_num == 4 ~ "Hard"            # O
    ),
    comm_type = "None",  # Keine Kommunikation im alten Experiment
    data_source = "Historical"
  ) %>%
  # Order-String als order-Spalte verwenden (analog zu aktuellen Daten)
  dplyr::rename(order = order_string) %>%
  select(-condition_num, -order_position)

print(sprintf("Historische Daten: %d Beobachtungen, %d Teams, %d Teilnehmer",
              nrow(historical_data),
              length(unique(historical_data$session_id)),
              length(unique(historical_data$participant_id))))

# Kombiniere beide Datensätze
combined_data <- bind_rows(current_data, historical_data) %>%
  # Faktoren für Modellierung erstellen
  mutate(
    difficulty = factor(difficulty, levels = c("Easy", "Optimal_Selected", "Optimal_Calibrated", "Hard")),
    comm_type = factor(comm_type, levels = c("None", "Chat", "Video")),
    data_source = factor(data_source, levels = c("Historical", "Current"))
  )

print(sprintf("Kombinierte Daten: %d Beobachtungen gesamt",
              nrow(combined_data)))

# Überblick über die Datenverteilung
print("\n--- Datenverteilung nach Communication Type ---")
print(table(combined_data$comm_type, combined_data$data_source))

print("\n--- Datenverteilung nach Difficulty ---")
print(table(combined_data$difficulty, combined_data$comm_type))

# ================================================================================
# TEIL 2: DESKRIPTIVE STATISTIKEN
# ================================================================================

print("\n=== DESKRIPTIVE STATISTIKEN ===")

# Grundstatistiken nach Communication Type
descriptive_stats <- combined_data %>%
  group_by(comm_type, data_source) %>%
  summarise(
    n_obs = n(),
    n_teams = length(unique(session_id)),
    n_participants = length(unique(participant_id)),
    mean_flow = mean(flow_score, na.rm = TRUE),
    sd_flow = sd(flow_score, na.rm = TRUE),
    mean_fp = mean(fp_total, na.rm = TRUE),
    sd_fp = sd(fp_total, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(across(where(is.numeric), ~ round(.x, 3)))

print("Deskriptive Statistiken nach Communication Type:")
print(descriptive_stats)

# Detaillierte Statistiken nach Difficulty
detailed_stats <- combined_data %>%
  group_by(comm_type, difficulty) %>%
  summarise(
    n_obs = n(),
    mean_flow = mean(flow_score, na.rm = TRUE),
    sd_flow = sd(flow_score, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(across(where(is.numeric), ~ round(.x, 3)))

print("\n--- Flow Scores nach Communication Type und Difficulty ---")
print(detailed_stats)

# ================================================================================
# TEIL 3: REGRESSIONSMODELLE FÜR VERGLEICH
# ================================================================================

print("\n=== REGRESSIONSANALYSE - HISTORISCHER VERGLEICH ===")

# Modell 1: Nur Communication Type (Haupteffekt)
print("\n--- MODELL 1: Communication Type Haupteffekt ---")
model_historical_1 <- lmer(
  flow_score ~ comm_type + 
    (1 | session_id) + (1 | participant_id),
  data = combined_data
)
summary(model_historical_1)

# Modell 2: Communication Type + Difficulty
print("\n--- MODELL 2: Communication Type + Difficulty ---")
model_historical_2 <- lmer(
  flow_score ~ comm_type + difficulty + 
    (1 | session_id) + (1 | participant_id),
  data = combined_data
)
summary(model_historical_2)

# Modell 3: Mit Interaktion
print("\n--- MODELL 3: Communication Type × Difficulty Interaktion ---")
model_historical_3 <- lmer(
  flow_score ~ comm_type * difficulty + 
    (1 | session_id) + (1 | participant_id),
  data = combined_data
)
summary(model_historical_3)

# Modell 4: Vollständiges Modell mit Flow Proneness
print("\n--- MODELL 4: Vollständiges Modell mit Covariaten ---")
model_historical_4 <- lmer(
  flow_score ~ comm_type * difficulty + fp_total +
    (1 | session_id) + (1 | participant_id),
  data = combined_data
)
summary(model_historical_4)

# Modellvergleich
print("\n--- MODELLVERGLEICH ---")
historical_aic <- AIC(model_historical_1, model_historical_2, model_historical_3, model_historical_4)
historical_bic <- BIC(model_historical_1, model_historical_2, model_historical_3, model_historical_4)

print("AIC Vergleich:")
print(historical_aic)
print("BIC Vergleich:")
print(historical_bic)

# ================================================================================
# TEIL 4: EMMEANS POST-HOC TESTS
# ================================================================================

print("\n=== EMMEANS POST-HOC ANALYSEN - HISTORISCHER VERGLEICH ===")

# Bestes Modell für Post-Hoc Tests verwenden (anpassbar)
best_historical_model <- model_historical_4

# Communication Type Vergleiche (Paarweise)
print("\n--- Communication Type Paarvergleiche ---")
comm_comparisons <- emmeans(best_historical_model, specs = pairwise ~ comm_type, adjust = "tukey")$contrasts %>%
  as_tibble() %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Communication Type Vergleiche:")
print(comm_comparisons)

# Communication Type Vergleiche für jede Difficulty
print("\n--- Communication Type Vergleiche pro Difficulty ---")
comm_by_difficulty <- emmeans(best_historical_model, specs = pairwise ~ comm_type|difficulty, adjust = "tukey")$contrasts %>%
  as_tibble() %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Communication Type Vergleiche nach Difficulty:")
print(comm_by_difficulty)

# Marginal Means für Interpretation
print("\n--- Marginal Means: Communication Type × Difficulty ---")
historical_marginal_means <- emmeans(best_historical_model, ~ comm_type * difficulty) %>%
  as_tibble() %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))
print(historical_marginal_means)

# ================================================================================
# TEIL 5: SPEZIFISCHE VERGLEICHE
# ================================================================================

print("\n=== SPEZIFISCHE HISTORISCHE VERGLEICHE ===")

# 1. None vs. Current Communication (Chat + Video combined)
print("\n--- None vs. Current Communication (kombiniert) ---")

# Erstelle eine neue Gruppierungsvariable
combined_data_grouped <- combined_data %>%
  mutate(
    comm_era = case_when(
      comm_type == "None" ~ "Historical_NoComm",
      comm_type %in% c("Chat", "Video") ~ "Current_WithComm"
    )
  )

model_era_comparison <- lmer(
  flow_score ~ comm_era * difficulty + fp_total +
    (1 | session_id) + (1 | participant_id),
  data = combined_data_grouped
)

era_comparisons <- emmeans(model_era_comparison, specs = pairwise ~ comm_era, adjust = "none")$contrasts %>%
  as_tibble() %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Historical vs. Current Era Vergleich:")
print(era_comparisons)

# 2. Specific Focus: None vs Video, None vs Chat
print("\n--- Spezifische Paarvergleiche: None vs Chat vs Video ---")

# Filtere für spezielle Vergleiche
specific_comparisons <- comm_comparisons %>%
  filter(
    grepl("None.*Chat|Chat.*None|None.*Video|Video.*None", contrast)
  )

if (nrow(specific_comparisons) > 0) {
  print("Wichtige Vergleiche (None vs. Kommunikationsmedien):")
  print(specific_comparisons)
}

# ================================================================================
# TEIL 6: ZUSAMMENFASSUNG UND INTERPRETATION
# ================================================================================

print("\n=== ZUSAMMENFASSUNG HISTORISCHER VERGLEICH ===")

# Signifikante Communication Type Effekte
significant_comm_effects <- comm_comparisons %>%
  filter(p.value < 0.05)

if (nrow(significant_comm_effects) > 0) {
  print("Signifikante Communication Type Unterschiede:")
  print(significant_comm_effects %>% select(contrast, estimate, p.value))
} else {
  print("Keine signifikanten Communication Type Unterschiede gefunden.")
}

# Effect Sizes für Interpretation
print("\n--- Effect Sizes (Cohen's d approximation) ---")
pooled_sd <- sd(combined_data$flow_score, na.rm = TRUE)

effect_sizes <- comm_comparisons %>%
  mutate(
    cohens_d = abs(estimate) / pooled_sd,
    effect_magnitude = case_when(
      cohens_d < 0.2 ~ "Negligible",
      cohens_d < 0.5 ~ "Small", 
      cohens_d < 0.8 ~ "Medium",
      TRUE ~ "Large"
    )
  ) %>%
  select(contrast, estimate, cohens_d, effect_magnitude)

print("Effect Sizes für Communication Type Vergleiche:")
print(effect_sizes)
```

Comparision to single player treatment

```{r}
# Vollständiger historischer Vergleich: Single Player vs Multi Player vs Current (Chat/Video)
# Vier-Weg Vergleich: Allein vs Zusammen (ohne Kommunikation) vs Chat vs Video

library(dplyr)
library(lme4)
library(emmeans)
library(tibble)

print("=== VOLLSTÄNDIGER HISTORISCHER VERGLEICH ===")
print("Single Player vs Multi Player vs Current Communication")

# ================================================================================
# TEIL 1: ERWEITERTE DATENAUFBEREITUNG
# ================================================================================

# Aktuelle Daten: Nur Math Task (bereits vorbereitet, aber nochmal für Vollständigkeit)
current_data_extended <- flow_scores %>%
  filter(task == "Math") %>%
  select(participant.code, team_id, difficulty, comm, flow_score, order, fp_total) %>%
  dplyr::rename(
    participant_id = participant.code,
    session_id = team_id
  ) %>%
  mutate(
    communication_condition = case_when(
      comm == "Jitsi" ~ "Together_Video",
      comm == "Chat" ~ "Together_Chat"
    ),
    data_source = "Current"
  ) %>%
  select(-comm)

print(sprintf("Aktuelle Daten: %d Beobachtungen, %d Teams, %d Teilnehmer",
              nrow(current_data_extended),
              length(unique(current_data_extended$session_id)),
              length(unique(current_data_extended$participant_id))))

# Historische Daten: ALLE Treatments (SP + MP)
historical_data_prep_all <- data_old %>%
  select(SessionID, SubjectID, Treatment, Condition, Order, flowFKS_9, flowProne.General)

# Order-Strings für ALLE historischen Teams erstellen
historical_order_mapping_all <- historical_data_prep_all %>%
  select(SessionID, Condition, Order) %>%
  distinct() %>%
  mutate(
    difficulty_code = case_when(
      Condition == 1 ~ "B",  # Easy
      Condition == 2 ~ "A",  # Optimal_Selected
      Condition == 3 ~ "F",  # Optimal_Calibrated  
      Condition == 4 ~ "O"   # Hard
    )
  ) %>%
  arrange(SessionID, Order) %>%
  group_by(SessionID) %>%
  summarise(
    order_string = paste0("['", paste(difficulty_code, collapse = "', '"), "']"),
    .groups = "drop"
  )

# Historische Daten mit beiden Treatments
historical_data_extended <- historical_data_prep_all %>%
  dplyr::rename(
    session_id = SessionID,
    participant_id = SubjectID,
    treatment = Treatment,
    condition_num = Condition,
    order_position = Order,
    flow_score = flowFKS_9,
    fp_total = flowProne.General
  ) %>%
  left_join(historical_order_mapping_all, by = c("session_id" = "SessionID")) %>%
  mutate(
    difficulty = case_when(
      condition_num == 1 ~ "Easy",
      condition_num == 2 ~ "Optimal_Selected", 
      condition_num == 3 ~ "Optimal_Calibrated",
      condition_num == 4 ~ "Hard"
    ),
    communication_condition = case_when(
      treatment == "SP" ~ "Alone",
      treatment == "MP" ~ "Together_None"
    ),
    data_source = "Historical"
  ) %>%
  dplyr::rename(order = order_string) %>%
  select(-condition_num, -order_position, -treatment)

print(sprintf("Historische Daten: %d Beobachtungen, %d Teams, %d Teilnehmer",
              nrow(historical_data_extended),
              length(unique(historical_data_extended$session_id)),
              length(unique(historical_data_extended$participant_id))))

# Kombiniere ALLE Daten
comprehensive_data <- bind_rows(current_data_extended, historical_data_extended) %>%
  mutate(
    difficulty = factor(difficulty, levels = c("Easy", "Optimal_Selected", "Optimal_Calibrated", "Hard")),
    communication_condition = factor(communication_condition, 
                                   levels = c("Alone", "Together_None", "Together_Chat", "Together_Video")),
    data_source = factor(data_source, levels = c("Historical", "Current"))
  )

print(sprintf("Kombinierte Daten: %d Beobachtungen gesamt", nrow(comprehensive_data)))

# ================================================================================
# TEIL 2: UMFASSENDE DESKRIPTIVE STATISTIKEN
# ================================================================================

print("\n=== UMFASSENDE DESKRIPTIVE STATISTIKEN ===")

# Grundstatistiken nach Communication Condition
comprehensive_stats <- comprehensive_data %>%
  group_by(communication_condition, data_source) %>%
  summarise(
    n_obs = n(),
    n_sessions = length(unique(session_id)),
    n_participants = length(unique(participant_id)),
    mean_flow = mean(flow_score, na.rm = TRUE),
    sd_flow = sd(flow_score, na.rm = TRUE),
    median_flow = median(flow_score, na.rm = TRUE),
    mean_fp = mean(fp_total, na.rm = TRUE),
    sd_fp = sd(fp_total, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(across(where(is.numeric), ~ round(.x, 3)))

print("Umfassende Statistiken nach Communication Condition:")
print(comprehensive_stats)

# Vereinfachte Übersicht (kombiniert über data_source)
simple_stats <- comprehensive_data %>%
  group_by(communication_condition) %>%
  summarise(
    n_total = n(),
    mean_flow = mean(flow_score, na.rm = TRUE),
    sd_flow = sd(flow_score, na.rm = TRUE),
    ci_lower = mean_flow - 1.96 * sd_flow / sqrt(n()),
    ci_upper = mean_flow + 1.96 * sd_flow / sqrt(n()),
    .groups = "drop"
  ) %>%
  mutate(across(where(is.numeric), ~ round(.x, 3)))

print("\nVereinfachte Übersicht (alle Conditions):")
print(simple_stats)

# Datenverteilung prüfen
print("\n--- Datenverteilung ---")
print("Nach Communication Condition:")
print(table(comprehensive_data$communication_condition))

print("\nNach Communication Condition und Data Source:")
print(table(comprehensive_data$communication_condition, comprehensive_data$data_source))

print("\nNach Difficulty und Communication Condition:")
difficulty_table <- table(comprehensive_data$difficulty, comprehensive_data$communication_condition)
print(difficulty_table)

# ================================================================================
# TEIL 3: UMFASSENDE REGRESSIONSANALYSE
# ================================================================================

print("\n=== UMFASSENDE REGRESSIONSANALYSE ===")

# Modell 1: Nur Communication Condition
print("\n--- MODELL 1: Communication Condition Haupteffekt ---")
model_comprehensive_1 <- lmer(
  flow_score ~ communication_condition + 
    (1 | session_id) + (1 | participant_id),
  data = comprehensive_data
)
summary(model_comprehensive_1)

# Modell 2: Communication Condition + Difficulty
print("\n--- MODELL 2: Communication Condition + Difficulty ---")
model_comprehensive_2 <- lmer(
  flow_score ~ communication_condition + difficulty + 
    (1 | session_id) + (1 | participant_id),
  data = comprehensive_data
)
summary(model_comprehensive_2)

# Modell 3: Mit Interaktion
print("\n--- MODELL 3: Communication Condition × Difficulty ---")
model_comprehensive_3 <- lmer(
  flow_score ~ communication_condition * difficulty + 
    (1 | session_id) + (1 | participant_id),
  data = comprehensive_data
)
summary(model_comprehensive_3)

# Modell 4: Vollständiges Modell mit Flow Proneness
print("\n--- MODELL 4: Vollständiges Modell ---")
model_comprehensive_4 <- lmer(
  flow_score ~ communication_condition * difficulty + fp_total +
    (1 | session_id) + (1 | participant_id),
  data = comprehensive_data
)
summary(model_comprehensive_4)

# Modellvergleich
print("\n--- MODELLVERGLEICH ---")
comprehensive_aic <- AIC(model_comprehensive_1, model_comprehensive_2, model_comprehensive_3, model_comprehensive_4)
comprehensive_bic <- BIC(model_comprehensive_1, model_comprehensive_2, model_comprehensive_3, model_comprehensive_4)

print("AIC Vergleich:")
print(comprehensive_aic)
print("BIC Vergleich:")
print(comprehensive_bic)

# ================================================================================
# TEIL 4: EMMEANS PAARVERGLEICHE
# ================================================================================

print("\n=== EMMEANS PAARVERGLEICHE - ALLE CONDITIONS ===")

# Bestes Modell verwenden
best_comprehensive_model <- model_comprehensive_4

# Alle paarweisen Vergleiche zwischen Communication Conditions
print("\n--- Alle Communication Condition Paarvergleiche ---")
all_comm_comparisons <- emmeans(best_comprehensive_model, specs = pairwise ~ communication_condition, adjust = "tukey")$contrasts %>%
  as_tibble() %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Alle Communication Condition Vergleiche:")
print(all_comm_comparisons)

# Spezifische Vergleiche von Interesse
print("\n--- Spezifische Vergleiche von Interesse ---")

key_comparisons <- all_comm_comparisons %>%
  filter(
    grepl("Alone.*Together", contrast) |
    grepl("Together_None.*Together_Chat", contrast) |
    grepl("Together_None.*Together_Video", contrast) |
    grepl("Together_Chat.*Together_Video", contrast)
  )

if (nrow(key_comparisons) > 0) {
  print("Wichtige Vergleiche:")
  print(key_comparisons %>% select(contrast, estimate, p.value))
}

# Marginal Means für alle Conditions
print("\n--- Marginal Means: Communication Conditions ---")
comprehensive_marginal_means <- emmeans(best_comprehensive_model, ~ communication_condition) %>%
  as_tibble() %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))
print(comprehensive_marginal_means)

# Communication Condition × Difficulty Marginal Means
print("\n--- Marginal Means: Communication × Difficulty ---")
interaction_marginal_means <- emmeans(best_comprehensive_model, ~ communication_condition * difficulty) %>%
  as_tibble() %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))
print(interaction_marginal_means)

# ================================================================================
# TEIL 5: EFFEKTGRÖSSEN UND PRAKTISCHE BEDEUTSAMKEIT
# ================================================================================

print("\n=== EFFEKTGRÖSSEN UND PRAKTISCHE BEDEUTSAMKEIT ===")

# Berechne Effect Sizes (Cohen's d approximation)
pooled_sd_comprehensive <- sd(comprehensive_data$flow_score, na.rm = TRUE)

effect_sizes_comprehensive <- all_comm_comparisons %>%
  mutate(
    cohens_d = abs(estimate) / pooled_sd_comprehensive,
    effect_magnitude = case_when(
      cohens_d < 0.2 ~ "Vernachlässigbar",
      cohens_d < 0.5 ~ "Klein", 
      cohens_d < 0.8 ~ "Mittel",
      TRUE ~ "Groß"
    ),
    practical_significance = case_when(
      p.value < 0.001 & cohens_d >= 0.5 ~ "Hoch signifikant + praktisch relevant",
      p.value < 0.05 & cohens_d >= 0.5 ~ "Signifikant + praktisch relevant",
      p.value < 0.05 & cohens_d >= 0.2 ~ "Signifikant + kleiner Effekt",
      p.value >= 0.05 ~ "Nicht signifikant",
      TRUE ~ "Signifikant aber vernachlässigbare Effektgröße"
    )
  ) %>%
  select(contrast, estimate, p.value, cohens_d, effect_magnitude, practical_significance)

print("Effect Sizes für alle Communication Condition Vergleiche:")
print(effect_sizes_comprehensive)

# ================================================================================
# TEIL 6: ZUSAMMENFASSUNG UND INTERPRETATION
# ================================================================================

print("\n=== ZUSAMMENFASSUNG: VIER-WEG VERGLEICH ===")

# Flow Score Ranking
flow_ranking <- comprehensive_marginal_means %>%
  arrange(desc(emmean)) %>%
  select(communication_condition, emmean, SE) %>%
  mutate(
    rank = row_number(),
    condition_german = case_when(
      communication_condition == "Alone" ~ "Allein",
      communication_condition == "Together_None" ~ "Zusammen (keine Kommunikation)",
      communication_condition == "Together_Chat" ~ "Zusammen (Chat)",
      communication_condition == "Together_Video" ~ "Zusammen (Video)"
    )
  )

print("Flow Score Ranking (höchste zu niedrigste):")
print(flow_ranking %>% select(rank, condition_german, emmean, SE))

# Signifikante Unterschiede zusammenfassen
significant_comparisons <- effect_sizes_comprehensive %>%
  filter(p.value < 0.05) %>%
  arrange(p.value)

print("\nSignifikante Unterschiede (nach p-Wert sortiert):")
if (nrow(significant_comparisons) > 0) {
  print(significant_comparisons %>% select(contrast, estimate, p.value, effect_magnitude))
} else {
  print("Keine signifikanten Unterschiede gefunden.")
}

```

ANOVA for treatment effects

```{r}
# ================================================================================
   # UMFASSENDE ANOVA-ANALYSE
# ================================================================================

library(car)        # Für Levene-Test und Typ II/III ANOVA
library(effectsize) # Für Effektgrößen (eta-squared, omega-squared)
library(multcomp)   # Für erweiterte Post-hoc Tests
library(nortest)    # Für erweiterte Normalitätstests

print("\n=== UMFASSENDE ANOVA-ANALYSE ===")

# ================================================================================
# SCHRITT 1: ANNAHMEN-ÜBERPRÜFUNG
# ================================================================================

print("\n--- ANNAHMEN-ÜBERPRÜFUNG ---")

# 1. Normalitätstest
print("\n1. NORMALITÄTSTEST")

# Shapiro-Wilk Test (für kleinere Stichproben pro Gruppe)
normality_by_group <- comprehensive_data %>%
  group_by(communication_condition) %>%
  summarise(
    n = n(),
    shapiro_w = ifelse(n >= 3 & n <= 5000, shapiro.test(flow_score)$statistic, NA),
    shapiro_p = ifelse(n >= 3 & n <= 5000, shapiro.test(flow_score)$p.value, NA),
    .groups = "drop"
  )

print("Normalitätstest pro Communication Condition:")
print(normality_by_group)

# Anderson-Darling Test für größere Gruppen
print("\nAnderson-Darling Normalitätstest (gesamt):")
ad_test <- nortest::ad.test(comprehensive_data$flow_score)
print(paste("A =", round(ad_test$statistic, 4), ", p =", round(ad_test$p.value, 4)))

# QQ-Plot Daten für visuelle Inspektion vorbereiten
qq_data <- comprehensive_data %>%
  group_by(communication_condition) %>%
  arrange(flow_score) %>%
  mutate(
    theoretical_quantile = qnorm(ppoints(n())),
    sample_quantile = flow_score
  ) %>%
  ungroup()

print("QQ-Plot Daten bereit für visuelle Inspektion")

# 2. Homogenität der Varianzen
print("\n2. HOMOGENITÄT DER VARIANZEN")

# Levene-Test
levene_test <- leveneTest(flow_score ~ communication_condition, data = comprehensive_data)
print("Levene-Test für Varianzhomogenität:")
print(levene_test)

# Bartlett-Test (sensitiver für Normalitätsabweichungen)
bartlett_test <- bartlett.test(flow_score ~ communication_condition, data = comprehensive_data)
print("Bartlett-Test für Varianzhomogenität:")
print(paste("Chi-squared =", round(bartlett_test$statistic, 4), 
           ", df =", bartlett_test$parameter, 
           ", p =", round(bartlett_test$p.value, 4)))

# Deskriptive Statistiken pro Gruppe
variance_stats <- comprehensive_data %>%
  group_by(communication_condition) %>%
  summarise(
    n = n(),
    mean = mean(flow_score, na.rm = TRUE),
    sd = sd(flow_score, na.rm = TRUE),
    variance = var(flow_score, na.rm = TRUE),
    min_val = min(flow_score, na.rm = TRUE),
    max_val = max(flow_score, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(across(where(is.numeric) & !matches("n"), ~ round(.x, 3)))

print("\nDeskriptive Statistiken pro Communication Condition:")
print(variance_stats)

# Varianzenverhältnis prüfen (Faustregel: größte/kleinste Varianz < 4)
variance_ratio <- max(variance_stats$variance) / min(variance_stats$variance)
print(paste("Varianzenverhältnis (max/min):", round(variance_ratio, 3)))
print(paste("Homogenitätsannahme erfüllt (< 4):", variance_ratio < 4))

# ================================================================================
# SCHRITT 2: EINFAKTORIELLE ANOVA - COMMUNICATION CONDITION
# ================================================================================

print("\n--- EINFAKTORIELLE ANOVA: COMMUNICATION CONDITION ---")

# Standard ANOVA
anova_comm <- aov(flow_score ~ communication_condition, data = comprehensive_data)
anova_comm_summary <- summary(anova_comm)
print("Einfaktorielle ANOVA - Communication Condition:")
print(anova_comm_summary)

# Typ II ANOVA (robuster bei unbalancierten Designs)
anova_comm_type2 <- Anova(anova_comm, type = "II")
print("\nTyp II ANOVA - Communication Condition:")
print(anova_comm_type2)

# Effektgrößen
eta_squared_comm <- eta_squared(anova_comm, partial = TRUE)
omega_squared_comm <- omega_squared(anova_comm, partial = TRUE)

print("\nEffektgrößen - Communication Condition:")
print("Eta-squared (partiell):")
print(eta_squared_comm)
print("Omega-squared (partiell):")
print(omega_squared_comm)

# ================================================================================
# SCHRITT 3: ZWEIFAKTORIELLE ANOVA - COMMUNICATION × DIFFICULTY  
# ================================================================================

print("\n--- ZWEIFAKTORIELLE ANOVA: COMMUNICATION × DIFFICULTY ---")

# Überprüfe Zellenbesetzung
cell_counts <- table(comprehensive_data$communication_condition, comprehensive_data$difficulty)
print("Zellenbesetzung (Communication × Difficulty):")
print(cell_counts)

# Identifiziere leere Zellen
empty_cells <- which(cell_counts == 0, arr.ind = TRUE)
if(nrow(empty_cells) > 0) {
  print("WARNUNG: Leere Zellen gefunden!")
  print(empty_cells)
} else {
  print("Keine leeren Zellen - Design ist balanciert genug für ANOVA")
}

# Zweifaktorielle ANOVA
anova_full <- aov(flow_score ~ communication_condition * difficulty, data = comprehensive_data)
anova_full_summary <- summary(anova_full)
print("\nZweifaktorielle ANOVA (Communication × Difficulty):")
print(anova_full_summary)

# Typ III ANOVA (für unbalancierte Designs mit Interaktionen)
anova_full_type3 <- Anova(anova_full, type = "III")
print("\nTyp III ANOVA - Communication × Difficulty:")
print(anova_full_type3)

# Effektgrößen für alle Faktoren
eta_squared_full <- eta_squared(anova_full, partial = TRUE)
omega_squared_full <- omega_squared(anova_full, partial = TRUE)

print("\nEffektgrößen - Vollständiges Modell:")
print("Eta-squared (partiell):")
print(eta_squared_full)
print("Omega-squared (partiell):")
print(omega_squared_full)

# ================================================================================
# SCHRITT 4: ANCOVA - MIT FLOW PRONENESS
# ================================================================================

print("\n--- ANCOVA: COMMUNICATION × DIFFICULTY + FLOW PRONENESS ---")

# ANCOVA mit Flow Proneness als Kovariate
ancova_model <- aov(flow_score ~ communication_condition * difficulty + fp_total, 
                   data = comprehensive_data)
ancova_summary <- summary(ancova_model)
print("ANCOVA mit Flow Proneness:")
print(ancova_summary)

# Typ III ANCOVA
ancova_type3 <- Anova(ancova_model, type = "III")
print("\nTyp III ANCOVA:")
print(ancova_type3)

# Effektgrößen ANCOVA
eta_squared_ancova <- eta_squared(ancova_model, partial = TRUE)
print("\nEffektgrößen ANCOVA:")
print(eta_squared_ancova)

# ================================================================================
# SCHRITT 5: POST-HOC TESTS
# ================================================================================

print("\n--- POST-HOC TESTS ---")

# Tukey HSD für Communication Condition
print("\n1. TUKEY HSD - COMMUNICATION CONDITION")
tukey_comm <- TukeyHSD(anova_comm)
print("Tukey HSD Ergebnisse:")
print(tukey_comm)

# Tukey HSD für vollständiges Modell (falls Interaktion signifikant)
if(anova_full_summary[[1]][3, 5] < 0.05) {  # Interaktion p-Wert
  print("\n2. TUKEY HSD - COMMUNICATION × DIFFICULTY (da Interaktion signifikant)")
  tukey_full <- TukeyHSD(anova_full, which = "communication_condition:difficulty")
  print(tukey_full)
} else {
  print("\n2. Interaktion nicht signifikant - keine Post-hoc Tests für Interaktion nötig")
}

# Bonferroni-korrigierte paarweise t-Tests als Alternative
print("\n3. BONFERRONI-KORRIGIERTE PAARWEISE T-TESTS")
pairwise_t <- pairwise.t.test(comprehensive_data$flow_score, 
                             comprehensive_data$communication_condition, 
                             p.adjust.method = "bonferroni")
print("Bonferroni-korrigierte paarweise t-Tests:")
print(pairwise_t)

# ================================================================================
# SCHRITT 6: ROBUSTHEITSANALYSEN
# ================================================================================

print("\n--- ROBUSTHEITSANALYSEN ---")

# Welch-ANOVA (robust gegen Varianzheteroskedastizität)
print("\n1. WELCH-ANOVA (robust gegen ungleiche Varianzen)")
welch_anova <- oneway.test(flow_score ~ communication_condition, 
                          data = comprehensive_data, 
                          var.equal = FALSE)
print(welch_anova)

# Kruskal-Wallis Test (nicht-parametrische Alternative)
print("\n2. KRUSKAL-WALLIS TEST (nicht-parametrisch)")
kruskal_test <- kruskal.test(flow_score ~ communication_condition, 
                           data = comprehensive_data)
print(kruskal_test)

# Bei signifikantem Kruskal-Wallis: Dunn-Test für Post-hoc
if(kruskal_test$p.value < 0.05) {
  print("\n3. DUNN-TEST (Post-hoc für Kruskal-Wallis)")
  # Vereinfachter paarweiser Wilcoxon-Test mit Bonferroni-Korrektur
  dunn_alternative <- pairwise.wilcox.test(comprehensive_data$flow_score, 
                                         comprehensive_data$communication_condition,
                                         p.adjust.method = "bonferroni")
  print("Paarweise Wilcoxon-Tests (Bonferroni-korrigiert):")
  print(dunn_alternative)
}

# ================================================================================
# SCHRITT 7: MODELLVERGLEICH UND ZUSAMMENFASSUNG
# ================================================================================

print("\n--- MODELLVERGLEICH: ANOVA vs MIXED-EFFECTS ---")

# AIC/BIC Vergleich zwischen ANOVA und Mixed-Effects Modellen
anova_lm <- lm(flow_score ~ communication_condition * difficulty + fp_total, 
               data = comprehensive_data)

print("Modellvergleich (AIC/BIC):")
print("Mixed-Effects Modell (aus vorheriger Analyse):")
print(paste("AIC:", round(AIC(model_comprehensive_4), 2)))
print(paste("BIC:", round(BIC(model_comprehensive_4), 2)))

print("Standard lineares Modell (ANCOVA):")
print(paste("AIC:", round(AIC(anova_lm), 2)))
print(paste("BIC:", round(BIC(anova_lm), 2)))

# R-squared für ANOVA-Modelle
r_squared_comm <- summary(lm(flow_score ~ communication_condition, data = comprehensive_data))$r.squared
r_squared_full <- summary(lm(flow_score ~ communication_condition * difficulty, data = comprehensive_data))$r.squared  
r_squared_ancova <- summary(anova_lm)$r.squared

print("\nErklärte Varianz (R²):")
print(paste("Nur Communication Condition:", round(r_squared_comm, 4)))
print(paste("Communication × Difficulty:", round(r_squared_full, 4)))
print(paste("ANCOVA (+ Flow Proneness):", round(r_squared_ancova, 4)))

# ================================================================================
# SCHRITT 8: ZUSAMMENFASSUNG DER ANOVA-BEFUNDE
# ================================================================================

print("\n=== ZUSAMMENFASSUNG DER ANOVA-BEFUNDE ===")

# Hauptbefunde extrahieren
comm_f_stat <- anova_comm_summary[[1]][1, 4]  # F-Statistik
comm_p_val <- anova_comm_summary[[1]][1, 5]   # p-Wert
comm_eta_sq <- eta_squared_comm$Eta2_partial[1] # Eta-squared

print("HAUPTBEFUNDE:")
print(sprintf("1. Communication Condition Haupteffekt: F(%d,%d) = %.3f, p = %.4f, η²p = %.3f",
              anova_comm_summary[[1]][1, 1],  # df1
              anova_comm_summary[[1]][2, 1],  # df2  
              comm_f_stat, comm_p_val, comm_eta_sq))

if(comm_p_val < 0.001) {
  significance_level <- "hoch signifikant (p < .001)"
} else if(comm_p_val < 0.01) {
  significance_level <- "sehr signifikant (p < .01)"
} else if(comm_p_val < 0.05) {
  significance_level <- "signifikant (p < .05)"
} else {
  significance_level <- "nicht signifikant (p ≥ .05)"
}

effect_size_interpretation <- case_when(
  comm_eta_sq < 0.01 ~ "vernachlässigbar",
  comm_eta_sq < 0.06 ~ "klein", 
  comm_eta_sq < 0.14 ~ "mittel",
  TRUE ~ "groß"
)

print(sprintf("   Interpretation: %s mit %s Effekt", significance_level, effect_size_interpretation))

# Annahmen-Check Zusammenfassung
assumptions_met <- TRUE
assumption_violations <- c()

if(levene_test$`Pr(>F)`[1] < 0.05) {
  assumptions_met <- FALSE
  assumption_violations <- c(assumption_violations, "Varianzhomogenität verletzt")
}

if(ad_test$p.value < 0.05) {
  assumptions_met <- FALSE  
  assumption_violations <- c(assumption_violations, "Normalität verletzt")
}

print("\n2. ANNAHMEN-ÜBERPRÜFUNG:")
if(assumptions_met) {
  print("   Alle ANOVA-Annahmen erfüllt ✓")
  print("   → Parametrische ANOVA-Ergebnisse sind vertrauenswürdig")
} else {
  print(paste("   Verletzungen:", paste(assumption_violations, collapse = ", ")))
  print("   → Robustheitsanalysen (Welch-ANOVA, Kruskal-Wallis) beachten!")
}

# Konsistenz mit Mixed-Effects Check
print("\n3. KONSISTENZ MIT MIXED-EFFECTS MODELL:")
print("   (Detaillierte Vergleiche siehe vorherige Emmeans-Analyse)")
print("   → Bei ähnlichen p-Werten: Befunde robust über Analysemethoden hinweg")
print("   → Bei unterschiedlichen Ergebnissen: Mixed-Effects bevorzugen (berücksichtigt Datenstruktur)")

print("\n=== ENDE DER ANOVA-ANALYSE ===")
```

Shared flow calculation via Intra-class coefficient (univariate and multivariate)

```{r}
# ================================================================================
# TEIL 1: UNIVARIATE ICCs (nach Task UND Kommunikationsmedium getrennt)
# ================================================================================

print("\n--- UNIVARIATE ICCs ---")

# Daten aufteilen nach Task und Kommunikationsmedium
math_jitsi_data <- model_data_math %>% filter(comm == "Jitsi")
math_chat_data <- model_data_math %>% filter(comm == "Chat")
hp_jitsi_data <- model_data_hp %>% filter(comm == "Jitsi")
hp_chat_data <- model_data_hp %>% filter(comm == "Chat")

# Funktion für Univariate ICC (orientiert an deiner ursprünglichen Version)
compute_univariate_icc_simple <- function(data, name) {
  cat(sprintf("\nUnivariate ICC (%s):\n", name))
  cat(sprintf("Sample: %d Beobachtungen, %d Teams\n", nrow(data), length(unique(data$team_id))))
  
  # Verwende nur team_id als Random Effect (wie in deiner compute_icc Funktion)
  model <- lmer(flow_score ~ 1 + (1 | team_id), data = data)
  icc_result <- icc(model)
  
  print(icc_result)
  return(list(name = name, model = model, icc = icc_result))
}

# Berechne Univariate ICCs für alle Kombinationen
icc_math_jitsi <- compute_univariate_icc_simple(math_jitsi_data, "Math-Jitsi")
icc_math_chat <- compute_univariate_icc_simple(math_chat_data, "Math-Chat")
icc_hp_jitsi <- compute_univariate_icc_simple(hp_jitsi_data, "HP-Jitsi")
icc_hp_chat <- compute_univariate_icc_simple(hp_chat_data, "HP-Chat")

# ================================================================================
# TEIL 2: MULTIVARIATE ICCs nach Difficulty
# ================================================================================

print("\n--- MULTIVARIATE ICCs nach Difficulty separiert---")

# Funktion für Multivariate ICC (wie deine ursprüngliche compute_icc Funktion)
compute_multivariate_icc_simple <- function(data, base_name) {
  cat(sprintf("\nMultivariate ICCs für %s (Separate ICC pro Difficulty):\n", base_name))
  
  # Verfügbare Difficulty Levels
  difficulties <- unique(data$difficulty)
  cat(sprintf("Difficulty Levels: %s\n", paste(difficulties, collapse = ", ")))
  
  results <- list()
  
  for (level in difficulties) {
    subset_data <- data %>% filter(difficulty == level)
    
    if (nrow(subset_data) > 0 && length(unique(subset_data$team_id)) > 1) {
      cat(sprintf("\n%s - %s:\n", base_name, level))
      cat(sprintf("Sample: %d Beobachtungen, %d Teams\n", 
                  nrow(subset_data), length(unique(subset_data$team_id))))
      
      model <- lmer(flow_score ~ 1 + (1 | team_id), data = subset_data)
      icc_result <- icc(model)
      print(icc_result)
      
      results[[level]] <- list(
        difficulty = level,
        model = model,
        icc = icc_result,
        n_obs = nrow(subset_data),
        n_teams = length(unique(subset_data$team_id))
      )
    } else {
      cat(sprintf("\n%s - %s: Nicht genügend Daten\n", base_name, level))
      results[[level]] <- NULL
    }
  }
  
  return(results)
}

# ================================================================================
# TEIL 2b: ECHTER MULTIVARIATE ICC
# ================================================================================

print("\n--- ECHTER MULTIVARIATE ICC ---")

# Funktion für echten Multivariate ICC mit multilevel::mult.icc
compute_true_multivariate_icc <- function(data, base_name) {
  cat(sprintf("\nEchter Multivariate ICC für %s:\n", base_name))
  
  # Prüfe verfügbare Daten
  cat(sprintf("Gesamt Sample: %d Beobachtungen, %d Teams\n", 
              nrow(data), length(unique(data$team_id))))
  
  difficulties <- unique(data$difficulty)
  cat(sprintf("Difficulty Levels: %s\n", paste(difficulties, collapse = ", ")))
  
  # Bereite Daten für mult.icc vor
  icc_data <- data %>%
    select(flow_score, difficulty, team_id) %>%
    # Verwende explizite dplyr::rename
    dplyr::rename(val = flow_score, Condition = difficulty, SessionID = team_id) %>%
    filter(!is.na(val), !is.na(Condition), !is.na(SessionID))
  
  cat(sprintf("Nach Filterung: %d gültige Beobachtungen\n", nrow(icc_data)))
  
  tryCatch({
    # Multivariate ICC
    multivariate_result <- icc_data %>%
      group_by(Condition) %>%
      mutate(dummy = 1:n()) %>%  # Dummy Variable
      # Prüfe ob genug Daten pro Condition
      filter(n() >= 3, length(unique(SessionID)) >= 2) %>%
      do(multilevel::mult.icc(x = as.data.frame(.[, c("val", "dummy")]), .$SessionID)) %>%
      ungroup() %>%
      select(-ICC2) %>%  # Nur ICC1 behalten
      spread(Condition, ICC1) %>%  # Von Long zu Wide
      filter(Variable != "dummy")  # Dummy-Zeile entfernen
    
    # Nur zeigen wenn Ergebnisse vorhanden
    if (!is.null(multivariate_result) && nrow(multivariate_result) > 0) {
      cat("Multivariate ICC Ergebnisse (ICC1 nach Difficulty):\n")
      print(multivariate_result)
    } else {
      cat("Keine gültigen Multivariate ICC Ergebnisse\n")
    }
    
    return(list(
      name = base_name,
      result = multivariate_result,
      raw_data = icc_data
    ))
    
  }, error = function(e) {
    cat(sprintf("Fehler bei Multivariate ICC Berechnung: %s\n", e$message))
    cat("Mögliche Gründe: Zu wenig Daten pro Condition oder zu wenig Teams\n")
    return(NULL)
  })
}

# Berechne separate univariate ICCs pro Difficulty
cat("\n=== SEPARATE ICCs PRO DIFFICULTY LEVEL ===")
multivariate_math_jitsi <- compute_multivariate_icc_simple(math_jitsi_data, "Math-Jitsi")
multivariate_math_chat <- compute_multivariate_icc_simple(math_chat_data, "Math-Chat")
multivariate_hp_jitsi <- compute_multivariate_icc_simple(hp_jitsi_data, "HP-Jitsi")
multivariate_hp_chat <- compute_multivariate_icc_simple(hp_chat_data, "HP-Chat")

# Berechne echte Multivariate ICCs
cat("\n=== ECHTER MULTIVARIATE ICC ===")
true_multi_math_jitsi <- compute_true_multivariate_icc(math_jitsi_data, "Math-Jitsi")
true_multi_math_chat <- compute_true_multivariate_icc(math_chat_data, "Math-Chat")
true_multi_hp_jitsi <- compute_true_multivariate_icc(hp_jitsi_data, "HP-Jitsi")
true_multi_hp_chat <- compute_true_multivariate_icc(hp_chat_data, "HP-Chat")

# ================================================================================
# TEIL 3: ZUSAMMENFASSUNG UND VERGLEICHE
# ================================================================================

print("\n\n=== ZUSAMMENFASSUNG UND VERGLEICHE ===")

# Extrahiere ICC-Werte für Vergleich (robust gegen verschiedene icc() Output-Formate)
extract_icc_value <- function(icc_result) {
  if (is.list(icc_result)) {
    # Versuche verschiedene Feldnamen
    if ("ICC_adjusted" %in% names(icc_result)) {
      return(icc_result$ICC_adjusted)
    } else if ("ICC_conditional" %in% names(icc_result)) {
      return(icc_result$ICC_conditional)  
    } else if (length(icc_result) > 0) {
      return(icc_result[[1]]) # Nimm den ersten Wert
    }
  } else if (is.numeric(icc_result)) {
    return(icc_result[1]) # Falls es ein numerischer Vektor ist
  }
  return(NA)
}

# Erstelle Vergleichstabelle für Univariate ICCs
univariate_comparison <- data.frame(
  Task_Communication = c("Math-Jitsi", "Math-Chat", "HP-Jitsi", "HP-Chat"),
  ICC_Value = c(
    extract_icc_value(icc_math_jitsi$icc),
    extract_icc_value(icc_math_chat$icc),
    extract_icc_value(icc_hp_jitsi$icc),
    extract_icc_value(icc_hp_chat$icc)
  ),
  N_Observations = c(
    nrow(math_jitsi_data),
    nrow(math_chat_data),
    nrow(hp_jitsi_data),
    nrow(hp_chat_data)
  ),
  N_Teams = c(
    length(unique(math_jitsi_data$team_id)),
    length(unique(math_chat_data$team_id)),
    length(unique(hp_jitsi_data$team_id)),
    length(unique(hp_chat_data$team_id))
  )
)

cat("\nUnivariate ICC Vergleich:\n")
print(univariate_comparison)

# Analysiere Unterschiede zwischen Kommunikationsmedien
cat("\nVergleich zwischen Kommunikationsmedien:\n")

# Math Task Vergleich
math_jitsi_value <- univariate_comparison$ICC_Value[univariate_comparison$Task_Communication == "Math-Jitsi"]
math_chat_value <- univariate_comparison$ICC_Value[univariate_comparison$Task_Communication == "Math-Chat"]

if (!is.na(math_jitsi_value) && !is.na(math_chat_value)) {
  cat(sprintf("Math Task: Jitsi (%.4f) vs Chat (%.4f) - Differenz: %.4f\n", 
              math_jitsi_value, math_chat_value, math_jitsi_value - math_chat_value))
}

# HP Task Vergleich  
hp_jitsi_value <- univariate_comparison$ICC_Value[univariate_comparison$Task_Communication == "HP-Jitsi"]
hp_chat_value <- univariate_comparison$ICC_Value[univariate_comparison$Task_Communication == "HP-Chat"]

if (!is.na(hp_jitsi_value) && !is.na(hp_chat_value)) {
  cat(sprintf("HP Task: Jitsi (%.4f) vs Chat (%.4f) - Differenz: %.4f\n", 
              hp_jitsi_value, hp_chat_value, hp_jitsi_value - hp_chat_value))
}

# Erstelle Multivariate Zusammenfassung (beide Methoden)
create_multivariate_summary <- function(results, name) {
  cat(sprintf("\n%s - Separate ICCs nach Difficulty:\n", name))
  
  for (difficulty in names(results)) {
    result <- results[[difficulty]]
    if (!is.null(result)) {
      icc_val <- extract_icc_value(result$icc)
      if (!is.na(icc_val)) {
        cat(sprintf("  %s: ICC = %.4f (%d obs, %d teams)\n", 
                    difficulty, icc_val, result$n_obs, result$n_teams))
      }
    }
  }
}

create_multivariate_summary(multivariate_math_jitsi, "Math-Jitsi")
create_multivariate_summary(multivariate_math_chat, "Math-Chat")
create_multivariate_summary(multivariate_hp_jitsi, "HP-Jitsi")
create_multivariate_summary(multivariate_hp_chat, "HP-Chat")

cat("\n=== ECHTE MULTIVARIATE ICC ERGEBNISSE ===\n")

show_true_multivariate <- function(result, name) {
  cat(sprintf("\n%s - Echter Multivariate ICC:\n", name))
  
  if (!is.null(result) && !is.null(result$result)) {
    if (nrow(result$result) > 0) {
      # Extrahiere und zeige die ICC-Werte ohne die Tabelle nochmal zu printen
      numeric_cols <- sapply(result$result, is.numeric)
      if (any(numeric_cols)) {
        cat("ICC1-Werte nach Difficulty:\n")
        for (col in names(result$result)[numeric_cols]) {
          val <- result$result[[col]][1]
          if (!is.na(val)) {
            cat(sprintf("  %s: ICC1 = %.4f\n", col, val))
          }
        }
      }
    } else {
      cat("Keine gültigen Ergebnisse verfügbar\n")
    }
  } else {
    cat("Multivariate ICC konnte nicht berechnet werden\n")
  }
}

if (!is.null(true_multi_math_jitsi)) show_true_multivariate(true_multi_math_jitsi, "Math-Jitsi")
if (!is.null(true_multi_math_chat)) show_true_multivariate(true_multi_math_chat, "Math-Chat")
if (!is.null(true_multi_hp_jitsi)) show_true_multivariate(true_multi_hp_jitsi, "HP-Jitsi")
if (!is.null(true_multi_hp_chat)) show_true_multivariate(true_multi_hp_chat, "HP-Chat")
```

Correlation of flow with anticipated mediators

```{r}
# Master Thesis Analysis: Flow Mediation durch Team Composition, Information Sharing und Emotional Synchrony
# Separate Analysen für Math und Hidden Profile Tasks

library(dplyr)
library(corrplot)
library(psych)
library(lme4)
library(ggplot2)
library(tidyr)

# Konflikt zwischen plyr und dplyr lösen - dplyr Funktionen explizit verwenden
summarise <- dplyr::summarise
mutate <- dplyr::mutate
select <- dplyr::select

# Annahme: data und flow_scores DataFrames sind bereits geladen
# data <- read.csv("your_data.csv")
# flow_scores <- read.csv("flow_scores.csv")

# ================================================================================
# TEIL 1: DATENAUFBEREITUNG
# ================================================================================

# Funktion zur Extraktion und Verarbeitung der Team Composition Daten
extract_team_composition <- function(data) {
  # Team Composition Items (einmal pro Aufgabentyp erhoben)
  # tsz = team size, td = team diversity, tsc = team skills complementary
  tc_vars <- c("tsz1", "tsz2", "tsz3", "td1", "td2", "td3", "tsc1", "tsc2", "tsc3")
  
  tc_data <- data %>%
    select(participant.code, contains("player")) %>%
    select(participant.code, matches("(mathJitsi|mathChat|HiddenProfile_Jitsi|HiddenProfile_Chat).*\\.(tsz1|tsz2|tsz3|td1|td2|td3|tsc1|tsc2|tsc3)$"))
  
  # Umstrukturierung für bessere Verarbeitung
  tc_long <- tc_data %>%
    pivot_longer(cols = -participant.code, 
                 names_to = "variable", 
                 values_to = "value") %>%
    filter(!is.na(value)) %>%
    mutate(
      task = case_when(
        grepl("^math", variable) ~ "Math",
        grepl("^HiddenProfile", variable) ~ "HP"
      ),
      comm = case_when(
        grepl("Jitsi", variable) ~ "Jitsi",
        grepl("Chat", variable) ~ "Chat"
      ),
      dimension = case_when(
        grepl("tsz", variable) ~ "team_size",
        grepl("td", variable) ~ "team_diversity", 
        grepl("tsc", variable) ~ "team_skills_complementary"
      ),
      item = gsub(".*\\.(tsz|td|tsc)(\\d+)$", "\\2", variable)
    )
  
  return(tc_long)
}

# Funktion zur Extraktion von Information Sharing Daten
extract_information_sharing <- function(data) {
  is_data <- data %>%
    select(participant.code, contains("player")) %>%
    select(participant.code, matches("(mathJitsi|mathChat|HiddenProfile_Jitsi|HiddenProfile_Chat).*\\.(info1|info2)$"))
  
  is_long <- is_data %>%
    pivot_longer(cols = -participant.code, 
                 names_to = "variable", 
                 values_to = "value") %>%
    filter(!is.na(value)) %>%
    mutate(
      task = case_when(
        grepl("^math", variable) ~ "Math",
        grepl("^HiddenProfile", variable) ~ "HP"
      ),
      comm = case_when(
        grepl("Jitsi", variable) ~ "Jitsi",
        grepl("Chat", variable) ~ "Chat"
      ),
      round = gsub(".*\\.(\\d+)\\.player.*", "\\1", variable),
      item = gsub(".*\\.(info)(\\d+)$", "\\2", variable)
    )
  
  return(is_long)
}

# Funktion zur Extraktion von Emotional Synchrony Daten
extract_emotional_synchrony <- function(data) {
  # ec = emotional contagion, es = emotional synchrony
  es_data <- data %>%
    select(participant.code, contains("player")) %>%
    select(participant.code, matches("(mathJitsi|mathChat|HiddenProfile_Jitsi|HiddenProfile_Chat).*\\.(ec1|es1)$"))
  
  es_long <- es_data %>%
    pivot_longer(cols = -participant.code, 
                 names_to = "variable", 
                 values_to = "value") %>%
    filter(!is.na(value)) %>%
    mutate(
      task = case_when(
        grepl("^math", variable) ~ "Math",
        grepl("^HiddenProfile", variable) ~ "HP"
      ),
      comm = case_when(
        grepl("Jitsi", variable) ~ "Jitsi",
        grepl("Chat", variable) ~ "Chat"
      ),
      round = gsub(".*\\.(\\d+)\\.player.*", "\\1", variable),
      item_type = case_when(
        grepl("ec1", variable) ~ "emotional_contagion",
        grepl("es1", variable) ~ "emotional_synchrony"
      )
    )
  
  return(es_long)
}

# Datenextraktion durchführen
tc_long <- extract_team_composition(data)
is_long <- extract_information_sharing(data)
es_long <- extract_emotional_synchrony(data)

# ================================================================================
# TEIL 2: RELIABILITÄTSANALYSEN UND AGGREGATION
# ================================================================================

# Team Composition: Reliabilität prüfen und aggregieren
tc_reliability <- tc_long %>%
  dplyr::group_by(participant.code, task, comm, dimension) %>%
  dplyr::summarise(
    n_items = dplyr::n(),
    mean_score = mean(value, na.rm = TRUE),
    .groups = "drop"
  )

print("Team Composition Reliabilität (nach Dimension):")
print(tc_reliability)

# Team Composition Scores aggregieren (KORRIGIERT)
tc_scores <- tc_long %>%
  dplyr::group_by(participant.code, task, comm, dimension) %>%
  dplyr::summarise(score = mean(value, na.rm = TRUE), .groups = "keep") %>%
  dplyr::ungroup() %>%
  tidyr::pivot_wider(names_from = dimension, values_from = score) %>%
  dplyr::mutate(team_composition_score = rowMeans(dplyr::select(., team_size, team_diversity, team_skills_complementary), na.rm = TRUE))

# Information Sharing: Korrelation und Aggregation
# info = information sharing

is_wide_test <- is_long %>%
  tidyr::pivot_wider(names_from = item, values_from = value, names_prefix = "info")

is_correlation <- is_wide_test %>%
  dplyr::group_by(task, comm) %>%
  dplyr::summarise(
    n_total = dplyr::n(),
    n_info1 = sum(!is.na(info1)),
    n_info2 = sum(!is.na(info2)),
    n_pairs = sum(!is.na(info1) & !is.na(info2)),
    correlation = ifelse(n_pairs >= 3, 
                        cor(info1, info2, use = "complete.obs"), 
                        NA_real_),
    .groups = "drop"
  )

print("Information Sharing Korrelationen:")
print(is_correlation)

# Information Sharing aggregieren
is_scores <- is_long %>%
  group_by(participant.code, task, comm, round) %>%
  summarise(round_score = mean(value, na.rm = TRUE), .groups = "drop") %>%
  group_by(participant.code, task, comm) %>%
  summarise(information_sharing_score = mean(round_score, na.rm = TRUE), .groups = "drop")

# Emotional Synchrony: Korrelation und Aggregation
# ec = emotional contagion, es = emotional synchrony
es_correlation <- es_long %>%
  tidyr::pivot_wider(names_from = item_type, values_from = value) %>%
  dplyr::group_by(task, comm) %>%
  dplyr::summarise(
    n_total = dplyr::n(),
    n_ec = sum(!is.na(emotional_contagion)),
    n_es = sum(!is.na(emotional_synchrony)),
    n_pairs = sum(!is.na(emotional_contagion) & !is.na(emotional_synchrony)),
    correlation = ifelse(n_pairs >= 3, 
                        cor(emotional_contagion, emotional_synchrony, use = "complete.obs"), 
                        NA_real_),
    .groups = "drop"
  )

print("Emotional Synchrony Korrelationen:")
print(es_correlation)

# Zusätzlich: Schaue dir die Rohdaten an
print("Emotional Synchrony Rohdaten-Check:")
es_check <- es_long %>%
  dplyr::group_by(task, comm, item_type) %>%
  dplyr::summarise(
    n_total = dplyr::n(),
    n_valid = sum(!is.na(value)),
    n_missing = sum(is.na(value)),
    .groups = "drop"
  )
print(es_check)

# Emotional Synchrony aggregieren
es_scores <- es_long %>%
  group_by(participant.code, task, comm, round) %>%
  summarise(round_score = mean(value, na.rm = TRUE), .groups = "drop") %>%
  group_by(participant.code, task, comm) %>%
  summarise(emotional_synchrony_score = mean(round_score, na.rm = TRUE), .groups = "drop")

# ================================================================================
# TEIL 3: HAUPTDATENSATZ ERSTELLEN
# ================================================================================

# Alle Scores zusammenfügen
mediation_data <- tc_scores %>%
  select(participant.code, task, comm, team_composition_score) %>%
  left_join(is_scores, by = c("participant.code", "task", "comm")) %>%
  left_join(es_scores, by = c("participant.code", "task", "comm")) %>%
  # Mit Flow Scores verbinden
  left_join(
    flow_scores %>%
      group_by(participant.code, task, comm) %>%
      summarise(mean_flow_score = mean(flow_score, na.rm = TRUE), .groups = "drop"),
    by = c("participant.code", "task", "comm")
  ) %>%
  # Synchronicity Variable hinzufügen
  mutate(
    synchronicity = case_when(
      comm == "Jitsi" ~ "High",
      comm == "Chat" ~ "Low"
    )
  )

# ================================================================================
# TEIL 4: EXPLORATIVE ANALYSEN - GETRENNT FÜR MATH UND HIDDEN PROFILE
# ================================================================================

# ---- MATH TASK ANALYSE ----
print("=== MATH TASK ANALYSE ===")

math_data <- mediation_data %>% filter(task == "Math")

# Deskriptive Statistiken
math_descriptives <- math_data %>%
  group_by(comm) %>%
  summarise(
    n = n(),
    across(c(team_composition_score, information_sharing_score, 
             emotional_synchrony_score, mean_flow_score), 
           list(mean = ~mean(.x, na.rm = TRUE), 
                sd = ~sd(.x, na.rm = TRUE)), 
           .names = "{.col}_{.fn}"),
    .groups = "drop"
  )

print("Math Task - Deskriptive Statistiken:")
print(math_descriptives)

# Korrelationsmatrix für Math
math_cor_vars <- c("team_composition_score", "information_sharing_score", 
                   "emotional_synchrony_score", "mean_flow_score")
math_cor_matrix <- cor(math_data[math_cor_vars], use = "complete.obs")

print("Math Task - Korrelationsmatrix:")
print(round(math_cor_matrix, 3))

# Korrelationsplot
corrplot(math_cor_matrix, method = "color", type = "upper", 
         title = "Math Task - Korrelationen", mar = c(0,0,2,0))

# Regressionsanalysen für Math
# Basis-Modell: Synchronicity -> Flow
math_model_base <- lm(mean_flow_score ~ synchronicity, data = math_data)

# Erweiterte Modelle mit Mediatoren
math_model_tc <- lm(mean_flow_score ~ synchronicity + team_composition_score, data = math_data)
math_model_is <- lm(mean_flow_score ~ synchronicity + information_sharing_score, data = math_data)
math_model_es <- lm(mean_flow_score ~ synchronicity + emotional_synchrony_score, data = math_data)
math_model_full <- lm(mean_flow_score ~ synchronicity + team_composition_score + 
                      information_sharing_score + emotional_synchrony_score, data = math_data)

# Mediator-Modelle (Synchronicity -> Mediatoren)
math_mediator_tc <- lm(team_composition_score ~ synchronicity, data = math_data)
math_mediator_is <- lm(information_sharing_score ~ synchronicity, data = math_data)
math_mediator_es <- lm(emotional_synchrony_score ~ synchronicity, data = math_data)

print("Math Task - Modell-Ergebnisse:")
print("Basis-Modell (Synchronicity -> Flow):")
print(summary(math_model_base))
print("Vollständiges Modell:")
print(summary(math_model_full))

# ---- HIDDEN PROFILE TASK ANALYSE ----
print("\n=== HIDDEN PROFILE TASK ANALYSE ===")

hp_data <- mediation_data %>% filter(task == "HP")

# Deskriptive Statistiken
hp_descriptives <- hp_data %>%
  group_by(comm) %>%
  summarise(
    n = n(),
    across(c(team_composition_score, information_sharing_score, 
             emotional_synchrony_score, mean_flow_score), 
           list(mean = ~mean(.x, na.rm = TRUE), 
                sd = ~sd(.x, na.rm = TRUE)), 
           .names = "{.col}_{.fn}"),
    .groups = "drop"
  )

print("Hidden Profile Task - Deskriptive Statistiken:")
print(hp_descriptives)

# Korrelationsmatrix für Hidden Profile
hp_cor_matrix <- cor(hp_data[math_cor_vars], use = "complete.obs")

print("Hidden Profile Task - Korrelationsmatrix:")
print(round(hp_cor_matrix, 3))

# Korrelationsplot
corrplot(hp_cor_matrix, method = "color", type = "upper", 
         title = "Hidden Profile Task - Korrelationen", mar = c(0,0,2,0))

# Regressionsanalysen für Hidden Profile
# Basis-Modell: Synchronicity -> Flow
hp_model_base <- lm(mean_flow_score ~ synchronicity, data = hp_data)

# Erweiterte Modelle mit Mediatoren
hp_model_tc <- lm(mean_flow_score ~ synchronicity + team_composition_score, data = hp_data)
hp_model_is <- lm(mean_flow_score ~ synchronicity + information_sharing_score, data = hp_data)
hp_model_es <- lm(mean_flow_score ~ synchronicity + emotional_synchrony_score, data = hp_data)
hp_model_full <- lm(mean_flow_score ~ synchronicity + team_composition_score + 
                    information_sharing_score + emotional_synchrony_score, data = hp_data)

# Mediator-Modelle (Synchronicity -> Mediatoren)
hp_mediator_tc <- lm(team_composition_score ~ synchronicity, data = hp_data)
hp_mediator_is <- lm(information_sharing_score ~ synchronicity, data = hp_data)
hp_mediator_es <- lm(emotional_synchrony_score ~ synchronicity, data = hp_data)

print("Hidden Profile Task - Modell-Ergebnisse:")
print("Basis-Modell (Synchronicity -> Flow):")
print(summary(hp_model_base))
print("Vollständiges Modell:")
print(summary(hp_model_full))

# ================================================================================
# TEIL 5: MEDIATION ANALYSE (vereinfacht)
# ================================================================================

# Funktion für einfache Mediation (Baron & Kenny Ansatz)
simple_mediation_test <- function(data, mediator_name) {
  # Schritt 1: X -> Y (Total Effect)
  model_c <- lm(mean_flow_score ~ synchronicity, data = data)
  
  # Schritt 2: X -> M (a path)
  model_a <- lm(get(mediator_name) ~ synchronicity, data = data)
  
  # Schritt 3: X + M -> Y (Direct Effect)
  formula_str <- paste("mean_flow_score ~ synchronicity +", mediator_name)
  model_b <- lm(as.formula(formula_str), data = data)
  
  # Ergebnisse sammeln
  results <- list(
    mediator = mediator_name,
    total_effect = summary(model_c)$coefficients["synchronicityLow", "Estimate"],
    total_effect_p = summary(model_c)$coefficients["synchronicityLow", "Pr(>|t|)"],
    a_path = summary(model_a)$coefficients["synchronicityLow", "Estimate"],
    a_path_p = summary(model_a)$coefficients["synchronicityLow", "Pr(>|t|)"],
    b_path = summary(model_b)$coefficients[mediator_name, "Estimate"],
    b_path_p = summary(model_b)$coefficients[mediator_name, "Pr(>|t|)"],
    direct_effect = summary(model_b)$coefficients["synchronicityLow", "Estimate"],
    direct_effect_p = summary(model_b)$coefficients["synchronicityLow", "Pr(>|t|)"]
  )
  
  # Indirekte Effekt
  results$indirect_effect <- results$a_path * results$b_path
  
  return(results)
}

# Mediation Tests für Math Task
print("\n=== MEDIATION ANALYSEN - MATH TASK ===")
math_mediation_tc <- simple_mediation_test(math_data, "team_composition_score")
math_mediation_is <- simple_mediation_test(math_data, "information_sharing_score")
math_mediation_es <- simple_mediation_test(math_data, "emotional_synchrony_score")

mediation_results_math <- data.frame(
  Mediator = c("Team Composition", "Information Sharing", "Emotional Synchrony"),
  Total_Effect = c(math_mediation_tc$total_effect, math_mediation_is$total_effect, math_mediation_es$total_effect),
  Direct_Effect = c(math_mediation_tc$direct_effect, math_mediation_is$direct_effect, math_mediation_es$direct_effect),
  Indirect_Effect = c(math_mediation_tc$indirect_effect, math_mediation_is$indirect_effect, math_mediation_es$indirect_effect),
  A_Path = c(math_mediation_tc$a_path, math_mediation_is$a_path, math_mediation_es$a_path),
  B_Path = c(math_mediation_tc$b_path, math_mediation_is$b_path, math_mediation_es$b_path)
)

print("Math Task - Mediation Ergebnisse:")
# Nur die numerischen Spalten runden
mediation_results_math_rounded <- mediation_results_math
mediation_results_math_rounded[, 2:6] <- round(mediation_results_math[, 2:6], 4)
print(mediation_results_math_rounded)

# Mediation Tests für Hidden Profile Task  
print("\n=== MEDIATION ANALYSEN - HIDDEN PROFILE TASK ===")
hp_mediation_tc <- simple_mediation_test(hp_data, "team_composition_score")
hp_mediation_is <- simple_mediation_test(hp_data, "information_sharing_score")
hp_mediation_es <- simple_mediation_test(hp_data, "emotional_synchrony_score")

mediation_results_hp <- data.frame(
  Mediator = c("Team Composition", "Information Sharing", "Emotional Synchrony"),
  Total_Effect = c(hp_mediation_tc$total_effect, hp_mediation_is$total_effect, hp_mediation_es$total_effect),
  Direct_Effect = c(hp_mediation_tc$direct_effect, hp_mediation_is$direct_effect, hp_mediation_es$direct_effect),
  Indirect_Effect = c(hp_mediation_tc$indirect_effect, hp_mediation_is$indirect_effect, hp_mediation_es$indirect_effect),
  A_Path = c(hp_mediation_tc$a_path, hp_mediation_is$a_path, hp_mediation_es$a_path),
  B_Path = c(hp_mediation_tc$b_path, hp_mediation_is$b_path, hp_mediation_es$b_path)
)

print("Hidden Profile Task - Mediation Ergebnisse:")
# Nur die numerischen Spalten runden
mediation_results_hp_rounded <- mediation_results_hp
mediation_results_hp_rounded[, 2:6] <- round(mediation_results_hp[, 2:6], 4)
print(mediation_results_hp_rounded)

# ================================================================================
# TEIL 6: VISUALISIERUNGEN
# ================================================================================

# Vergleich der Mediator-Werte zwischen den Kommunikationsformen
mediation_plot_data <- mediation_data %>%
  pivot_longer(cols = c(team_composition_score, information_sharing_score, emotional_synchrony_score),
               names_to = "mediator", values_to = "score") %>%
  mutate(mediator = case_when(
    mediator == "team_composition_score" ~ "Team Composition",
    mediator == "information_sharing_score" ~ "Information Sharing", 
    mediator == "emotional_synchrony_score" ~ "Emotional Synchrony"
  ))

# Plot für Math Task
ggplot(mediation_plot_data %>% filter(task == "Math"), 
       aes(x = mediator, y = score, fill = comm)) +
  geom_boxplot() +
  labs(title = "Math Task - Mediator Scores by Communication Medium",
       x = "Mediator", y = "Score", fill = "Communication") +
  theme_minimal()

# Plot für Hidden Profile Task
ggplot(mediation_plot_data %>% filter(task == "HP"), 
       aes(x = mediator, y = score, fill = comm)) +
  geom_boxplot() +
  labs(title = "Hidden Profile Task - Mediator Scores by Communication Medium",
       x = "Mediator", y = "Score", fill = "Communication") +
  theme_minimal()

# Zusammenfassung der wichtigsten Erkenntnisse
print("\n=== ZUSAMMENFASSUNG ===")
print("1. Reliabilitätsanalysen der Items durchgeführt")
print("2. Korrelationsanalysen für Information Sharing und Emotional Synchrony")
print("3. Aggregation zu finalen Scores für jeden Mediator")
print("4. Separate explorative Analysen für Math und Hidden Profile Tasks")
print("5. Einfache Mediationsanalysen nach Baron & Kenny")
print("6. Visualisierungen der Unterschiede zwischen Kommunikationsmedien")

# Datensatz für weitere Analysen speichern
# write.csv(mediation_data, "mediation_analysis_data.csv", row.names = FALSE)
```

Regression of flow on familiarity scores

```{r}
# --- 1. Korrelationsanalysen für Familiarity-Variablen ----------------------

# Math Jitsi
math_jitsi_fam <- data %>%
  select(starts_with("mathJitsi.6.player.fam")) %>%
  select(contains("lightcoral"), contains("lightgreen"), contains("lightblue"))

cor_math_jitsi <- cor(math_jitsi_fam, use = "pairwise.complete.obs")
print("Correlation Matrix - Math Jitsi:")
print(round(cor_math_jitsi, 3))

# Math Chat
math_chat_fam <- data %>%
  select(starts_with("mathChat.6.player.fam")) %>%
  select(contains("lightcoral"), contains("lightgreen"), contains("lightblue"))

cor_math_chat <- cor(math_chat_fam, use = "pairwise.complete.obs")
print("\nCorrelation Matrix - Math Chat:")
print(round(cor_math_chat, 3))

# HP Jitsi
hp_jitsi_fam <- data %>%
  select(starts_with("HiddenProfile_Jitsi.3.player.fam")) %>%
  select(contains("lightcoral"), contains("lightgreen"), contains("lightblue"))

cor_hp_jitsi <- cor(hp_jitsi_fam, use = "pairwise.complete.obs")
print("\nCorrelation Matrix - HP Jitsi:")
print(round(cor_hp_jitsi, 3))

# HP Chat
hp_chat_fam <- data %>%
  select(starts_with("HiddenProfile_Chat.3.player.fam")) %>%
  select(contains("lightcoral"), contains("lightgreen"), contains("lightblue"))

cor_hp_chat <- cor(hp_chat_fam, use = "pairwise.complete.obs")
print("\nCorrelation Matrix - HP Chat:")
print(round(cor_hp_chat, 3))

# --- 2. Familiarity: Aggregation pro Farbe (fam1 & fam2 mitteln) ----------------------

data <- data %>%
  mutate(
    # Math – Jitsi
    fam_mathJitsi_coral = rowMeans(select(., mathJitsi.6.player.fam1_lightcoral, mathJitsi.6.player.fam2_lightcoral), na.rm = TRUE),
    fam_mathJitsi_green = rowMeans(select(., mathJitsi.6.player.fam1_lightgreen, mathJitsi.6.player.fam2_lightgreen), na.rm = TRUE),
    fam_mathJitsi_blue  = rowMeans(select(., mathJitsi.6.player.fam1_lightblue,  mathJitsi.6.player.fam2_lightblue),  na.rm = TRUE),
    
    # Math – Chat
    fam_mathChat_coral = rowMeans(select(., mathChat.6.player.fam1_lightcoral, mathChat.6.player.fam2_lightcoral), na.rm = TRUE),
    fam_mathChat_green = rowMeans(select(., mathChat.6.player.fam1_lightgreen, mathChat.6.player.fam2_lightgreen), na.rm = TRUE),
    fam_mathChat_blue  = rowMeans(select(., mathChat.6.player.fam1_lightblue,  mathChat.6.player.fam2_lightblue),  na.rm = TRUE),

    # HP – Jitsi
    fam_hpJitsi_coral = rowMeans(select(., HiddenProfile_Jitsi.3.player.fam1_lightcoral, HiddenProfile_Jitsi.3.player.fam2_lightcoral), na.rm = TRUE),
    fam_hpJitsi_green = rowMeans(select(., HiddenProfile_Jitsi.3.player.fam1_lightgreen, HiddenProfile_Jitsi.3.player.fam2_lightgreen), na.rm = TRUE),
    fam_hpJitsi_blue  = rowMeans(select(., HiddenProfile_Jitsi.3.player.fam1_lightblue,  HiddenProfile_Jitsi.3.player.fam2_lightblue),  na.rm = TRUE),

    # HP – Chat
    fam_hpChat_coral = rowMeans(select(., HiddenProfile_Chat.3.player.fam1_lightcoral, HiddenProfile_Chat.3.player.fam2_lightcoral), na.rm = TRUE),
    fam_hpChat_green = rowMeans(select(., HiddenProfile_Chat.3.player.fam1_lightgreen, HiddenProfile_Chat.3.player.fam2_lightgreen), na.rm = TRUE),
    fam_hpChat_blue  = rowMeans(select(., HiddenProfile_Chat.3.player.fam1_lightblue,  HiddenProfile_Chat.3.player.fam2_lightblue),  na.rm = TRUE)
  )

# --- 3. Familiarity: Aggregation pro Bedingung (über die zwei befüllten Farben) -------

data <- data %>%
  mutate(
    fam_mathJitsi = rowMeans(select(., fam_mathJitsi_coral, fam_mathJitsi_green, fam_mathJitsi_blue), na.rm = TRUE),
    fam_mathChat  = rowMeans(select(., fam_mathChat_coral, fam_mathChat_green, fam_mathChat_blue), na.rm = TRUE),
    fam_hpJitsi   = rowMeans(select(., fam_hpJitsi_coral, fam_hpJitsi_green, fam_hpJitsi_blue), na.rm = TRUE),
    fam_hpChat    = rowMeans(select(., fam_hpChat_coral, fam_hpChat_green, fam_hpChat_blue), na.rm = TRUE)
  )

# --- 4. Recognition: Kategorisierung für Analyse ---------------

data <- data %>%
  mutate(
    # Individual recognition scores
    rec_coral = Outro.1.player.rec_lightcoral,
    rec_green = Outro.1.player.rec_lightgreen,
    rec_blue  = Outro.1.player.rec_lightblue
  ) %>%
  mutate(
    # Mean recognition across teammates
    rec_mean = rowMeans(select(., rec_coral, rec_green, rec_blue), na.rm = TRUE),
    
    # Count how many teammates were known (>4 on 7-point scale)
    rec_count = rowSums(select(., rec_coral, rec_green, rec_blue) > 4, na.rm = TRUE),
    
    # Categorical variable
    rec_category = case_when(
      rec_count == 0 ~ "Nobody known",
      rec_count == 1 ~ "One person known",
      rec_count == 2 ~ "Both known",
      TRUE ~ NA_character_
    )
  )

# --- 5. Deskriptive Statistiken ---------------

# Recognition categories
print("\n--- Recognition Categories ---")
table(data$rec_category)

# Familiarity means by condition
print("\n--- Mean Familiarity by Condition ---")
data %>%
  summarise(
    MathJitsi = mean(fam_mathJitsi, na.rm = TRUE),
    MathChat = mean(fam_mathChat, na.rm = TRUE),
    HPJitsi = mean(fam_hpJitsi, na.rm = TRUE),
    HPChat = mean(fam_hpChat, na.rm = TRUE)
  ) %>%
  print()

# --- 6. Long format for regression analyses ---------------

# Da die Flow-Scores in einem separaten Dataframe sind, müssen wir anders vorgehen
# Erst aggregieren wir die Flow-Scores pro Bedingung (über alle Schwierigkeiten)

print("\n--- Aggregating flow scores ---")
flow_aggregated <- flow_scores %>%
  group_by(participant.code, task, comm) %>%
  summarise(
    flow_score = mean(flow_score, na.rm = TRUE),
    n_difficulties = n(),  # Anzahl der Schwierigkeitsstufen
    .groups = 'drop'
  )

print("Sample of aggregated flow scores:")
print(head(flow_aggregated))

# Erstelle das Long-Format für Familiarity
familiarity_long <- data %>%
  select(participant.code,
         fam_mathJitsi, fam_mathChat, fam_hpJitsi, fam_hpChat,
         rec_mean, rec_count, rec_category) %>%
  pivot_longer(
    cols = starts_with("fam_"),
    names_to = "condition",
    values_to = "familiarity"
  ) %>%
  mutate(
    task = case_when(
      str_detect(condition, "math") ~ "Math", 
      TRUE ~ "HP"  # HP wie in flow_scores
    ),
    comm = case_when(
      str_detect(condition, "Chat") ~ "Chat", 
      TRUE ~ "Jitsi"
    )
  )

# Merge mit Flow-Scores
familiarity_long <- familiarity_long %>%
  left_join(
    flow_aggregated %>% select(participant.code, task, comm, flow_score),
    by = c("participant.code", "task", "comm")
  ) %>%
  drop_na(flow_score)

# Check wie viele Zeilen wir haben
print(paste("\nRows in familiarity_long:", nrow(familiarity_long)))
print(paste("Unique participants:", n_distinct(familiarity_long$participant.code)))

# Überprüfe die Verteilung
print("\nDistribution by condition:")
print(table(familiarity_long$task, familiarity_long$comm))

# --- 7. Regression Models ---------------

print("\n--- Model A: Basic (Familiarity + Recognition) ---")
model_a <- lm(flow_score ~ familiarity + rec_mean, data = familiarity_long)
summary(model_a)

print("\n--- Model B: With Task and Communication ---")
model_b <- lm(flow_score ~ familiarity + rec_mean + task + comm, data = familiarity_long)
summary(model_b)

print("\n--- Model C: With Interactions ---")
model_c <- lm(flow_score ~ familiarity * comm + rec_mean * comm + task, data = familiarity_long)
summary(model_c)

print("\n--- Model D: Recognition Categories ---")
model_d <- lm(flow_score ~ familiarity + rec_category + task + comm, data = familiarity_long)
summary(model_d)

print("\n--- Model E: Non-linear Recognition Effect ---")
model_e <- lm(flow_score ~ familiarity + rec_count + I(rec_count^2) + task + comm, data = familiarity_long)
summary(model_e)

# --- 8. Visualizations ---------------

# Plot 1: Familiarity vs Flow by Communication Type
p1 <- ggplot(familiarity_long, aes(x = familiarity, y = flow_score, color = comm)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = TRUE) +
  facet_wrap(~ task) +
  labs(
    title = "Familiarity vs. Flow by Communication Type and Task",
    x = "Familiarity with Teammates (during task)",
    y = "Flow Score",
    color = "Communication"
  ) +
  theme_minimal()

print(p1)

# Plot 2: Recognition Categories and Flow
p2 <- ggplot(familiarity_long, aes(x = rec_category, y = flow_score, fill = rec_category)) +
  geom_boxplot(alpha = 0.7) +
  geom_jitter(width = 0.2, alpha = 0.3) +
  facet_grid(task ~ comm) +
  labs(
    title = "Flow by Prior Recognition of Teammates",
    x = "Prior Recognition",
    y = "Flow Score"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p2)

# Plot 3: Recognition Mean vs Flow
p3 <- ggplot(familiarity_long, aes(x = rec_mean, y = flow_score)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = TRUE) +
  facet_grid(task ~ comm) +
  labs(
    title = "Mean Prior Recognition vs. Flow",
    x = "Mean Recognition Score",
    y = "Flow Score"
  ) +
  theme_minimal()

print(p3)

# --- 9. Additional Analyses ---------------

# Check if recognition effect differs by communication type
print("\n--- Model F: Recognition × Communication Interaction ---")
model_f <- lm(flow_score ~ familiarity + rec_mean * comm + task, data = familiarity_long)
summary(model_f)

# Correlation between familiarity and recognition
print("\n--- Correlation: Familiarity vs Recognition ---")
cor.test(familiarity_long$familiarity, familiarity_long$rec_mean)

# Mean flow by recognition categories
print("\n--- Mean Flow by Recognition Category ---")
familiarity_long %>%
  group_by(rec_category) %>%
  summarise(
    mean_flow = mean(flow_score, na.rm = TRUE),
    sd_flow = sd(flow_score, na.rm = TRUE),
    n = n()
  ) %>%
  print()

```

Regression of flow on gender distribution

```{r}
# --- Team Gender Composition Analysis ---

# 1. Team-ID ist bereits in data vorhanden (erstellt mit session.code + group.custom_group_id)
print("Checking for team_id in data:")
print("team_id" %in% names(data))

# 2. Teamzusammensetzung berechnen
team_gender_composition <- data %>%
  select(participant.code, team_id, gender = Intro.1.player.gender) %>%
  filter(!is.na(gender), !is.na(team_id)) %>%
  group_by(team_id) %>%
  summarise(
    n_members = n(),
    n_male = sum(gender == "Male", na.rm = TRUE),
    n_female = sum(gender == "Female", na.rm = TRUE),
    n_other = sum(!gender %in% c("Male", "Female"), na.rm = TRUE),
    unique_genders = n_distinct(gender),
    .groups = "drop"
  ) %>%
  mutate(
    gender_comp = case_when(
      n_male == n_members ~ "all_male",
      n_female == n_members ~ "all_female",
      n_male > 0 & n_female > 0 ~ "mixed",
      TRUE ~ "other"
    )
  )

# 3. Check der Verteilung
print("\nTeam Gender Composition Distribution:")
table(team_gender_composition$gender_comp)

# 4. Gender composition zu flow_scores hinzufügen
flow_scores_gender <- flow_scores %>%
  left_join(team_gender_composition %>% select(team_id, gender_comp), 
            by = "team_id")

# 5. Visualisierung: Boxplot
p1 <- ggplot(flow_scores_gender, aes(x = gender_comp, y = flow_score, fill = gender_comp)) +
  geom_boxplot(alpha = 0.7) +
  geom_jitter(width = 0.2, alpha = 0.3) +
  labs(
    title = "Flow Scores by Team Gender Composition",
    x = "Team Gender Composition",
    y = "Flow Score",
    fill = "Gender Composition"
  ) +
  theme_minimal() +
  scale_fill_brewer(palette = "Set2")

print(p1)

# 6. Visualisierung nach Task und Communication
p2 <- ggplot(flow_scores_gender, aes(x = gender_comp, y = flow_score, fill = gender_comp)) +
  geom_boxplot(alpha = 0.7) +
  facet_grid(task ~ comm) +
  labs(
    title = "Flow Scores by Gender Composition, Task and Communication",
    x = "Team Gender Composition",
    y = "Flow Score",
    fill = "Gender Composition"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_brewer(palette = "Set2")

print(p2)

# 7. Statistische Modelle

# Basis-Modell
print("\n--- Model 1: Gender Composition Only ---")
model_gender <- lm(flow_score ~ gender_comp, data = flow_scores_gender)
summary(model_gender)

# Mit Task und Communication
print("\n--- Model 2: Gender Comp + Task + Communication ---")
model_gender_task <- lm(flow_score ~ gender_comp + task + comm, data = flow_scores_gender)
summary(model_gender_task)

# Mit Schwierigkeit
print("\n--- Model 3: Gender Comp + Difficulty ---")
model_gender_diff <- lm(flow_score ~ gender_comp + difficulty, data = flow_scores_gender)
summary(model_gender_diff)

# Volles Modell mit Interaktionen
print("\n--- Model 4: Full Model with Interactions ---")
model_gender_full <- lm(flow_score ~ gender_comp * task + comm + difficulty, 
                       data = flow_scores_gender)
summary(model_gender_full)

# 8. Deskriptive Statistiken
print("\n--- Mean Flow by Gender Composition ---")
flow_scores_gender %>%
  group_by(gender_comp) %>%
  summarise(
    mean_flow = mean(flow_score, na.rm = TRUE),
    sd_flow = sd(flow_score, na.rm = TRUE),
    n = n(),
    n_teams = n_distinct(team_id)
  ) %>%
  print()

# 9. ANOVA für Gruppenunterschiede
print("\n--- ANOVA: Gender Composition ---")
anova_gender <- aov(flow_score ~ gender_comp, data = flow_scores_gender)
summary(anova_gender)

# Post-hoc Test falls signifikant
if(summary(anova_gender)[[1]][["Pr(>F)"]][1] < 0.05) {
  print("\n--- Post-hoc Test (Tukey HSD) ---")
  TukeyHSD(anova_gender)
}

# 10. Zusätzlich: Gender composition zu familiarity_long hinzufügen
familiarity_long_gender <- familiarity_long %>%
  left_join(flow_scores %>% select(participant.code, team_id) %>% distinct(), 
            by = "participant.code") %>%
  left_join(team_gender_composition %>% select(team_id, gender_comp), 
            by = "team_id")

# Model mit Familiarity und Gender Composition
print("\n--- Model 5: Familiarity + Gender Composition ---")
model_fam_gender <- lm(flow_score ~ familiarity + rec_mean + gender_comp + task + comm, 
                      data = familiarity_long_gender)
summary(model_fam_gender)

# Visualisierung: Familiarity vs Flow nach Gender Composition
p3 <- ggplot(familiarity_long_gender, aes(x = familiarity, y = flow_score, color = gender_comp)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = TRUE) +
  facet_wrap(~ task) +
  labs(
    title = "Familiarity vs. Flow by Gender Composition",
    x = "Familiarity with Teammates",
    y = "Flow Score",
    color = "Gender Composition"
  ) +
  theme_minimal() +
  scale_color_brewer(palette = "Set2")

print(p3)

# Erweiterte Analyse: Gender Composition Effekte mit Mediatoren und Familiarity

# ================================================================================
# TEIL 1: DATEN VORBEREITEN
# ================================================================================

# Gender Composition zu mediation_data hinzufügen
mediation_data_gender <- mediation_data %>%
  left_join(
    data %>% select(participant.code, team_id) %>% distinct(),
    by = "participant.code"
  ) %>%
  left_join(
    team_gender_composition %>% select(team_id, gender_comp),
    by = "team_id" 
  )

# Familiarity Daten hinzufügen (aggregiert über Farben)
familiarity_summary <- data %>%
  mutate(
    fam_mathJitsi = rowMeans(select(., fam_mathJitsi_coral, fam_mathJitsi_green, fam_mathJitsi_blue), na.rm = TRUE),
    fam_mathChat = rowMeans(select(., fam_mathChat_coral, fam_mathChat_green, fam_mathChat_blue), na.rm = TRUE),
    fam_hpJitsi = rowMeans(select(., fam_hpJitsi_coral, fam_hpJitsi_green, fam_hpJitsi_blue), na.rm = TRUE),
    fam_hpChat = rowMeans(select(., fam_hpChat_coral, fam_hpChat_green, fam_hpChat_blue), na.rm = TRUE)
  ) %>%
  select(participant.code, starts_with("fam_")) %>%
  pivot_longer(
    cols = starts_with("fam_"),
    names_to = "condition",
    values_to = "familiarity"
  ) %>%
  mutate(
    task = case_when(
      str_detect(condition, "math") ~ "Math",
      TRUE ~ "HP"
    ),
    comm = case_when(
      str_detect(condition, "Chat") ~ "Chat",
      TRUE ~ "Jitsi"
    )
  )

# Alles zusammenführen
analysis_data <- mediation_data_gender %>%
  left_join(
    familiarity_summary %>% select(participant.code, task, comm, familiarity),
    by = c("participant.code", "task", "comm")
  ) %>%
  # Recognition hinzufügen
  left_join(
    data %>% select(participant.code, rec_mean, rec_count, rec_category),
    by = "participant.code"
  )

# ================================================================================
# TEIL 2: GENDER COMPOSITION EFFEKT AUFSCHLÜSSELN
# ================================================================================

print("=== GENDER COMPOSITION ANALYSE ===\n")

# 2.1 Deskriptive Statistiken für alle Variablen nach Gender Composition
gender_descriptives <- analysis_data %>%
  group_by(gender_comp) %>%
  summarise(
    n = n(),
    # Flow
    flow_mean = mean(mean_flow_score, na.rm = TRUE),
    flow_sd = sd(mean_flow_score, na.rm = TRUE),
    # Mediatoren
    tc_mean = mean(team_composition_score, na.rm = TRUE),
    is_mean = mean(information_sharing_score, na.rm = TRUE),
    es_mean = mean(emotional_synchrony_score, na.rm = TRUE),
    # Familiarity
    fam_mean = mean(familiarity, na.rm = TRUE),
    rec_mean_avg = mean(rec_mean, na.rm = TRUE),
    .groups = "drop"
  )

print("Deskriptive Statistiken nach Gender Composition:")
print(gender_descriptives)

# 2.2 Unterschiede zwischen Tasks
gender_task_descriptives <- analysis_data %>%
  group_by(gender_comp, task) %>%
  summarise(
    n = n(),
    flow_mean = mean(mean_flow_score, na.rm = TRUE),
    flow_sd = sd(mean_flow_score, na.rm = TRUE),
    .groups = "drop"
  )

print("\nFlow nach Gender Composition und Task:")
print(gender_task_descriptives)

# ================================================================================
# TEIL 3: MEDIATIONSANALYSE - WARUM HABEN ALL-MALE TEAMS HÖHEREN FLOW?
# ================================================================================

print("\n=== MEDIATIONSANALYSE: GENDER COMPOSITION -> MEDIATOREN -> FLOW ===\n")

# 3.1 Prüfe ob Gender Composition die Mediatoren beeinflusst
mediator_models <- list(
  tc = lm(team_composition_score ~ gender_comp, data = analysis_data),
  is = lm(information_sharing_score ~ gender_comp, data = analysis_data),
  es = lm(emotional_synchrony_score ~ gender_comp, data = analysis_data),
  fam = lm(familiarity ~ gender_comp, data = analysis_data)
)

print("Gender Composition -> Mediatoren:")
for(mediator in names(mediator_models)) {
  print(paste("\n", mediator, ":"))
  print(summary(mediator_models[[mediator]])$coefficients)
}

# 3.2 Schrittweise Modelle mit Mediatoren
model_base <- lm(mean_flow_score ~ gender_comp, data = analysis_data)
model_tc <- lm(mean_flow_score ~ gender_comp + team_composition_score, data = analysis_data)
model_is <- lm(mean_flow_score ~ gender_comp + information_sharing_score, data = analysis_data)
model_es <- lm(mean_flow_score ~ gender_comp + emotional_synchrony_score, data = analysis_data)
model_fam <- lm(mean_flow_score ~ gender_comp + familiarity + rec_mean, data = analysis_data)
model_full <- lm(mean_flow_score ~ gender_comp + team_composition_score + 
                 information_sharing_score + emotional_synchrony_score + 
                 familiarity + rec_mean, data = analysis_data)

print("\nModellvergleich - Wie verändert sich der Gender Effect?")
print("Base Model (nur Gender):")
print(summary(model_base)$coefficients)
print("\nMit allen Mediatoren:")
print(summary(model_full)$coefficients)

# ================================================================================
# TEIL 4: INTERAKTIONSEFFEKTE
# ================================================================================

print("\n=== INTERAKTIONSANALYSEN ===\n")

# 4.1 Gender × Task Interaktion
model_gender_task <- lm(mean_flow_score ~ gender_comp * task, data = analysis_data)
print("Gender × Task Interaktion:")
print(summary(model_gender_task))

# 4.2 Gender × Communication Interaktion
model_gender_comm <- lm(mean_flow_score ~ gender_comp * comm, data = analysis_data)
print("\nGender × Communication Interaktion:")
print(summary(model_gender_comm))

# 4.3 Gender × Familiarity Interaktion
model_gender_fam <- lm(mean_flow_score ~ gender_comp * familiarity, data = analysis_data)
print("\nGender × Familiarity Interaktion:")
print(summary(model_gender_fam))

# ================================================================================
# TEIL 5: TASK-SPEZIFISCHE ANALYSEN
# ================================================================================

print("\n=== TASK-SPEZIFISCHE GENDER EFFEKTE ===\n")

# Math Task
math_gender_data <- analysis_data %>% filter(task == "Math")
math_gender_model <- lm(mean_flow_score ~ gender_comp + team_composition_score + 
                       information_sharing_score + emotional_synchrony_score + 
                       familiarity, data = math_gender_data)
print("MATH TASK - Gender Effect mit Mediatoren:")
print(summary(math_gender_model))

# HP Task
hp_gender_data <- analysis_data %>% filter(task == "HP")
hp_gender_model <- lm(mean_flow_score ~ gender_comp + team_composition_score + 
                     information_sharing_score + emotional_synchrony_score + 
                     familiarity, data = hp_gender_data)
print("\nHP TASK - Gender Effect mit Mediatoren:")
print(summary(hp_gender_model))

# ================================================================================
# TEIL 6: VISUALISIERUNGEN
# ================================================================================

# 6.1 Mediator-Profile nach Gender Composition
mediator_profile <- analysis_data %>%
  group_by(gender_comp) %>%
  summarise(
    `Team Composition` = mean(team_composition_score, na.rm = TRUE),
    `Information Sharing` = mean(information_sharing_score, na.rm = TRUE),
    `Emotional Synchrony` = mean(emotional_synchrony_score, na.rm = TRUE),
    `Familiarity` = mean(familiarity, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  pivot_longer(cols = -gender_comp, names_to = "Mediator", values_to = "Score")

p_profile <- ggplot(mediator_profile, aes(x = Mediator, y = Score, fill = gender_comp)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Mediator Profile by Gender Composition",
    x = "Mediator",
    y = "Mean Score",
    fill = "Gender Composition"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p_profile)

# 6.2 Flow-Mediator Beziehungen nach Gender
p_mediators <- analysis_data %>%
  pivot_longer(
    cols = c(team_composition_score, information_sharing_score, 
             emotional_synchrony_score, familiarity),
    names_to = "mediator",
    values_to = "mediator_value"
  ) %>%
  mutate(
    mediator = case_when(
      mediator == "team_composition_score" ~ "Team Composition",
      mediator == "information_sharing_score" ~ "Information Sharing",
      mediator == "emotional_synchrony_score" ~ "Emotional Synchrony",
      mediator == "familiarity" ~ "Familiarity"
    )
  ) %>%
  ggplot(aes(x = mediator_value, y = mean_flow_score, color = gender_comp)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~ mediator, scales = "free_x") +
  labs(
    title = "Flow-Mediator Relationships by Gender Composition",
    x = "Mediator Score",
    y = "Flow Score",
    color = "Gender Composition"
  ) +
  theme_minimal()

print(p_mediators)

# ================================================================================
# TEIL 7: PFADANALYSE ZUSAMMENFASSUNG
# ================================================================================

print("\n=== PFADANALYSE ZUSAMMENFASSUNG ===\n")

# Berechne indirekte Effekte für all-male vs. mixed teams
for(mediator in c("team_composition_score", "information_sharing_score", 
                  "emotional_synchrony_score", "familiarity")) {
  
  # a-Pfad: Gender -> Mediator
  a_model <- lm(as.formula(paste(mediator, "~ gender_comp")), data = analysis_data)
  a_coef <- coef(a_model)["gender_compmixed"]
  
  # b-Pfad: Mediator -> Flow (kontrolliert für Gender)
  b_model <- lm(as.formula(paste("mean_flow_score ~ gender_comp +", mediator)), 
                data = analysis_data)
  b_coef <- coef(b_model)[mediator]
  
  # Indirekter Effekt
  indirect <- a_coef * b_coef
  
  print(paste(mediator, ":"))
  print(paste("  a-Pfad (all-male -> mixed):", round(a_coef, 3)))
  print(paste("  b-Pfad (Mediator -> Flow):", round(b_coef, 3)))
  print(paste("  Indirekter Effekt:", round(indirect, 3)))
  print("")
}

# ================================================================================
# TEIL 8: EMPFEHLUNGEN FÜR WEITERE ANALYSEN
# ================================================================================

print("\n=== EMPFEHLUNGEN ===\n")
print("1. Team Composition Score zeigt möglicherweise Unterschiede - all-male Teams")
print("   könnten sich als besser aufeinander abgestimmt wahrnehmen")
print("2. Emotional Synchrony könnte in homogenen Teams höher sein")
print("3. Prüfe ob der Effekt taskspezifisch ist (Math vs. HP)")
print("4. Untersuche ob Familiarity den Gender-Effekt moderiert")
print("5. Betrachte individuelle Gender-Effekte (männliche vs. weibliche Teilnehmer)")

# Zusätzlich: Prüfe individuelle Geschlechtseffekte
individual_gender <- data %>%
  select(participant.code, individual_gender = Intro.1.player.gender) %>%
  right_join(analysis_data, by = "participant.code")

model_individual <- lm(mean_flow_score ~ individual_gender + gender_comp, 
                      data = individual_gender)
print("\nIndividuelles Geschlecht + Team Composition:")
print(summary(model_individual))

```

Regression of flow on emoji count

```{r}
# Emoji-Analyse in Chat-Logs und Zusammenhang mit Flow

library(stringr)

# ================================================================================
# TEIL 1: EMOJI-EXTRAKTION AUS CHAT-LOGS
# ================================================================================

# Definiere die Text-Emojis, die wir suchen
emoji_patterns <- c(
  # Positive Emojis
  ":\\)", ";\\)", ":D", ";D", "[xX]D", "XD", "<3", ":P", ":p",
  # Negative/Neutrale Emojis  
  ":/", ":\\(", ":\\|", ":o", ":O",
  # Varianten mit Bindestrich (falls vorhanden)
  ":-\\)", ";-\\)", ":-D", ";-D", ":-/", ":-\\(", ":-\\|", ":-[oO]", ":-[pP]"
)

# Erstelle ein einzelnes Pattern für alle Emojis
emoji_regex <- paste0("(", paste(emoji_patterns, collapse = "|"), ")")

# Funktion zum Zählen von Emojis in einem Text
count_emojis <- function(text) {
  if(is.na(text) || text == "") return(0)
  matches <- str_extract_all(text, emoji_regex)[[1]]
  return(length(matches))
}

# Funktion zum Extrahieren spezifischer Emoji-Typen
extract_emoji_types <- function(text) {
  if(is.na(text) || text == "") {
    return(list(
      positive = 0,
      negative = 0,
      neutral = 0,
      total = 0
    ))
  }
  
  # Positive Emojis
  positive_pattern <- "(:\\)|;\\)|:D|;D|[xX]D|XD|<3|:P|:p|:-\\)|;-\\)|:-D|;-D|:-[pP])"
  positive_count <- length(str_extract_all(text, positive_pattern)[[1]])
  
  # Negative Emojis
  negative_pattern <- "(:\\(|:-\\()"
  negative_count <- length(str_extract_all(text, negative_pattern)[[1]])
  
  # Neutrale Emojis
  neutral_pattern <- "(:/|:\\||:o|:O|:-/|:-\\||:-[oO])"
  neutral_count <- length(str_extract_all(text, neutral_pattern)[[1]])
  
  return(list(
    positive = positive_count,
    negative = negative_count,
    neutral = neutral_count,
    total = positive_count + negative_count + neutral_count
  ))
}

# ================================================================================
# TEIL 2: CHAT-LOG DATEN EXTRAHIEREN UND ANALYSIEREN
# ================================================================================

# Extrahiere Math Chat Logs
math_chat_logs <- data %>%
  select(participant.code, team_id, starts_with("mathChat.") & ends_with(".player.chat_log"))

# Zähle Emojis für jede Runde
math_emoji_counts <- data.frame()

for(round in 1:6) {
  col_name <- paste0("mathChat.", round, ".player.chat_log")
  
  if(col_name %in% names(math_chat_logs)) {
    round_data <- math_chat_logs %>%
      select(participant.code, team_id, chat_log = all_of(col_name)) %>%
      mutate(
        round = round,
        emoji_data = map(chat_log, extract_emoji_types)
      ) %>%
      mutate(
        emoji_total = map_dbl(emoji_data, ~ .x$total),
        emoji_positive = map_dbl(emoji_data, ~ .x$positive),
        emoji_negative = map_dbl(emoji_data, ~ .x$negative),
        emoji_neutral = map_dbl(emoji_data, ~ .x$neutral),
        chat_length = nchar(chat_log)
      ) %>%
      select(-emoji_data)
    
    math_emoji_counts <- bind_rows(math_emoji_counts, round_data)
  }
}

# Aggregiere über alle Runden pro Teilnehmer
math_emoji_summary <- math_emoji_counts %>%
  group_by(participant.code, team_id) %>%
  summarise(
    total_emojis = sum(emoji_total, na.rm = TRUE),
    positive_emojis = sum(emoji_positive, na.rm = TRUE),
    negative_emojis = sum(emoji_negative, na.rm = TRUE),
    neutral_emojis = sum(emoji_neutral, na.rm = TRUE),
    total_chat_length = sum(chat_length, na.rm = TRUE),
    emoji_density = ifelse(total_chat_length > 0, total_emojis / total_chat_length * 100, 0),
    task = "Math",
    comm = "Chat",
    .groups = "drop"
  )

# Dasselbe für Hidden Profile Chat Logs
hp_chat_logs <- data %>%
  select(participant.code, team_id, starts_with("HiddenProfile_Chat.") & ends_with(".player.chat_log"))

hp_emoji_counts <- data.frame()

for(round in 1:3) {
  col_name <- paste0("HiddenProfile_Chat.", round, ".player.chat_log")
  
  if(col_name %in% names(hp_chat_logs)) {
    round_data <- hp_chat_logs %>%
      select(participant.code, team_id, chat_log = all_of(col_name)) %>%
      mutate(
        round = round,
        emoji_data = map(chat_log, extract_emoji_types)
      ) %>%
      mutate(
        emoji_total = map_dbl(emoji_data, ~ .x$total),
        emoji_positive = map_dbl(emoji_data, ~ .x$positive),
        emoji_negative = map_dbl(emoji_data, ~ .x$negative),
        emoji_neutral = map_dbl(emoji_data, ~ .x$neutral),
        chat_length = nchar(chat_log)
      ) %>%
      select(-emoji_data)
    
    hp_emoji_counts <- bind_rows(hp_emoji_counts, round_data)
  }
}

# Aggregiere HP Emojis
hp_emoji_summary <- hp_emoji_counts %>%
  group_by(participant.code, team_id) %>%
  summarise(
    total_emojis = sum(emoji_total, na.rm = TRUE),
    positive_emojis = sum(emoji_positive, na.rm = TRUE),
    negative_emojis = sum(emoji_negative, na.rm = TRUE),
    neutral_emojis = sum(emoji_neutral, na.rm = TRUE),
    total_chat_length = sum(chat_length, na.rm = TRUE),
    emoji_density = ifelse(total_chat_length > 0, total_emojis / total_chat_length * 100, 0),
    task = "HP",
    comm = "Chat",
    .groups = "drop"
  )

# Kombiniere Math und HP Emoji-Daten
all_emoji_summary <- bind_rows(math_emoji_summary, hp_emoji_summary)

# ================================================================================
# TEIL 3: TEAM-LEVEL EMOJI ANALYSE
# ================================================================================

# Aggregiere auf Team-Ebene
team_emoji_summary <- all_emoji_summary %>%
  group_by(team_id, task) %>%
  summarise(
    team_total_emojis = sum(total_emojis),
    team_positive_emojis = sum(positive_emojis),
    team_negative_emojis = sum(negative_emojis),
    team_emoji_density = mean(emoji_density),
    emoji_users = sum(total_emojis > 0),
    team_size = n(),
    emoji_user_ratio = emoji_users / team_size,
    .groups = "drop"
  )

# ================================================================================
# TEIL 4: EMOJI-DATEN MIT FLOW UND MEDIATOREN VERBINDEN
# ================================================================================

# Verbinde mit Flow-Scores
emoji_flow_data <- all_emoji_summary %>%
  left_join(
    flow_scores %>%
      filter(comm == "Chat") %>%
      group_by(participant.code, task) %>%
      summarise(mean_flow_score = mean(flow_score, na.rm = TRUE), .groups = "drop"),
    by = c("participant.code", "task")
  )

# Verbinde mit Mediatoren (aus mediation_data)
emoji_mediation_data <- emoji_flow_data %>%
  left_join(
    mediation_data %>%
      filter(comm == "Chat") %>%
      select(participant.code, task, team_composition_score, 
             information_sharing_score, emotional_synchrony_score),
    by = c("participant.code", "task")
  )

# ================================================================================
# TEIL 5: STATISTISCHE ANALYSEN
# ================================================================================

print("=== EMOJI-NUTZUNG DESKRIPTIVE STATISTIKEN ===\n")

# Gesamtübersicht
emoji_overview <- all_emoji_summary %>%
  group_by(task) %>%
  summarise(
    n_participants = n(),
    emoji_users = sum(total_emojis > 0),
    emoji_user_percentage = emoji_users / n_participants * 100,
    mean_emojis = mean(total_emojis),
    sd_emojis = sd(total_emojis),
    mean_positive = mean(positive_emojis),
    mean_negative = mean(negative_emojis),
    mean_neutral = mean(neutral_emojis),
    .groups = "drop"
  )

print("Emoji-Nutzung nach Task:")
print(emoji_overview)

# ================================================================================
# TEIL 6: EMOJI-FLOW ZUSAMMENHÄNGE
# ================================================================================

print("\n=== EMOJI-FLOW ZUSAMMENHÄNGE ===\n")

# Individuelle Ebene
emoji_flow_models <- list(
  math_total = lm(mean_flow_score ~ total_emojis, 
                  data = emoji_mediation_data %>% filter(task == "Math")),
  hp_total = lm(mean_flow_score ~ total_emojis, 
                data = emoji_mediation_data %>% filter(task == "HP")),
  math_types = lm(mean_flow_score ~ positive_emojis + negative_emojis + neutral_emojis, 
                  data = emoji_mediation_data %>% filter(task == "Math")),
  hp_types = lm(mean_flow_score ~ positive_emojis + negative_emojis + neutral_emojis, 
                data = emoji_mediation_data %>% filter(task == "HP"))
)

print("Math Task - Emoji Total -> Flow:")
print(summary(emoji_flow_models$math_total))

print("\nHP Task - Emoji Total -> Flow:")
print(summary(emoji_flow_models$hp_total))

print("\nMath Task - Emoji Types -> Flow:")
print(summary(emoji_flow_models$math_types))

# Team-Ebene Analyse
team_flow_data <- team_emoji_summary %>%
  left_join(
    emoji_flow_data %>%
      group_by(team_id, task) %>%
      summarise(team_mean_flow = mean(mean_flow_score, na.rm = TRUE), .groups = "drop"),
    by = c("team_id", "task")
  )

team_emoji_model <- lm(team_mean_flow ~ team_emoji_density + emoji_user_ratio + task, 
                      data = team_flow_data)
print("\nTeam-Level: Emoji Density & User Ratio -> Flow:")
print(summary(team_emoji_model))

# ================================================================================
# TEIL 7: MEDIATIONSANALYSE - EMOJIS -> EMOTIONAL SYNCHRONY -> FLOW
# ================================================================================

print("\n=== MEDIATIONSANALYSE: EMOJIS -> EMOTIONAL SYNCHRONY -> FLOW ===\n")

# Pfad a: Emojis -> Emotional Synchrony
path_a_model <- lm(emotional_synchrony_score ~ total_emojis, data = emoji_mediation_data)
print("Pfad a (Emojis -> Emotional Synchrony):")
print(summary(path_a_model))

# Pfad c: Emojis -> Flow (Total Effect)
path_c_model <- lm(mean_flow_score ~ total_emojis, data = emoji_mediation_data)
print("\nPfad c (Emojis -> Flow, Total Effect):")
print(summary(path_c_model))

# Pfad c': Emojis -> Flow (kontrolliert für Emotional Synchrony)
path_c_prime_model <- lm(mean_flow_score ~ total_emojis + emotional_synchrony_score, 
                         data = emoji_mediation_data)
print("\nPfad c' (Emojis -> Flow, kontrolliert für ES):")
print(summary(path_c_prime_model))

# Berechne indirekten Effekt
a_coef <- coef(path_a_model)["total_emojis"]
b_coef <- coef(path_c_prime_model)["emotional_synchrony_score"]
indirect_effect <- a_coef * b_coef

print(paste("\nIndirekter Effekt (a × b):", round(indirect_effect, 4)))

# ================================================================================
# TEIL 8: VISUALISIERUNGEN
# ================================================================================

# 8.1 Emoji-Nutzung vs Flow
p1 <- ggplot(emoji_flow_data, aes(x = total_emojis, y = mean_flow_score)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = TRUE) +
  facet_wrap(~ task) +
  labs(
    title = "Emoji Usage vs. Flow Score",
    x = "Total Emojis Used",
    y = "Mean Flow Score"
  ) +
  theme_minimal()

print(p1)

# 8.2 Emoji-Typen Verteilung
emoji_type_data <- emoji_mediation_data %>%
  select(participant.code, task, positive_emojis, negative_emojis, neutral_emojis) %>%
  pivot_longer(cols = c(positive_emojis, negative_emojis, neutral_emojis),
               names_to = "emoji_type", values_to = "count") %>%
  mutate(emoji_type = gsub("_emojis", "", emoji_type))

p2 <- ggplot(emoji_type_data %>% filter(count > 0), 
             aes(x = emoji_type, y = count, fill = emoji_type)) +
  geom_boxplot() +
  facet_wrap(~ task) +
  labs(
    title = "Distribution of Emoji Types",
    x = "Emoji Type",
    y = "Count"
  ) +
  theme_minimal() +
  scale_fill_brewer(palette = "Set2")

print(p2)

# 8.3 Team Emoji Density vs Team Flow
p3 <- ggplot(team_flow_data, aes(x = team_emoji_density, y = team_mean_flow)) +
  geom_point(aes(size = emoji_user_ratio), alpha = 0.7) +
  geom_smooth(method = "lm", se = TRUE) +
  facet_wrap(~ task) +
  labs(
    title = "Team Emoji Density vs. Team Flow",
    x = "Team Emoji Density (%)",
    y = "Team Mean Flow Score",
    size = "Emoji User Ratio"
  ) +
  theme_minimal()

print(p3)

# 8.4 Mediationsvisualisierung
p4 <- ggplot(emoji_mediation_data, aes(x = total_emojis, y = emotional_synchrony_score)) +
  geom_point(aes(color = mean_flow_score), alpha = 0.6) +
  geom_smooth(method = "lm", se = TRUE) +
  scale_color_gradient(low = "blue", high = "red") +
  labs(
    title = "Emojis -> Emotional Synchrony (colored by Flow)",
    x = "Total Emojis",
    y = "Emotional Synchrony Score",
    color = "Flow Score"
  ) +
  theme_minimal()

print(p4)

# ================================================================================
# TEIL 9: BEISPIEL-CHATLOGS MIT VIELEN EMOJIS
# ================================================================================

print("\n=== BEISPIELE FÜR EMOJI-REICHE CHATS ===\n")

# Finde Top 5 emoji-reichste Nachrichten
top_emoji_examples <- bind_rows(
  math_emoji_counts %>% mutate(task = "Math"),
  hp_emoji_counts %>% mutate(task = "HP")
) %>%
  filter(emoji_total > 0) %>%
  arrange(desc(emoji_total)) %>%
  slice_head(n = 5) %>%
  select(participant.code, task, round, emoji_total, chat_log)

print("Top 5 Nachrichten mit den meisten Emojis:")
for(i in 1:nrow(top_emoji_examples)) {
  cat(paste0("\n", i, ". Participant: ", top_emoji_examples$participant.code[i], 
             " (", top_emoji_examples$task[i], " Round ", top_emoji_examples$round[i], 
             ", ", top_emoji_examples$emoji_total[i], " emojis)\n"))
  cat(paste0("   \"", substr(top_emoji_examples$chat_log[i], 1, 100), "...\"\n"))
}

# ================================================================================
# TEIL 10: ZUSAMMENFASSUNG
# ================================================================================

print("\n=== ZUSAMMENFASSUNG DER EMOJI-ANALYSE ===\n")

# Berechne Korrelationen
cor_emoji_flow <- cor(emoji_flow_data$total_emojis, emoji_flow_data$mean_flow_score, 
                     use = "complete.obs")
cor_emoji_es <- cor(emoji_mediation_data$total_emojis, 
                   emoji_mediation_data$emotional_synchrony_score, 
                   use = "complete.obs")

print(paste("Korrelation Emojis-Flow:", round(cor_emoji_flow, 3)))
print(paste("Korrelation Emojis-Emotional Synchrony:", round(cor_emoji_es, 3)))
print(paste("Prozent der Teilnehmer die Emojis nutzen:", 
            round(sum(all_emoji_summary$total_emojis > 0) / nrow(all_emoji_summary) * 100, 1), "%"))

```



