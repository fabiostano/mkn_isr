---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

Setup & Import

```{r}
library(tidyverse)
library(lme4)
library(lmerTest)
library(sjPlot)
library(data.table)
library(dplyr)
library(plyr)
library(tidyr)
library(ggplot2)
library(stringr)
library(readr)
library(psych)
library(performance)
library(corrr)
library(purrr)
library(corrplot)
library(multilevel)
library(emmeans)
library(tibble)
library(rmcorr)

data <- read.csv("otree_cleaned1.csv")
data_old <- read.csv("flow_reports_exp1.csv")
data_old_rounds <- read.csv("round_reports_exp1.csv")
data_old_final <- read.csv("final_reports_exp1.csv")
```

Data quality check

```{r}
# Datenqualitäts-Check für unaufmerksame Antworten
# ================================================================================

# 1. Straight-Lining Detection (immer gleiche Antwort)
# ================================================================================

# Funktion zur Berechnung der Antwort-Variabilität
calculate_response_variability <- function(row_data) {
  # Nur numerische Spalten nehmen
  numeric_data <- row_data[sapply(row_data, is.numeric)]
  
  # Standardabweichung der Antworten pro Person
  sd_responses <- sd(unlist(numeric_data), na.rm = TRUE)
  
  # Anzahl unterschiedlicher Werte
  n_unique <- length(unique(unlist(numeric_data)))
  
  print(list(sd = sd_responses, n_unique = n_unique))
}

# Für Flow-Items (FSS)
fss_columns <- names(data)[grep("fss\\d+", names(data))]

if(length(fss_columns) > 0) {
  fss_variability <- data %>%
    select(participant.code, all_of(fss_columns)) %>%
    group_by(participant.code) %>%
    summarise(
      n_fss_items = sum(!is.na(across(all_of(fss_columns)))),
      fss_sd = sd(c_across(all_of(fss_columns)), na.rm = TRUE),
      fss_n_unique = n_distinct(c_across(all_of(fss_columns)), na.rm = TRUE),
      fss_straightline = fss_n_unique == 1  # Nur eine Antwortoption verwendet
    )
  
  print("Teilnehmer mit Straight-Lining bei Flow-Items:")
  print(filter(fss_variability, fss_straightline))
}

# 2. Inkonsistenz-Prüfung
# ================================================================================

# Beispiel: Prüfen ob Flow Proneness Items konsistent beantwortet wurden
# (Item 1 ist invers kodiert in jeder Dimension)
if(all(c("Outro.1.player.fpw1", "Outro.1.player.fpw2") %in% names(data))) {
  consistency_check <- data %>%
    select(participant.code, 
           fpw1 = Outro.1.player.fpw1, fpw2 = Outro.1.player.fpw2,
           fph1 = Outro.1.player.fph1, fph2 = Outro.1.player.fph2,
           fpl1 = Outro.1.player.fpl1, fpl2 = Outro.1.player.fpl2) %>%
    mutate(
      # Inverse Items umkehren für Konsistenzprüfung
      fpw1_rev = 6 - fpw1,
      fph1_rev = 6 - fph1,
      fpl1_rev = 6 - fpl1,
      
      # Große Differenzen zwischen verwandten Items?
      fpw_diff = abs(fpw1_rev - fpw2),
      fph_diff = abs(fph1_rev - fph2),
      fpl_diff = abs(fpl1_rev - fpl2),
      
      max_diff = pmax(fpw_diff, fph_diff, fpl_diff, na.rm = TRUE),
      inconsistent = max_diff >= 4  # Differenz von 4+ auf 5-Punkt Skala
    )
  
  print("\nTeilnehmer mit inkonsistenten Antworten:")
  print(filter(consistency_check, inconsistent))
}
```

Descriptive statistics

```{r}
# Demografische Übersicht
demographics <- data %>%
  summarise(
    # Geschlecht
    female_pct = mean(Intro.1.player.gender == "Female", na.rm = TRUE) * 100,
    male_pct = mean(Intro.1.player.gender == "Male", na.rm = TRUE) * 100,
    # Alter
    age_mean = mean(Intro.1.player.age, na.rm = TRUE),
    age_sd = sd(Intro.1.player.age, na.rm = TRUE),
    # Händigkeit
    right_handed_pct = mean(Intro.1.player.dominant_hand == "Right", na.rm = TRUE) * 100
  )

print(round(demographics, 1))

# Englischkenntnisse detailliert
cat("\nEnglischkenntnisse:\n")
english_table <- prop.table(table(data$Intro.1.player.english)) * 100
print(round(english_table, 1))

# Occupation und Field of Study - nur Top 5
cat("\nTop 5 Occupations:\n")
head(sort(table(data$Intro.1.player.occupation), decreasing = TRUE), 5)

cat("\nTop 5 Fields of Study:\n")
head(sort(table(data$Intro.1.player.field_of_study), decreasing = TRUE), 5)
```

Manipulation checks for team goal, team member independence, and ability to coordinate work

```{r}
# mathChat - Interdependence
int_mathChat <- data %>%
  select(mathChat.6.player.int1, mathChat.6.player.int2, mathChat.6.player.int3)
alpha(int_mathChat, check.keys=TRUE)

# mathChat - Common Goal
cg_mathChat <- data %>%
  select(mathChat.6.player.cg1, mathChat.6.player.cg2, mathChat.6.player.cg3, 
         mathChat.6.player.cg4, mathChat.6.player.cg5, mathChat.6.player.cg6)
alpha(cg_mathChat, check.keys=TRUE)

# mathChat - Means for Coordination
mc_mathChat <- data %>%
  select(mathChat.6.player.mc1, mathChat.6.player.mc2)
alpha(mc_mathChat, check.keys=TRUE) 

# mathJitsi - Interdependence
int_mathJitsi <- data %>%
  select(mathJitsi.6.player.int1, mathJitsi.6.player.int2, mathJitsi.6.player.int3)
alpha(int_mathJitsi, check.keys=TRUE)

# mathJitsi - Common Goal
cg_mathJitsi <- data %>%
  select(mathJitsi.6.player.cg1, mathJitsi.6.player.cg2, mathJitsi.6.player.cg3, 
         mathJitsi.6.player.cg4, mathJitsi.6.player.cg5, mathJitsi.6.player.cg6)
alpha(cg_mathJitsi, check.keys=TRUE)

# mathJitsi - Means for Coordination
mc_mathJitsi <- data %>%
  select(mathJitsi.6.player.mc1, mathJitsi.6.player.mc2)
alpha(mc_mathJitsi, check.keys=TRUE) 

# HiddenProfile_Chat - Interdependence
int_HiddenProfile_Chat <- data %>%
  select(HiddenProfile_Chat.3.player.int1, HiddenProfile_Chat.3.player.int2, HiddenProfile_Chat.3.player.int3)
alpha(int_HiddenProfile_Chat, check.keys=TRUE)

# HiddenProfile_Chat - Common Goal
cg_HiddenProfile_Chat <- data %>%
  select(HiddenProfile_Chat.3.player.cg1, HiddenProfile_Chat.3.player.cg2, HiddenProfile_Chat.3.player.cg3, 
         HiddenProfile_Chat.3.player.cg4, HiddenProfile_Chat.3.player.cg5, HiddenProfile_Chat.3.player.cg6)
alpha(cg_HiddenProfile_Chat, check.keys=TRUE)

# HiddenProfile_Chat - Means for Coordination
mc_HiddenProfile_Chat <- data %>%
  select(HiddenProfile_Chat.3.player.mc1, HiddenProfile_Chat.3.player.mc2)
alpha(mc_HiddenProfile_Chat, check.keys=TRUE) 

# HiddenProfile_Jitsi - Interdependence
int_HiddenProfile_Jitsi <- data %>%
  select(HiddenProfile_Jitsi.3.player.int1, HiddenProfile_Jitsi.3.player.int2, HiddenProfile_Jitsi.3.player.int3)
alpha(int_HiddenProfile_Jitsi, check.keys=TRUE)

# HiddenProfile_Jitsi - Common Goal
cg_HiddenProfile_Jitsi <- data %>%
  select(HiddenProfile_Jitsi.3.player.cg1, HiddenProfile_Jitsi.3.player.cg2, HiddenProfile_Jitsi.3.player.cg3, 
         HiddenProfile_Jitsi.3.player.cg4, HiddenProfile_Jitsi.3.player.cg5, HiddenProfile_Jitsi.3.player.cg6)
alpha(cg_HiddenProfile_Jitsi, check.keys=TRUE)

# HiddenProfile_Jitsi - Means for Coordination
mc_HiddenProfile_Jitsi <- data %>%
  select(HiddenProfile_Jitsi.3.player.mc1, HiddenProfile_Jitsi.3.player.mc2)
alpha(mc_HiddenProfile_Jitsi, check.keys=TRUE) 

# Items umpolen (nur die mit negativem Vorzeichen aus der Alpha-Analyse)
data <- data %>%
  mutate(
    # mathChat Items umpolen:
    mathChat.6.player.int1_rev = 8 - mathChat.6.player.int1,
    mathChat.6.player.cg1_rev = 8 - mathChat.6.player.cg1,
    mathChat.6.player.cg3_rev = 8 - mathChat.6.player.cg3,
    mathChat.6.player.cg5_rev = 8 - mathChat.6.player.cg5,
    mathChat.6.player.mc1_rev = 8 - mathChat.6.player.mc1
  )

# Skalenmittelwerte je Konstrukt pro Treatment
data <- data %>%
  mutate(
    mc_mathChat = rowMeans(select(., mathChat.6.player.mc1_rev, mathChat.6.player.mc2), na.rm = TRUE),
    mc_mathJitsi = rowMeans(select(., mathJitsi.6.player.mc1, mathJitsi.6.player.mc2), na.rm = TRUE),
    mc_hpChat = rowMeans(select(., HiddenProfile_Chat.3.player.mc1, HiddenProfile_Chat.3.player.mc2), na.rm = TRUE),
    mc_hpJitsi = rowMeans(select(., HiddenProfile_Jitsi.3.player.mc1, HiddenProfile_Jitsi.3.player.mc2), na.rm = TRUE),

    int_mathChat = rowMeans(select(., mathChat.6.player.int1_rev, mathChat.6.player.int2, mathChat.6.player.int3), na.rm = TRUE),
    int_mathJitsi = rowMeans(select(., mathJitsi.6.player.int1, mathJitsi.6.player.int2, mathJitsi.6.player.int3), na.rm = TRUE),
    int_hpChat = rowMeans(select(., HiddenProfile_Chat.3.player.int1, HiddenProfile_Chat.3.player.int2, HiddenProfile_Chat.3.player.int3), na.rm = TRUE),
    int_hpJitsi = rowMeans(select(., HiddenProfile_Jitsi.3.player.int1, HiddenProfile_Jitsi.3.player.int2, HiddenProfile_Jitsi.3.player.int3), na.rm = TRUE),

    cg_mathChat = rowMeans(select(., mathChat.6.player.cg1_rev, mathChat.6.player.cg2, mathChat.6.player.cg3_rev, 
                                  mathChat.6.player.cg4, mathChat.6.player.cg5_rev, mathChat.6.player.cg6), na.rm = TRUE),
    cg_mathJitsi = rowMeans(select(., starts_with("mathJitsi.6.player.cg")), na.rm = TRUE),
    cg_hpChat = rowMeans(select(., starts_with("HiddenProfile_Chat.3.player.cg")), na.rm = TRUE),
    cg_hpJitsi = rowMeans(select(., starts_with("HiddenProfile_Jitsi.3.player.cg")), na.rm = TRUE)
  )

mc_long <- data %>%
  select(mc_mathChat, mc_mathJitsi, mc_hpChat, mc_hpJitsi) %>%
  pivot_longer(cols = everything(), names_to = "Treatment", values_to = "Score") %>%
  mutate(Treatment = recode(Treatment,
                            mc_mathChat = "Math – Chat",
                            mc_mathJitsi = "Math – Jitsi",
                            mc_hpChat = "HiddenProfile – Chat",
                            mc_hpJitsi = "HiddenProfile – Jitsi"))

ggplot(mc_long, aes(x = Treatment, y = Score)) +
  geom_boxplot(fill = "skyblue", alpha = 0.7) +
  theme_minimal() +
  labs(title = "Means for Coordination (MC) – nach Treatment",
       y = "Skalenwert (1–7)", x = NULL) +
  ylim(1, 7)

int_long <- data %>%
  select(int_mathChat, int_mathJitsi, int_hpChat, int_hpJitsi) %>%
  pivot_longer(cols = everything(), names_to = "Treatment", values_to = "Score") %>%
  mutate(Treatment = recode(Treatment,
                            int_mathChat = "Math – Chat",
                            int_mathJitsi = "Math – Jitsi",
                            int_hpChat = "HiddenProfile – Chat",
                            int_hpJitsi = "HiddenProfile – Jitsi"))

ggplot(int_long, aes(x = Treatment, y = Score)) +
  geom_boxplot(fill = "orchid", alpha = 0.7) +
  theme_minimal() +
  labs(title = "Team Member Interdependence (INT) – nach Treatment",
       y = "Skalenwert (1–7)", x = NULL) +
  ylim(1, 7)

cg_long <- data %>%
  select(cg_mathChat, cg_mathJitsi, cg_hpChat, cg_hpJitsi) %>%
  pivot_longer(cols = everything(), names_to = "Treatment", values_to = "Score") %>%
  mutate(Treatment = recode(Treatment,
                            cg_mathChat = "Math – Chat",
                            cg_mathJitsi = "Math – Jitsi",
                            cg_hpChat = "HiddenProfile – Chat",
                            cg_hpJitsi = "HiddenProfile – Jitsi"))

ggplot(cg_long, aes(x = Treatment, y = Score)) +
  geom_boxplot(fill = "seagreen3", alpha = 0.7) +
  theme_minimal() +
  labs(title = "Common Goal (CG) – nach Treatment",
       y = "Skalenwert (1–7)", x = NULL) +
  ylim(1, 7)

```

Manipulation check for difficulty

```{r}
# Funktion um z.B. "['A', 'O', 'F', 'B']" in echten Vektor zu verwandeln
parse_order_string <- function(s) {
  s %>%
    str_remove_all("\\[|\\]|'|\"") %>%
    str_split(",\\s*") %>%
    unlist()
}

# Mapping Math-Codes zu Labels
map_math_difficulty <- function(code) {
  recode(code,
         "B" = "Easy",
         "A" = "Optimal_Selected",
         "F" = "Optimal_Calibrated",
         "O" = "Hard")
}

data <- data %>%
  mutate(
    math_order = map(as.character(participant.condition_order), parse_order_string) %>%
                   map(~ map_chr(.x, map_math_difficulty)),
    hp_order   = map(as.character(participant.hp_condition_order), parse_order_string) %>%
                   map(str_to_title)
  )

# MATH TASK
math_difficulty_long <- data %>%
  select(participant.code, math_order,
         starts_with("mathChat.3.player.csb"),
         starts_with("mathChat.4.player.csb"),
         starts_with("mathChat.5.player.csb"),
         starts_with("mathChat.6.player.csb"),
         starts_with("mathJitsi.3.player.csb"),
         starts_with("mathJitsi.4.player.csb"),
         starts_with("mathJitsi.5.player.csb"),
         starts_with("mathJitsi.6.player.csb")) %>%
  pivot_longer(cols = -c(participant.code, math_order),
               names_to = "var", values_to = "csb") %>%
  mutate(
    round = str_extract(var, "\\d+"),
    comm = ifelse(str_detect(var, "Chat"), "Chat", "Jitsi"),
    index = as.integer(round) - 2,  # weil math.3 = erste relevante Runde
    difficulty = map2_chr(math_order, index, ~ .x[.y])
  ) %>%
  drop_na(csb)

# HP TASK
hp_difficulty_long <- data %>%
  select(participant.code, hp_order,
         starts_with("HiddenProfile_Chat."),
         starts_with("HiddenProfile_Jitsi.")) %>%
  pivot_longer(cols = matches("HiddenProfile_.*\\.player\\.csb[12]"),
               names_to = "var", values_to = "csb") %>%
  mutate(
    round = str_extract(var, "(?<=\\.)\\d+"),
    comm = ifelse(str_detect(var, "Chat"), "Chat", "Jitsi"),
    index = as.integer(round),  # hier ist Runde = Index
    difficulty = map2_chr(hp_order, index, ~ .x[.y])
  ) %>%
  drop_na(csb)

# X-Achse splitten in zwei ästhetischere Achsen:
ggplot(math_difficulty_long, aes(x = difficulty, y = csb, fill = comm)) +
  geom_boxplot(position = position_dodge(width = 0.75)) +
  labs(title = "Math Task", x = "Difficulty", y = "Subjective Difficulty") +
  theme_minimal()

ggplot(hp_difficulty_long, aes(x = difficulty, y = csb, fill = comm)) +
  geom_boxplot(position = position_dodge(width = 0.75)) +
  labs(title = "Hidden Profile Task", x = "Difficulty", y = "Subjective Difficulty") +
  theme_minimal()

```

Internal consistency check for flow construct and flow score calculation

```{r}
# Reihenfolge-Parsing-Helfer
parse_order_string <- function(s) {
  s %>%
    str_remove_all("\\[|\\]|'|\"") %>%
    str_split(",\\s*") %>%
    unlist()
}

# Reihenfolgen extrahieren
data <- data %>%
  mutate(
    math_order = map(participant.condition_order, parse_order_string),
    hp_order   = map(participant.hp_condition_order, parse_order_string)
  )

# Funktion zum Remappen der Items von Runde -> Schwierigkeitsstufe
remap_fss_items <- function(df, prefix, rounds, difficulty_map) {
  out <- list()
  for (i in seq_along(rounds)) {
    r <- rounds[i]
    diff_code <- difficulty_map[i]
    for (j in 1:9) {
      old_name <- sprintf("%s.%d.player.fss%02d", prefix, r, j)
      new_name <- sprintf("%s.%s.player.fss%02d", prefix, diff_code, j)
      out[[new_name]] <- if (old_name %in% names(df)) df[[old_name]][1] else NA
    }
  }
  # Eine Zeile mit vielen Spalten zurückgeben
  return(as_tibble(out))
}

# Spaltennamen für Mathe-Items extrahieren
math_cols <- names(data)[startsWith(names(data), "math")]

# Mathe-Daten remappen
math_data <- data %>%
  mutate(math_items = pmap(
    c(list(math_order), select(., all_of(math_cols))),
    function(order, ...) {
      df <- tibble(...)
      bind_cols(
        remap_fss_items(df, "mathJitsi", 3:6, order),
        remap_fss_items(df, "mathChat", 3:6, order)
      )
    }
  ))

# Spaltennamen für Hidden Profile-Items extrahieren
hp_cols <- names(data)[startsWith(names(data), "HiddenProfile_")]

# HP-Daten remappen
hp_data <- data %>%
  mutate(hp_items = pmap(
    c(list(hp_order), select(., all_of(hp_cols))),
    function(order, ...) {
      df <- tibble(...)
      bind_cols(
        remap_fss_items(df, "HiddenProfile_Jitsi", 1:3, order),
        remap_fss_items(df, "HiddenProfile_Chat", 1:3, order)
      )
    }
  ))

# Funktion für Cronbach's Alpha und Mittelwert
aggregate_flow <- function(df, prefix, difficulties) {
  results <- list()
  
  # NEU: Alpha-Ausgabe Header
  cat("\n=== Cronbach's Alpha für", prefix, "===\n")
  
  for (d in difficulties) {
    for (comm in c("Chat", "Jitsi")) {
      # Prefix korrekt setzen – für HiddenProfile mit Unterstrich
      full_prefix <- if (prefix == "HiddenProfile") {
        paste0(prefix, "_", comm)
      } else {
        paste0(prefix, comm)
      }
      
      items <- sprintf("%s.%s.player.fss%02d", full_prefix, d, 1:10)
      valid_items <- items[items %in% names(df)]
      
      if (length(valid_items) >= 2) {
        item_df <- df[valid_items]
        alpha_val <- tryCatch(psych::alpha(item_df)$total$raw_alpha, error = function(e) NA)
        scale_mean <- rowMeans(item_df, na.rm = TRUE)
        
        # NEU: Alpha-Wert ausgeben
        scale_name <- paste0(prefix, "_", d, "_", comm)
        if (!is.na(alpha_val)) {
          cat(sprintf("%-20s: α = %.3f\n", scale_name, alpha_val))
        } else {
          cat(sprintf("%-20s: α = NA\n", scale_name))
        }
        
      } else {
        alpha_val <- NA
        scale_mean <- rep(NA, nrow(df))
        
        # NEU: Ausgabe für zu wenige Items
        scale_name <- paste0(prefix, "_", d, "_", comm)
        cat(sprintf("%-20s: α = NA (nur %d Items)\n", scale_name, length(valid_items)))
      }
      
      col_name <- paste0("fss_", prefix, "_", d, "_", comm)
      results[[col_name]] <- scale_mean
    }
  }
  as.data.frame(results)
}


full_items <- bind_cols(
  data["participant.code"],
  map_dfr(math_data$math_items, identity),
  map_dfr(hp_data$hp_items, identity)
)

# Skalen berechnen
math_scores <- aggregate_flow(full_items, "math", c("A", "O", "F", "B"))
hp_scores   <- aggregate_flow(full_items, "HiddenProfile", c("EASY", "MED", "HARD"))

# Enddatensatz mit allen Skalen
flow_scores <- bind_cols(full_items["participant.code"], math_scores, hp_scores)

# Wide → Long
flow_scores_long <- flow_scores %>%
  pivot_longer(
    cols = -participant.code,
    names_to = "scale_name",
    values_to = "flow_score"
  )

# Zerlegen von scale_name in task, difficulty, comm
flow_scores_long <- flow_scores_long %>%
  separate(scale_name, into = c("fss", "task", "difficulty", "comm"), sep = "_", remove = TRUE) %>%
  select(-fss)

# NaN-Zeilen rausfiltern: nur tatsächliche Kommunikationsbedingung behalten
flow_scores_long <- flow_scores_long %>%
  filter(!is.na(flow_score))

data <- data %>%
  mutate(
    math_order = map(participant.condition_order, parse_order_string),
    hp_order   = map(participant.hp_condition_order, parse_order_string)
  )

# condition_order ergänzen
flow_scores_long <- flow_scores_long %>%
  left_join(data %>% select(participant.code, participant.condition_order), by = "participant.code") %>%
  mutate(condition_order = participant.condition_order) %>%
  select(-participant.condition_order)

# condition_order ergänzen
flow_scores_long <- flow_scores_long %>%
  left_join(data %>% select(participant.code, participant.hp_condition_order), by = "participant.code") %>%
  mutate(hp_condition_order = participant.hp_condition_order) %>%
  select(-participant.hp_condition_order)

# Team-ID erzeugen (falls noch nicht erfolgt)
data <- data %>%
  mutate(team_id = paste(session.code, Intro.1.group.custom_group_id, sep = "_"))

# Team-ID ergänzen
flow_scores_long <- flow_scores_long %>%
  left_join(data %>% select(participant.code, team_id), by = "participant.code")

# Einheitliche Schreibweise für spätere Filter
flow_scores_long <- flow_scores_long %>%
  mutate(
    task = recode(task,
                  "math" = "Math",
                  "HiddenProfile" = "HP"),
    difficulty = recode(difficulty,
                        "B" = "Easy",
                        "A" = "Optimal_Selected",
                        "F" = "Optimal_Calibrated",
                        "O" = "Hard",
                        "EASY" = "Easy",
                        "MED" = "Medium",
                        "HARD" = "Hard")
  )

# Jetzt den Long-Datensatz als neuen flow_scores verwenden
flow_scores <- flow_scores_long

flow_scores <- flow_scores %>%
  mutate(
    order = case_when(
      task == "Math" ~ condition_order,
      task == "HP"   ~ hp_condition_order,
      TRUE           ~ NA_character_
    )
  ) %>%
  select(-condition_order, -hp_condition_order)

# Boxplot erstellen
ggplot(flow_scores, aes(x = interaction(task, comm, sep = " - "), y = flow_score, fill = task)) +
  geom_boxplot(outlier.shape = NA, alpha = 0.7) +
  geom_jitter(width = 0.2, alpha = 0.4, color = "black", size = 1) +
  labs(
    title = "Flow-Scores nach Experimentalbedingung",
    x = "Bedingung (Task - Medium)",
    y = "Flow-Score"
  ) +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")

```

Outlier check

```{r}
# Ausreißer-Analyse für Flow-Scores
# ================================================================================

# 1. Ausreißer identifizieren (gruppenweise)
flow_scores_outlier <- flow_scores %>%
  group_by(task, comm) %>%
  mutate(
    mean_flow = mean(flow_score, na.rm = TRUE),
    sd_flow = sd(flow_score, na.rm = TRUE),
    z_score = (flow_score - mean_flow) / sd_flow,
    is_outlier = abs(z_score) > 2,
    outlier_direction = case_when(
      z_score > 2 ~ "high",
      z_score < -2 ~ "low",
      TRUE ~ "normal"
    )
  ) %>%
  ungroup()

# 2. Zusammenfassung der Ausreißer
outlier_summary <- flow_scores_outlier %>%
  group_by(task, comm) %>%
  summarise(
    n_total = n(),
    n_outliers = sum(is_outlier),
    n_high = sum(outlier_direction == "high"),
    n_low = sum(outlier_direction == "low"),
    pct_outliers = round(mean(is_outlier) * 100, 2),
    mean_flow = round(mean(flow_score, na.rm = TRUE), 3),
    sd_flow = round(sd(flow_score, na.rm = TRUE), 3),
    .groups = "drop"
  )

print("Ausreißer-Zusammenfassung nach Bedingung:")
print(outlier_summary)

# 3. Gesamtübersicht
total_outliers <- flow_scores_outlier %>%
  summarise(
    total_observations = n(),
    total_outliers = sum(is_outlier),
    pct_outliers = round(mean(is_outlier) * 100, 2)
  )

print("\nGesamtanzahl Ausreißer:")
print(total_outliers)

# 4. Details zu den Ausreißern
outlier_details <- flow_scores_outlier %>%
  filter(is_outlier) %>%
  select(participant.code, task, comm, difficulty, flow_score, z_score, outlier_direction) %>%
  arrange(desc(abs(z_score)))

print("\nTop 10 extremste Ausreißer:")
print(head(outlier_details, 10))

# 5. Visualisierung der Ausreißer
library(ggplot2)

# Boxplot mit Ausreißern markiert
p1 <- ggplot(flow_scores_outlier, aes(x = interaction(task, comm), y = flow_score)) +
  geom_boxplot(aes(fill = task), alpha = 0.7) +
  geom_point(data = filter(flow_scores_outlier, is_outlier), 
             aes(color = outlier_direction), size = 3) +
  scale_color_manual(values = c("high" = "red", "low" = "blue")) +
  labs(title = "Flow-Scores mit markierten Ausreißern (>2 SD)",
       x = "Bedingung", y = "Flow Score",
       color = "Ausreißer-Typ") +
  theme_minimal()

print(p1)

# Z-Score Verteilung
p2 <- ggplot(flow_scores_outlier, aes(x = z_score)) +
  geom_histogram(bins = 30, fill = "lightblue", color = "black", alpha = 0.7) +
  geom_vline(xintercept = c(-2, 2), color = "red", linetype = "dashed") +
  facet_wrap(~ interaction(task, comm)) +
  labs(title = "Z-Score Verteilung der Flow-Werte",
       subtitle = "Rote Linien = ±2 SD Grenze",
       x = "Z-Score", y = "Häufigkeit") +
  theme_minimal()

print(p2)

# 6. Ausreißer nach Schwierigkeit untersuchen
outlier_by_difficulty <- flow_scores_outlier %>%
  group_by(difficulty, task) %>%
  summarise(
    n_total = n(),
    n_outliers = sum(is_outlier),
    pct_outliers = round(mean(is_outlier) * 100, 2),
    outlier_types = paste(
      "High:", sum(outlier_direction == "high"),
      "Low:", sum(outlier_direction == "low")
    ),
    .groups = "drop"
  )

print("\nAusreißer nach Schwierigkeitsstufe:")
print(outlier_by_difficulty)

# 7. Prüfen ob bestimmte Teilnehmer häufig Ausreißer sind
participant_outlier_freq <- flow_scores_outlier %>%
  group_by(participant.code) %>%
  summarise(
    n_measurements = n(),
    n_outlier = sum(is_outlier),
    pct_outlier = round(mean(is_outlier) * 100, 2)
  ) %>%
  filter(n_outlier > 0) %>%
  arrange(desc(n_outlier))

print("\nTeilnehmer mit Ausreißer-Messungen:")
print(head(participant_outlier_freq, 10))

# 8. Team-Level Ausreißer prüfen (für Shared Flow)
team_flow_outliers <- flow_scores_outlier %>%
  group_by(team_id, task, comm) %>%
  summarise(
    team_mean_flow = mean(flow_score, na.rm = TRUE),
    team_sd_flow = sd(flow_score, na.rm = TRUE),
    n_members = n(),
    n_outlier_members = sum(is_outlier),
    .groups = "drop"
  ) %>%
  group_by(task, comm) %>%
  mutate(
    grand_mean = mean(team_mean_flow, na.rm = TRUE),
    grand_sd = sd(team_mean_flow, na.rm = TRUE),
    team_z_score = (team_mean_flow - grand_mean) / grand_sd,
    is_team_outlier = abs(team_z_score) > 2
  ) %>%
  filter(is_team_outlier)

print("\nTeams mit extremen Flow-Werten:")
print(team_flow_outliers)
```


Flow proneness consistency check and score calculation

```{r}
# Flow Proneness Items
flowp_items <- data %>%
  select(participant.code,
         starts_with("Outro.1.player.fpw"),
         starts_with("Outro.1.player.fph"),
         starts_with("Outro.1.player.fpl"))

flowp_items <- flowp_items %>%
  mutate(
    `Outro.1.player.fpl1` = 6 - `Outro.1.player.fpl1`,
    `Outro.1.player.fph1` = 6 - `Outro.1.player.fph1`,
    `Outro.1.player.fpw1` = 6 - `Outro.1.player.fpw1`
  )

# Prüfe interne Konsistenz pro Dimension
alpha_work <- psych::alpha(flowp_items %>% select(starts_with("Outro.1.player.fpw")))
alpha_household <- psych::alpha(flowp_items %>% select(starts_with("Outro.1.player.fph")))
alpha_leisure <- psych::alpha(flowp_items %>% select(starts_with("Outro.1.player.fpl")))

# Aggregiere zu drei Scores + Gesamtwert
flow_proneness_scores <- flowp_items %>%
  mutate(
    fp_work = rowMeans(select(., starts_with("Outro.1.player.fpw")), na.rm = TRUE),
    fp_household = rowMeans(select(., starts_with("Outro.1.player.fph")), na.rm = TRUE),
    fp_leisure = rowMeans(select(., starts_with("Outro.1.player.fpl")), na.rm = TRUE)
  ) %>%
  mutate(fp_total = rowMeans(select(., fp_work, fp_household, fp_leisure), na.rm = TRUE)) %>%
  select(participant.code, fp_total)

flow_scores <- flow_scores %>%
  left_join(flow_proneness_scores, by = "participant.code")

flow_clean <- flow_clean %>%
  left_join(flow_proneness_scores, by = "participant.code")
```

Linear mixed model for flow with individual-level variables (level-1) nested within teams (level-2), Calculation of Goodness-of-Fit criteria AIC, BIC, Marginal R2 and Conditional R2

```{r}
# Erweiterte Regressionsanalyse mit emmeans Post-Hoc Tests
# Basierend auf Betreuer-Vorgaben

library(lme4)
library(performance)
library(emmeans)
library(dplyr)
library(tibble)

# ================================================================================
# TEIL 1: MATH TASK ANALYSE
# ================================================================================

print("=== MATH TASK REGRESSIONSANALYSE ===")

# Filter: Nur Mathe-Task
flow_scores_math <- flow_scores %>% 
  filter(task == "Math")

# Modell-Datensatz vorbereiten
model_data_math <- flow_scores_math %>%
  mutate(
    synchronicity = ifelse(comm == "Jitsi", "High", "Low"),
    synchronicity = factor(synchronicity, levels = c("Low", "High")),
    difficulty = factor(difficulty, levels = c("Easy", "Optimal_Selected", "Optimal_Calibrated", "Hard"))
  )

print(sprintf("Math Task: %d Beobachtungen, %d Teams, %d Teilnehmer", 
              nrow(model_data_math),
              length(unique(model_data_math$team_id)),
              length(unique(model_data_math$participant.code))))

# Modell 1: Synchronicity + Difficulty
print("\n--- MATH MODELL 1: Haupteffekte ---")
model_math_1 <- lmer(
  flow_score ~ synchronicity + difficulty + 
    (1 | team_id) + (1 | participant.code),
  data = model_data_math
)
summary(model_math_1)

# Modell 2: Interaktion hinzufügen
print("\n--- MATH MODELL 2: Mit Interaktion ---")
model_math_2 <- lmer(
  flow_score ~ synchronicity * difficulty + 
    (1 | team_id) + (1 | participant.code),
  data = model_data_math
)
summary(model_math_2)

# Modell 3: Zusätzlich Flow Proneness & Condition Order
print("\n--- MATH MODELL 3: Vollständiges Modell ---")
model_math_3 <- lmer(
  flow_score ~ synchronicity * difficulty + 
    fp_total + order +
    (1 | team_id) + (1 | participant.code),
  data = model_data_math
)
summary(model_math_3)

# Modellvergleich
print("\n--- MATH MODELLVERGLEICH ---")
aic_comparison_math <- AIC(model_math_1, model_math_2, model_math_3)
bic_comparison_math <- BIC(model_math_1, model_math_2, model_math_3)
print("AIC Vergleich:")
print(aic_comparison_math)
print("BIC Vergleich:")
print(bic_comparison_math)

# R² Vergleich
r2_math_1 <- r2(model_math_1)
r2_math_2 <- r2(model_math_2)
r2_math_3 <- r2(model_math_3)

print("\nR² Vergleich:")
print(paste("Modell 1 - Marginal R²:", round(r2_math_1$R2_marginal, 4), 
            "Conditional R²:", round(r2_math_1$R2_conditional, 4)))
print(paste("Modell 2 - Marginal R²:", round(r2_math_2$R2_marginal, 4), 
            "Conditional R²:", round(r2_math_2$R2_conditional, 4)))
print(paste("Modell 3 - Marginal R²:", round(r2_math_3$R2_marginal, 4), 
            "Conditional R²:", round(r2_math_3$R2_conditional, 4)))

# ================================================================================
# TEIL 1b: MATH TASK - EMMEANS POST-HOC TESTS
# ================================================================================

print("\n=== MATH TASK - EMMEANS POST-HOC ANALYSEN ===")

# Wähle das beste Modell für Post-Hoc Tests (basierend auf AIC/BIC)
best_math_model <- model_math_3  # Anpassbar je nach Ergebnissen

# Between-Subjects Tests: Synchronicity Unterschiede für jede Difficulty
print("\n--- Between-Subjects Tests: Synchronicity|Difficulty ---")
bs_tests_math <- emmeans(best_math_model, specs = pairwise ~ synchronicity|difficulty, adjust = "none")$contrasts %>%
  as_tibble() %>%
  # Manuelle BH Korrektur
  mutate(p.adj = p.adjust(p.value, "BH")) %>%
  # Difficulty Labels anpassen (falls gewünscht)
  mutate(difficulty = dplyr::recode(difficulty, 
                                   "Easy" = "Easy",
                                   "Optimal_Selected" = "Optimal\n(Selected)",
                                   "Optimal_Calibrated" = "Optimal\n(Calibrated)",
                                   "Hard" = "Hard")) %>%
  # Numerische Werte runden (neue Syntax)
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Between-Subjects Tests (Synchronicity Vergleiche pro Difficulty):")
print(bs_tests_math)

# Within-Subjects Tests: Difficulty Unterschiede für jede Synchronicity
print("\n--- Within-Subjects Tests: Difficulty|Synchronicity ---")
ws_tests_math <- emmeans(best_math_model, specs = pairwise ~ difficulty|synchronicity, adjust = "none")$contrasts %>%
  as_tibble() %>%
  # Manuelle BH Korrektur
  mutate(p.adj = p.adjust(p.value, "BH")) %>%
  # Numerische Werte runden (neue Syntax)
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Within-Subjects Tests (Difficulty Vergleiche pro Synchronicity):")
print(ws_tests_math)

# Marginal Means zur Interpretation
print("\n--- Marginal Means für Math Task ---")
marginal_means_math <- emmeans(best_math_model, ~ synchronicity * difficulty) %>%
  as_tibble() %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))
print(marginal_means_math)

# ================================================================================
# TEIL 2: HIDDEN PROFILE TASK ANALYSE
# ================================================================================

print("\n\n=== HIDDEN PROFILE TASK REGRESSIONSANALYSE ===")

# Filter: Nur HP-Task
flow_scores_hp <- flow_scores %>% 
  filter(task == "HP")

# Modell-Datensatz vorbereiten
model_data_hp <- flow_scores_hp %>%
  mutate(
    synchronicity = ifelse(comm == "Jitsi", "High", "Low"),
    synchronicity = factor(synchronicity, levels = c("Low", "High")),
    difficulty = factor(difficulty, levels = c("Easy", "Medium", "Hard"))  # HP hat nur 3 Stufen
  )

print(sprintf("HP Task: %d Beobachtungen, %d Teams, %d Teilnehmer", 
              nrow(model_data_hp),
              length(unique(model_data_hp$team_id)),
              length(unique(model_data_hp$participant.code))))

# Modell 1: Synchronicity + Difficulty
print("\n--- HP MODELL 1: Haupteffekte ---")
model_hp_1 <- lmer(
  flow_score ~ synchronicity + difficulty + 
    (1 | team_id) + (1 | participant.code),
  data = model_data_hp
)
summary(model_hp_1)

# Modell 2: Interaktion hinzufügen
print("\n--- HP MODELL 2: Mit Interaktion ---")
model_hp_2 <- lmer(
  flow_score ~ synchronicity * difficulty + 
    (1 | team_id) + (1 | participant.code),
  data = model_data_hp
)
summary(model_hp_2)

# Modell 3: Zusätzlich Flow Proneness & Condition Order
print("\n--- HP MODELL 3: Vollständiges Modell ---")
model_hp_3 <- lmer(
  flow_score ~ synchronicity * difficulty + 
    fp_total + order +
    (1 | team_id) + (1 | participant.code),
  data = model_data_hp
)
summary(model_hp_3)

# Modellvergleich
print("\n--- HP MODELLVERGLEICH ---")
aic_comparison_hp <- AIC(model_hp_1, model_hp_2, model_hp_3)
bic_comparison_hp <- BIC(model_hp_1, model_hp_2, model_hp_3)
print("AIC Vergleich:")
print(aic_comparison_hp)
print("BIC Vergleich:")
print(bic_comparison_hp)

# R² Vergleich
r2_hp_1 <- r2(model_hp_1)
r2_hp_2 <- r2(model_hp_2)
r2_hp_3 <- r2(model_hp_3)

print("\nR² Vergleich:")
print(paste("Modell 1 - Marginal R²:", round(r2_hp_1$R2_marginal, 4), 
            "Conditional R²:", round(r2_hp_1$R2_conditional, 4)))
print(paste("Modell 2 - Marginal R²:", round(r2_hp_2$R2_marginal, 4), 
            "Conditional R²:", round(r2_hp_2$R2_conditional, 4)))
print(paste("Modell 3 - Marginal R²:", round(r2_hp_3$R2_marginal, 4), 
            "Conditional R²:", round(r2_hp_3$R2_conditional, 4)))

# ================================================================================
# TEIL 2b: HIDDEN PROFILE TASK - EMMEANS POST-HOC TESTS
# ================================================================================

print("\n=== HIDDEN PROFILE TASK - EMMEANS POST-HOC ANALYSEN ===")

# Wähle das beste Modell für Post-Hoc Tests
best_hp_model <- model_hp_3  # Anpassbar je nach Ergebnissen

# Between-Subjects Tests: Synchronicity Unterschiede für jede Difficulty
print("\n--- Between-Subjects Tests: Synchronicity|Difficulty ---")
bs_tests_hp <- emmeans(best_hp_model, specs = pairwise ~ synchronicity|difficulty, adjust = "none")$contrasts %>%
  as_tibble() %>%
  # Manuelle BH Korrektur
  mutate(p.adj = p.adjust(p.value, "BH")) %>%
  # Numerische Werte runden (neue Syntax)
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Between-Subjects Tests (Synchronicity Vergleiche pro Difficulty):")
print(bs_tests_hp)

# Within-Subjects Tests: Difficulty Unterschiede für jede Synchronicity
print("\n--- Within-Subjects Tests: Difficulty|Synchronicity ---")
ws_tests_hp <- emmeans(best_hp_model, specs = pairwise ~ difficulty|synchronicity, adjust = "none")$contrasts %>%
  as_tibble() %>%
  # Manuelle BH Korrektur
  mutate(p.adj = p.adjust(p.value, "BH")) %>%
  # Numerische Werte runden (neue Syntax)
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Within-Subjects Tests (Difficulty Vergleiche pro Synchronicity):")
print(ws_tests_hp)

# Marginal Means zur Interpretation
print("\n--- Marginal Means für HP Task ---")
marginal_means_hp <- emmeans(best_hp_model, ~ synchronicity * difficulty) %>%
  as_tibble() %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))
print(marginal_means_hp)

# ================================================================================
# TEIL 3: ÜBERGREIFENDE ZUSAMMENFASSUNG
# ================================================================================

print("\n\n=== ÜBERGREIFENDE ZUSAMMENFASSUNG ===")

# Sammle signifikante Ergebnisse
print("\n--- Signifikante Between-Subjects Effekte (Synchronicity) ---")

# Math Task signifikante BS Effekte
math_significant_bs <- bs_tests_math %>%
  filter(p.adj < 0.05) %>%
  mutate(Task = "Math")

# HP Task signifikante BS Effekte  
hp_significant_bs <- bs_tests_hp %>%
  filter(p.adj < 0.05) %>%
  mutate(Task = "HP")

# Kombiniere signifikante Ergebnisse
all_significant_bs <- bind_rows(math_significant_bs, hp_significant_bs)

if (nrow(all_significant_bs) > 0) {
  print("Signifikante Synchronicity Unterschiede:")
  print(all_significant_bs %>% select(Task, difficulty, estimate, p.value, p.adj))
} else {
  print("Keine signifikanten Synchronicity Unterschiede gefunden.")
}

print("\n--- Signifikante Within-Subjects Effekte (Difficulty) ---")

# Math Task signifikante WS Effekte
math_significant_ws <- ws_tests_math %>%
  filter(p.adj < 0.05) %>%
  mutate(Task = "Math")

# HP Task signifikante WS Effekte
hp_significant_ws <- ws_tests_hp %>%
  filter(p.adj < 0.05) %>%
  mutate(Task = "HP")

# Kombiniere signifikante Ergebnisse
all_significant_ws <- bind_rows(math_significant_ws, hp_significant_ws)

if (nrow(all_significant_ws) > 0) {
  print("Signifikante Difficulty Unterschiede:")
  print(all_significant_ws %>% 
         select(Task, synchronicity, contrast, estimate, p.value, p.adj) %>%
         head(10))  # Zeige nur die ersten 10 wegen der Vielzahl an Paarvergleichen
} else {
  print("Keine signifikanten Difficulty Unterschiede gefunden.")
}

# ================================================================================
# TEIL 4: EXPORTIERBARE ERGEBNISTABELLEN
# ================================================================================

print("\n=== VERFÜGBARE OBJEKTE FÜR WEITERE ANALYSEN ===")
cat("MODELLE:\n")
cat("- model_math_1, model_math_2, model_math_3: Math Task Modelle\n")
cat("- model_hp_1, model_hp_2, model_hp_3: Hidden Profile Task Modelle\n")
cat("\nPOST-HOC TESTS:\n")
cat("- bs_tests_math, ws_tests_math: Math Task emmeans Tests\n")
cat("- bs_tests_hp, ws_tests_hp: Hidden Profile Task emmeans Tests\n")
cat("- marginal_means_math, marginal_means_hp: Geschätzte Randmittel\n")
cat("\nZUSAMMENFASSUNGEN:\n")
cat("- all_significant_bs: Alle signifikanten Between-Subjects Effekte\n")
cat("- all_significant_ws: Alle signifikanten Within-Subjects Effekte\n")

# Optional: Exportiere Ergebnisse
# write.csv(bs_tests_math, "math_between_subjects_tests.csv", row.names = FALSE)
# write.csv(bs_tests_hp, "hp_between_subjects_tests.csv", row.names = FALSE)
# write.csv(marginal_means_math, "math_marginal_means.csv", row.names = FALSE)
# write.csv(marginal_means_hp, "hp_marginal_means.csv", row.names = FALSE)

print("\n=== INTERPRETATION GUIDELINES ===")
cat("Between-Subjects Tests zeigen:\n")
cat("- Unterschiede zwischen Synchronicity (Jitsi vs Chat) für jede Difficulty\n")
cat("- Negative estimates: Chat > Jitsi Flow\n")
cat("- Positive estimates: Jitsi > Chat Flow\n\n")
cat("Within-Subjects Tests zeigen:\n")
cat("- Unterschiede zwischen Difficulty Levels für jede Synchronicity\n")
cat("- Wichtig für die Interpretation von Difficulty-Effekten\n\n")
cat("Marginal Means geben dir die geschätzten Mittelwerte für jede Bedingung.\n")
```

Central LMMs without Outliers

```{r}
library(emmeans)
library(dplyr)
library(tibble)

# ================================================================================
# MATH TASK - OUTLIER-BEREINIGTER DATENSATZ
# ================================================================================

print("=== MATH TASK - OUTLIER-BEREINIGTE ANALYSE ===")

# Filter: Nur Mathe-Task
flow_clean_math <- flow_clean %>% 
  filter(task == "Math")

# Modell-Datensatz vorbereiten
model_data_math_clean <- flow_clean_math %>%
  mutate(
    synchronicity = ifelse(comm == "Jitsi", "High", "Low"),
    synchronicity = factor(synchronicity, levels = c("Low", "High")),
    difficulty = factor(difficulty, levels = c("Easy", "Optimal_Selected", "Optimal_Calibrated", "Hard"))
  )

print(sprintf("Math Task (clean): %d Beobachtungen, %d Teams, %d Teilnehmer", 
              nrow(model_data_math_clean),
              length(unique(model_data_math_clean$team_id)),
              length(unique(model_data_math_clean$participant.code))))

# Modell 3 (bestes Modell)
print("\n--- MATH MODELL 3 (OUTLIER-BEREINIGT) ---")
model_math_3_clean <- lmer(
  flow_score ~ synchronicity * difficulty + 
    fp_total + order +
    (1 | team_id) + (1 | participant.code),
  data = model_data_math_clean
)
summary(model_math_3_clean)

# ================================================================================
# MATH TASK - EMMEANS POST-HOC TESTS (OUTLIER-BEREINIGT)
# ================================================================================

print("\n=== MATH TASK - EMMEANS POST-HOC ANALYSEN (OUTLIER-BEREINIGT) ===")

# Between-Subjects Tests: Synchronicity Unterschiede für jede Difficulty
print("\n--- Between-Subjects Tests: Synchronicity|Difficulty (clean) ---")
bs_tests_math_clean <- emmeans(model_math_3_clean, specs = pairwise ~ synchronicity|difficulty, adjust = "none")$contrasts %>%
  as_tibble() %>%
  # Manuelle BH Korrektur
  mutate(p.adj = p.adjust(p.value, "BH")) %>%
  # Difficulty Labels anpassen (falls gewünscht)
  mutate(difficulty = dplyr::recode(difficulty, 
                                   "Easy" = "Easy",
                                   "Optimal_Selected" = "Optimal\n(Selected)",
                                   "Optimal_Calibrated" = "Optimal\n(Calibrated)",
                                   "Hard" = "Hard")) %>%
  # Numerische Werte runden (neue Syntax)
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Between-Subjects Tests (Synchronicity Vergleiche pro Difficulty - clean):")
print(bs_tests_math_clean)

# Within-Subjects Tests: Difficulty Unterschiede für jede Synchronicity
print("\n--- Within-Subjects Tests: Difficulty|Synchronicity (clean) ---")
ws_tests_math_clean <- emmeans(model_math_3_clean, specs = pairwise ~ difficulty|synchronicity, adjust = "none")$contrasts %>%
  as_tibble() %>%
  # Manuelle BH Korrektur
  mutate(p.adj = p.adjust(p.value, "BH")) %>%
  # Numerische Werte runden (neue Syntax)
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Within-Subjects Tests (Difficulty Vergleiche pro Synchronicity - clean):")
print(ws_tests_math_clean)

# Marginal Means zur Interpretation
print("\n--- Marginal Means für Math Task (clean) ---")
marginal_means_math_clean <- emmeans(model_math_3_clean, ~ synchronicity * difficulty) %>%
  as_tibble() %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))
print(marginal_means_math_clean)

# ================================================================================
# HIDDEN PROFILE TASK - OUTLIER-BEREINIGTER DATENSATZ
# ================================================================================

print("\n\n=== HIDDEN PROFILE TASK - OUTLIER-BEREINIGTE ANALYSE ===")

# Filter: Nur HP-Task
flow_clean_hp <- flow_clean %>% 
  filter(task == "HP")

# Modell-Datensatz vorbereiten
model_data_hp_clean <- flow_clean_hp %>%
  mutate(
    synchronicity = ifelse(comm == "Jitsi", "High", "Low"),
    synchronicity = factor(synchronicity, levels = c("Low", "High")),
    difficulty = factor(difficulty, levels = c("Easy", "Medium", "Hard"))  # HP hat nur 3 Stufen
  )

print(sprintf("HP Task (clean): %d Beobachtungen, %d Teams, %d Teilnehmer", 
              nrow(model_data_hp_clean),
              length(unique(model_data_hp_clean$team_id)),
              length(unique(model_data_hp_clean$participant.code))))

# Modell 3 (bestes Modell)
print("\n--- HP MODELL 3 (OUTLIER-BEREINIGT) ---")
model_hp_3_clean <- lmer(
  flow_score ~ synchronicity * difficulty + 
    fp_total + order +
    (1 | team_id) + (1 | participant.code),
  data = model_data_hp_clean
)
summary(model_hp_3_clean)

# ================================================================================
# HIDDEN PROFILE TASK - EMMEANS POST-HOC TESTS (OUTLIER-BEREINIGT)
# ================================================================================

print("\n=== HIDDEN PROFILE TASK - EMMEANS POST-HOC ANALYSEN (OUTLIER-BEREINIGT) ===")

# Between-Subjects Tests: Synchronicity Unterschiede für jede Difficulty
print("\n--- Between-Subjects Tests: Synchronicity|Difficulty (clean) ---")
bs_tests_hp_clean <- emmeans(model_hp_3_clean, specs = pairwise ~ synchronicity|difficulty, adjust = "none")$contrasts %>%
  as_tibble() %>%
  # Manuelle BH Korrektur
  mutate(p.adj = p.adjust(p.value, "BH")) %>%
  # Numerische Werte runden (neue Syntax)
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Between-Subjects Tests (Synchronicity Vergleiche pro Difficulty - clean):")
print(bs_tests_hp_clean)

# Within-Subjects Tests: Difficulty Unterschiede für jede Synchronicity
print("\n--- Within-Subjects Tests: Difficulty|Synchronicity (clean) ---")
ws_tests_hp_clean <- emmeans(model_hp_3_clean, specs = pairwise ~ difficulty|synchronicity, adjust = "none")$contrasts %>%
  as_tibble() %>%
  # Manuelle BH Korrektur
  mutate(p.adj = p.adjust(p.value, "BH")) %>%
  # Numerische Werte runden (neue Syntax)
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Within-Subjects Tests (Difficulty Vergleiche pro Synchronicity - clean):")
print(ws_tests_hp_clean)

# Marginal Means zur Interpretation
print("\n--- Marginal Means für HP Task (clean) ---")
marginal_means_hp_clean <- emmeans(model_hp_3_clean, ~ synchronicity * difficulty) %>%
  as_tibble() %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))
print(marginal_means_hp_clean)

# ================================================================================
# VERGLEICH: ORIGINAL vs OUTLIER-BEREINIGT
# ================================================================================

print("\n\n=== VERGLEICH: ORIGINAL vs OUTLIER-BEREINIGT ===")

# Funktion zum Vergleich von Between-Subjects Effekten
compare_bs_effects <- function(original, clean, task_name) {
  cat(sprintf("\n--- %s Task: BS-Effekte Vergleich ---\n", task_name))
  
  # Kombiniere Datensätze mit korrigierter rename Syntax
  comparison <- original %>%
    select(difficulty, estimate, p.value, p.adj) %>%
    dplyr::rename(original_estimate = estimate, original_p = p.value, original_p.adj = p.adj) %>%
    left_join(
      clean %>%
        select(difficulty, estimate, p.value, p.adj) %>%
        dplyr::rename(clean_estimate = estimate, clean_p = p.value, clean_p.adj = p.adj),
      by = "difficulty"
    ) %>%
    mutate(
      estimate_diff = clean_estimate - original_estimate,
      p_change = clean_p.adj < 0.05 & original_p.adj >= 0.05 | clean_p.adj >= 0.05 & original_p.adj < 0.05
    )
  
  print("Vergleich Between-Subjects Effekte (Original vs Clean):")
  print(comparison)
  
  # Zeige bedeutsame Änderungen
  if (any(comparison$p_change, na.rm = TRUE)) {
    cat("ACHTUNG: Signifikanzänderungen nach Outlier-Bereinigung!\n")
  }
  
  return(comparison)
}

# Vergleiche die Ergebnisse (falls Original-Ergebnisse verfügbar)
if (exists("bs_tests_math") && exists("bs_tests_hp")) {
  math_comparison <- compare_bs_effects(bs_tests_math, bs_tests_math_clean, "Math")
  hp_comparison <- compare_bs_effects(bs_tests_hp, bs_tests_hp_clean, "HP")
}

# ================================================================================
# ZUSAMMENFASSUNG FÜR OUTLIER-BEREINIGTE ANALYSEN
# ================================================================================

print("\n=== ZUSAMMENFASSUNG - OUTLIER-BEREINIGTE ANALYSEN ===")

# Sammle signifikante Ergebnisse für outlier-bereinigte Daten
print("\n--- Signifikante Between-Subjects Effekte (CLEAN) ---")

# Math Task signifikante BS Effekte
math_significant_bs_clean <- bs_tests_math_clean %>%
  filter(p.adj < 0.05) %>%
  mutate(Task = "Math")

# HP Task signifikante BS Effekte  
hp_significant_bs_clean <- bs_tests_hp_clean %>%
  filter(p.adj < 0.05) %>%
  mutate(Task = "HP")

# Kombiniere signifikante Ergebnisse
all_significant_bs_clean <- bind_rows(math_significant_bs_clean, hp_significant_bs_clean)

if (nrow(all_significant_bs_clean) > 0) {
  print("Signifikante Synchronicity Unterschiede (clean data):")
  print(all_significant_bs_clean %>% select(Task, difficulty, estimate, p.value, p.adj))
} else {
  print("Keine signifikanten Synchronicity Unterschiede in bereinigten Daten gefunden.")
}

```

Comparison of communication media (NONE/CHAT/JITSI)

```{r}
# ================================================================================
# TEIL 1: DATENAUFBEREITUNG UND -ZUSAMMENFÜHRUNG
# ================================================================================

# Aktuelle Daten: Nur Math Task
current_data <- flow_scores %>%
  filter(task == "Math") %>%
  select(participant.code, team_id, difficulty, comm, flow_score, order, fp_total) %>%
  # Spaltennamen für Konsistenz anpassen
  dplyr::rename(
    participant_id = participant.code,
    session_id = team_id
  ) %>%
  # Communication Medium als kategoriale Variable
  mutate(
    comm_type = case_when(
      comm == "Jitsi" ~ "Video",
      comm == "Chat" ~ "Chat"
    ),
    data_source = "Current"
  ) %>%
  select(-comm)  # Original comm Spalte entfernen

print(sprintf("Aktuelle Daten: %d Beobachtungen, %d Teams, %d Teilnehmer",
              nrow(current_data),
              length(unique(current_data$session_id)),
              length(unique(current_data$participant_id))))

# Historische Daten: Nur MP Treatment
historical_data_prep <- data_old %>%
  filter(Treatment == "MP") %>%  # Nur Multi-Player
  select(SessionID, SubjectID, Condition, Order, flowFKS_9, flowProne.General)

# Erstelle Order-Strings für historische Daten (analog zu flow_scores)
print("Erstelle Order-Strings für historische Daten...")
historical_order_mapping <- historical_data_prep %>%
  select(SessionID, Condition, Order) %>%
  distinct() %>%
  # Condition zu Difficulty-Buchstaben mapping (entsprechend deinem Recode)
  mutate(
    difficulty_code = case_when(
      Condition == 1 ~ "B",  # Easy
      Condition == 2 ~ "A",  # Optimal_Selected
      Condition == 3 ~ "F",  # Optimal_Calibrated  
      Condition == 4 ~ "O"   # Hard
    )
  ) %>%
  # Sortiere nach Order um die richtige Reihenfolge zu bekommen
  arrange(SessionID, Order) %>%
  # Erstelle Order-String für jedes Team
  group_by(SessionID) %>%
  summarise(
    order_string = paste0("['", paste(difficulty_code, collapse = "', '"), "']"),
    .groups = "drop"
  )

print("Beispiel Order-Mappings:")
print(head(historical_order_mapping, 5))

# Jetzt die historischen Daten mit Order-Strings verknüpfen
historical_data <- historical_data_prep %>%
  # Spaltennamen für Konsistenz anpassen
  dplyr::rename(
    session_id = SessionID,
    participant_id = SubjectID,
    condition_num = Condition,
    order_position = Order,
    flow_score = flowFKS_9,
    fp_total = flowProne.General
  ) %>%
  # Order-String hinzufügen
  left_join(historical_order_mapping, by = c("session_id" = "SessionID")) %>%
  # Condition Numbers zu Difficulty Labels konvertieren (entsprechend deinem Mapping)
  mutate(
    difficulty = case_when(
      condition_num == 1 ~ "Easy",           # B
      condition_num == 2 ~ "Optimal_Selected",  # A
      condition_num == 3 ~ "Optimal_Calibrated", # F
      condition_num == 4 ~ "Hard"            # O
    ),
    comm_type = "None",  # Keine Kommunikation im alten Experiment
    data_source = "Historical"
  ) %>%
  # Order-String als order-Spalte verwenden (analog zu aktuellen Daten)
  dplyr::rename(order = order_string) %>%
  select(-condition_num, -order_position)

print(sprintf("Historische Daten: %d Beobachtungen, %d Teams, %d Teilnehmer",
              nrow(historical_data),
              length(unique(historical_data$session_id)),
              length(unique(historical_data$participant_id))))

# Kombiniere beide Datensätze
combined_data <- bind_rows(current_data, historical_data) %>%
  # Faktoren für Modellierung erstellen
  mutate(
    difficulty = factor(difficulty, levels = c("Easy", "Optimal_Selected", "Optimal_Calibrated", "Hard")),
    comm_type = factor(comm_type, levels = c("None", "Chat", "Video")),
    data_source = factor(data_source, levels = c("Historical", "Current"))
  )

print(sprintf("Kombinierte Daten: %d Beobachtungen gesamt",
              nrow(combined_data)))

# Überblick über die Datenverteilung
print("\n--- Datenverteilung nach Communication Type ---")
print(table(combined_data$comm_type, combined_data$data_source))

print("\n--- Datenverteilung nach Difficulty ---")
print(table(combined_data$difficulty, combined_data$comm_type))

# ================================================================================
# TEIL 2: DESKRIPTIVE STATISTIKEN
# ================================================================================

print("\n=== DESKRIPTIVE STATISTIKEN ===")

# Grundstatistiken nach Communication Type
descriptive_stats <- combined_data %>%
  group_by(comm_type, data_source) %>%
  summarise(
    n_obs = n(),
    n_teams = length(unique(session_id)),
    n_participants = length(unique(participant_id)),
    mean_flow = mean(flow_score, na.rm = TRUE),
    sd_flow = sd(flow_score, na.rm = TRUE),
    mean_fp = mean(fp_total, na.rm = TRUE),
    sd_fp = sd(fp_total, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(across(where(is.numeric), ~ round(.x, 3)))

print("Deskriptive Statistiken nach Communication Type:")
print(descriptive_stats)

# Detaillierte Statistiken nach Difficulty
detailed_stats <- combined_data %>%
  group_by(comm_type, difficulty) %>%
  summarise(
    n_obs = n(),
    mean_flow = mean(flow_score, na.rm = TRUE),
    sd_flow = sd(flow_score, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(across(where(is.numeric), ~ round(.x, 3)))

print("\n--- Flow Scores nach Communication Type und Difficulty ---")
print(detailed_stats)

# ================================================================================
# TEIL 3: REGRESSIONSMODELLE FÜR VERGLEICH
# ================================================================================

print("\n=== REGRESSIONSANALYSE - HISTORISCHER VERGLEICH ===")

# Modell 1: Nur Communication Type (Haupteffekt)
print("\n--- MODELL 1: Communication Type Haupteffekt ---")
model_historical_1 <- lmer(
  flow_score ~ comm_type + 
    (1 | session_id) + (1 | participant_id),
  data = combined_data
)
summary(model_historical_1)

# Modell 2: Communication Type + Difficulty
print("\n--- MODELL 2: Communication Type + Difficulty ---")
model_historical_2 <- lmer(
  flow_score ~ comm_type + difficulty + 
    (1 | session_id) + (1 | participant_id),
  data = combined_data
)
summary(model_historical_2)

# Modell 3: Mit Interaktion
print("\n--- MODELL 3: Communication Type × Difficulty Interaktion ---")
model_historical_3 <- lmer(
  flow_score ~ comm_type * difficulty + 
    (1 | session_id) + (1 | participant_id),
  data = combined_data
)
summary(model_historical_3)

# Modell 4: Vollständiges Modell mit Flow Proneness
print("\n--- MODELL 4: Vollständiges Modell mit Covariaten ---")
model_historical_4 <- lmer(
  flow_score ~ comm_type * difficulty + fp_total +
    (1 | session_id) + (1 | participant_id),
  data = combined_data
)
summary(model_historical_4)

# Modellvergleich
print("\n--- MODELLVERGLEICH ---")
historical_aic <- AIC(model_historical_1, model_historical_2, model_historical_3, model_historical_4)
historical_bic <- BIC(model_historical_1, model_historical_2, model_historical_3, model_historical_4)

print("AIC Vergleich:")
print(historical_aic)
print("BIC Vergleich:")
print(historical_bic)

# ================================================================================
# TEIL 4: EMMEANS POST-HOC TESTS
# ================================================================================

print("\n=== EMMEANS POST-HOC ANALYSEN - HISTORISCHER VERGLEICH ===")

# Bestes Modell für Post-Hoc Tests verwenden (anpassbar)
best_historical_model <- model_historical_4

# Communication Type Vergleiche (Paarweise)
print("\n--- Communication Type Paarvergleiche ---")
comm_comparisons <- emmeans(best_historical_model, specs = pairwise ~ comm_type, adjust = "tukey")$contrasts %>%
  as_tibble() %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Communication Type Vergleiche:")
print(comm_comparisons)

# Communication Type Vergleiche für jede Difficulty
print("\n--- Communication Type Vergleiche pro Difficulty ---")
comm_by_difficulty <- emmeans(best_historical_model, specs = pairwise ~ comm_type|difficulty, adjust = "tukey")$contrasts %>%
  as_tibble() %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Communication Type Vergleiche nach Difficulty:")
print(comm_by_difficulty)

# Marginal Means für Interpretation
print("\n--- Marginal Means: Communication Type × Difficulty ---")
historical_marginal_means <- emmeans(best_historical_model, ~ comm_type * difficulty) %>%
  as_tibble() %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))
print(historical_marginal_means)

# ================================================================================
# TEIL 5: SPEZIFISCHE VERGLEICHE
# ================================================================================

print("\n=== SPEZIFISCHE HISTORISCHE VERGLEICHE ===")

# 1. None vs. Current Communication (Chat + Video combined)
print("\n--- None vs. Current Communication (kombiniert) ---")

# Erstelle eine neue Gruppierungsvariable
combined_data_grouped <- combined_data %>%
  mutate(
    comm_era = case_when(
      comm_type == "None" ~ "Historical_NoComm",
      comm_type %in% c("Chat", "Video") ~ "Current_WithComm"
    )
  )

model_era_comparison <- lmer(
  flow_score ~ comm_era * difficulty + fp_total +
    (1 | session_id) + (1 | participant_id),
  data = combined_data_grouped
)

era_comparisons <- emmeans(model_era_comparison, specs = pairwise ~ comm_era, adjust = "none")$contrasts %>%
  as_tibble() %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Historical vs. Current Era Vergleich:")
print(era_comparisons)

# 2. Specific Focus: None vs Video, None vs Chat
print("\n--- Spezifische Paarvergleiche: None vs Chat vs Video ---")

# Filtere für spezielle Vergleiche
specific_comparisons <- comm_comparisons %>%
  filter(
    grepl("None.*Chat|Chat.*None|None.*Video|Video.*None", contrast)
  )

if (nrow(specific_comparisons) > 0) {
  print("Wichtige Vergleiche (None vs. Kommunikationsmedien):")
  print(specific_comparisons)
}

# ================================================================================
# TEIL 6: ZUSAMMENFASSUNG UND INTERPRETATION
# ================================================================================

print("\n=== ZUSAMMENFASSUNG HISTORISCHER VERGLEICH ===")

# Signifikante Communication Type Effekte
significant_comm_effects <- comm_comparisons %>%
  filter(p.value < 0.05)

if (nrow(significant_comm_effects) > 0) {
  print("Signifikante Communication Type Unterschiede:")
  print(significant_comm_effects %>% select(contrast, estimate, p.value))
} else {
  print("Keine signifikanten Communication Type Unterschiede gefunden.")
}

# Effect Sizes für Interpretation
print("\n--- Effect Sizes (Cohen's d approximation) ---")
pooled_sd <- sd(combined_data$flow_score, na.rm = TRUE)

effect_sizes <- comm_comparisons %>%
  mutate(
    cohens_d = abs(estimate) / pooled_sd,
    effect_magnitude = case_when(
      cohens_d < 0.2 ~ "Negligible",
      cohens_d < 0.5 ~ "Small", 
      cohens_d < 0.8 ~ "Medium",
      TRUE ~ "Large"
    )
  ) %>%
  select(contrast, estimate, cohens_d, effect_magnitude)

print("Effect Sizes für Communication Type Vergleiche:")
print(effect_sizes)
```

Comparision to single player treatment

```{r}
# Vollständiger historischer Vergleich: Single Player vs Multi Player vs Current (Chat/Video)
# Vier-Weg Vergleich: Allein vs Zusammen (ohne Kommunikation) vs Chat vs Video

library(dplyr)
library(lme4)
library(emmeans)
library(tibble)

print("=== VOLLSTÄNDIGER HISTORISCHER VERGLEICH ===")
print("Single Player vs Multi Player vs Current Communication")

# ================================================================================
# TEIL 1: ERWEITERTE DATENAUFBEREITUNG
# ================================================================================

# Aktuelle Daten: Nur Math Task (bereits vorbereitet, aber nochmal für Vollständigkeit)
current_data_extended <- flow_scores %>%
  filter(task == "Math") %>%
  select(participant.code, team_id, difficulty, comm, flow_score, order, fp_total) %>%
  dplyr::rename(
    participant_id = participant.code,
    session_id = team_id
  ) %>%
  mutate(
    communication_condition = case_when(
      comm == "Jitsi" ~ "Together_Video",
      comm == "Chat" ~ "Together_Chat"
    ),
    data_source = "Current"
  ) %>%
  select(-comm)

print(sprintf("Aktuelle Daten: %d Beobachtungen, %d Teams, %d Teilnehmer",
              nrow(current_data_extended),
              length(unique(current_data_extended$session_id)),
              length(unique(current_data_extended$participant_id))))

# Historische Daten: ALLE Treatments (SP + MP)
historical_data_prep_all <- data_old %>%
  select(SessionID, SubjectID, Treatment, Condition, Order, flowFKS_9, flowProne.General)

# Order-Strings für ALLE historischen Teams erstellen
historical_order_mapping_all <- historical_data_prep_all %>%
  select(SessionID, Condition, Order) %>%
  distinct() %>%
  mutate(
    difficulty_code = case_when(
      Condition == 1 ~ "B",  # Easy
      Condition == 2 ~ "A",  # Optimal_Selected
      Condition == 3 ~ "F",  # Optimal_Calibrated  
      Condition == 4 ~ "O"   # Hard
    )
  ) %>%
  arrange(SessionID, Order) %>%
  group_by(SessionID) %>%
  summarise(
    order_string = paste0("['", paste(difficulty_code, collapse = "', '"), "']"),
    .groups = "drop"
  )

# Historische Daten mit beiden Treatments
historical_data_extended <- historical_data_prep_all %>%
  dplyr::rename(
    session_id = SessionID,
    participant_id = SubjectID,
    treatment = Treatment,
    condition_num = Condition,
    order_position = Order,
    flow_score = flowFKS_9,
    fp_total = flowProne.General
  ) %>%
  left_join(historical_order_mapping_all, by = c("session_id" = "SessionID")) %>%
  mutate(
    difficulty = case_when(
      condition_num == 1 ~ "Easy",
      condition_num == 2 ~ "Optimal_Selected", 
      condition_num == 3 ~ "Optimal_Calibrated",
      condition_num == 4 ~ "Hard"
    ),
    communication_condition = case_when(
      treatment == "SP" ~ "Alone",
      treatment == "MP" ~ "Together_None"
    ),
    data_source = "Historical"
  ) %>%
  dplyr::rename(order = order_string) %>%
  select(-condition_num, -order_position, -treatment)

print(sprintf("Historische Daten: %d Beobachtungen, %d Teams, %d Teilnehmer",
              nrow(historical_data_extended),
              length(unique(historical_data_extended$session_id)),
              length(unique(historical_data_extended$participant_id))))

# Kombiniere ALLE Daten
comprehensive_data <- bind_rows(current_data_extended, historical_data_extended) %>%
  mutate(
    difficulty = factor(difficulty, levels = c("Easy", "Optimal_Selected", "Optimal_Calibrated", "Hard")),
    communication_condition = factor(communication_condition, 
                                   levels = c("Alone", "Together_None", "Together_Chat", "Together_Video")),
    data_source = factor(data_source, levels = c("Historical", "Current"))
  )

print(sprintf("Kombinierte Daten: %d Beobachtungen gesamt", nrow(comprehensive_data)))

# ================================================================================
# TEIL 2: UMFASSENDE DESKRIPTIVE STATISTIKEN
# ================================================================================

print("\n=== UMFASSENDE DESKRIPTIVE STATISTIKEN ===")

# Grundstatistiken nach Communication Condition
comprehensive_stats <- comprehensive_data %>%
  group_by(communication_condition, data_source) %>%
  summarise(
    n_obs = n(),
    n_sessions = length(unique(session_id)),
    n_participants = length(unique(participant_id)),
    mean_flow = mean(flow_score, na.rm = TRUE),
    sd_flow = sd(flow_score, na.rm = TRUE),
    median_flow = median(flow_score, na.rm = TRUE),
    mean_fp = mean(fp_total, na.rm = TRUE),
    sd_fp = sd(fp_total, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(across(where(is.numeric), ~ round(.x, 3)))

print("Umfassende Statistiken nach Communication Condition:")
print(comprehensive_stats)

# Vereinfachte Übersicht (kombiniert über data_source)
simple_stats <- comprehensive_data %>%
  group_by(communication_condition) %>%
  summarise(
    n_total = n(),
    mean_flow = mean(flow_score, na.rm = TRUE),
    sd_flow = sd(flow_score, na.rm = TRUE),
    ci_lower = mean_flow - 1.96 * sd_flow / sqrt(n()),
    ci_upper = mean_flow + 1.96 * sd_flow / sqrt(n()),
    .groups = "drop"
  ) %>%
  mutate(across(where(is.numeric), ~ round(.x, 3)))

print("\nVereinfachte Übersicht (alle Conditions):")
print(simple_stats)

# Datenverteilung prüfen
print("\n--- Datenverteilung ---")
print("Nach Communication Condition:")
print(table(comprehensive_data$communication_condition))

print("\nNach Communication Condition und Data Source:")
print(table(comprehensive_data$communication_condition, comprehensive_data$data_source))

print("\nNach Difficulty und Communication Condition:")
difficulty_table <- table(comprehensive_data$difficulty, comprehensive_data$communication_condition)
print(difficulty_table)

# ================================================================================
# TEIL 3: UMFASSENDE REGRESSIONSANALYSE
# ================================================================================

print("\n=== UMFASSENDE REGRESSIONSANALYSE ===")

# Modell 1: Nur Communication Condition
print("\n--- MODELL 1: Communication Condition Haupteffekt ---")
model_comprehensive_1 <- lmer(
  flow_score ~ communication_condition + 
    (1 | session_id) + (1 | participant_id),
  data = comprehensive_data
)
summary(model_comprehensive_1)

# Modell 2: Communication Condition + Difficulty
print("\n--- MODELL 2: Communication Condition + Difficulty ---")
model_comprehensive_2 <- lmer(
  flow_score ~ communication_condition + difficulty + 
    (1 | session_id) + (1 | participant_id),
  data = comprehensive_data
)
summary(model_comprehensive_2)

# Modell 3: Mit Interaktion
print("\n--- MODELL 3: Communication Condition × Difficulty ---")
model_comprehensive_3 <- lmer(
  flow_score ~ communication_condition * difficulty + 
    (1 | session_id) + (1 | participant_id),
  data = comprehensive_data
)
summary(model_comprehensive_3)

# Modell 4: Vollständiges Modell mit Flow Proneness
print("\n--- MODELL 4: Vollständiges Modell ---")
model_comprehensive_4 <- lmer(
  flow_score ~ communication_condition * difficulty + fp_total +
    (1 | session_id) + (1 | participant_id),
  data = comprehensive_data
)
summary(model_comprehensive_4)

# Modellvergleich
print("\n--- MODELLVERGLEICH ---")
comprehensive_aic <- AIC(model_comprehensive_1, model_comprehensive_2, model_comprehensive_3, model_comprehensive_4)
comprehensive_bic <- BIC(model_comprehensive_1, model_comprehensive_2, model_comprehensive_3, model_comprehensive_4)

print("AIC Vergleich:")
print(comprehensive_aic)
print("BIC Vergleich:")
print(comprehensive_bic)

# ================================================================================
# TEIL 4: EMMEANS PAARVERGLEICHE
# ================================================================================

print("\n=== EMMEANS PAARVERGLEICHE - ALLE CONDITIONS ===")

# Bestes Modell verwenden
best_comprehensive_model <- model_comprehensive_4

# Alle paarweisen Vergleiche zwischen Communication Conditions
print("\n--- Alle Communication Condition Paarvergleiche ---")
all_comm_comparisons <- emmeans(best_comprehensive_model, specs = pairwise ~ communication_condition, adjust = "tukey")$contrasts %>%
  as_tibble() %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))

print("Alle Communication Condition Vergleiche:")
print(all_comm_comparisons)

# Spezifische Vergleiche von Interesse
print("\n--- Spezifische Vergleiche von Interesse ---")

key_comparisons <- all_comm_comparisons %>%
  filter(
    grepl("Alone.*Together", contrast) |
    grepl("Together_None.*Together_Chat", contrast) |
    grepl("Together_None.*Together_Video", contrast) |
    grepl("Together_Chat.*Together_Video", contrast)
  )

if (nrow(key_comparisons) > 0) {
  print("Wichtige Vergleiche:")
  print(key_comparisons %>% select(contrast, estimate, p.value))
}

# Marginal Means für alle Conditions
print("\n--- Marginal Means: Communication Conditions ---")
comprehensive_marginal_means <- emmeans(best_comprehensive_model, ~ communication_condition) %>%
  as_tibble() %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))
print(comprehensive_marginal_means)

# Communication Condition × Difficulty Marginal Means
print("\n--- Marginal Means: Communication × Difficulty ---")
interaction_marginal_means <- emmeans(best_comprehensive_model, ~ communication_condition * difficulty) %>%
  as_tibble() %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))
print(interaction_marginal_means)

# ================================================================================
# TEIL 5: EFFEKTGRÖSSEN UND PRAKTISCHE BEDEUTSAMKEIT
# ================================================================================

print("\n=== EFFEKTGRÖSSEN UND PRAKTISCHE BEDEUTSAMKEIT ===")

# Berechne Effect Sizes (Cohen's d approximation)
pooled_sd_comprehensive <- sd(comprehensive_data$flow_score, na.rm = TRUE)

effect_sizes_comprehensive <- all_comm_comparisons %>%
  mutate(
    cohens_d = abs(estimate) / pooled_sd_comprehensive,
    effect_magnitude = case_when(
      cohens_d < 0.2 ~ "Vernachlässigbar",
      cohens_d < 0.5 ~ "Klein", 
      cohens_d < 0.8 ~ "Mittel",
      TRUE ~ "Groß"
    ),
    practical_significance = case_when(
      p.value < 0.001 & cohens_d >= 0.5 ~ "Hoch signifikant + praktisch relevant",
      p.value < 0.05 & cohens_d >= 0.5 ~ "Signifikant + praktisch relevant",
      p.value < 0.05 & cohens_d >= 0.2 ~ "Signifikant + kleiner Effekt",
      p.value >= 0.05 ~ "Nicht signifikant",
      TRUE ~ "Signifikant aber vernachlässigbare Effektgröße"
    )
  ) %>%
  select(contrast, estimate, p.value, cohens_d, effect_magnitude, practical_significance)

print("Effect Sizes für alle Communication Condition Vergleiche:")
print(effect_sizes_comprehensive)

# ================================================================================
# TEIL 6: ZUSAMMENFASSUNG UND INTERPRETATION
# ================================================================================

print("\n=== ZUSAMMENFASSUNG: VIER-WEG VERGLEICH ===")

# Flow Score Ranking
flow_ranking <- comprehensive_marginal_means %>%
  arrange(desc(emmean)) %>%
  select(communication_condition, emmean, SE) %>%
  mutate(
    rank = row_number(),
    condition_german = case_when(
      communication_condition == "Alone" ~ "Allein",
      communication_condition == "Together_None" ~ "Zusammen (keine Kommunikation)",
      communication_condition == "Together_Chat" ~ "Zusammen (Chat)",
      communication_condition == "Together_Video" ~ "Zusammen (Video)"
    )
  )

print("Flow Score Ranking (höchste zu niedrigste):")
print(flow_ranking %>% select(rank, condition_german, emmean, SE))

# Signifikante Unterschiede zusammenfassen
significant_comparisons <- effect_sizes_comprehensive %>%
  filter(p.value < 0.05) %>%
  arrange(p.value)

print("\nSignifikante Unterschiede (nach p-Wert sortiert):")
if (nrow(significant_comparisons) > 0) {
  print(significant_comparisons %>% select(contrast, estimate, p.value, effect_magnitude))
} else {
  print("Keine signifikanten Unterschiede gefunden.")
}

```

ANOVA for treatment effects

```{r}
# ================================================================================
   # UMFASSENDE ANOVA-ANALYSE
# ================================================================================

library(car)        # Für Levene-Test und Typ II/III ANOVA
library(effectsize) # Für Effektgrößen (eta-squared, omega-squared)
library(multcomp)   # Für erweiterte Post-hoc Tests
library(nortest)    # Für erweiterte Normalitätstests

print("\n=== UMFASSENDE ANOVA-ANALYSE ===")

# ================================================================================
# SCHRITT 1: ANNAHMEN-ÜBERPRÜFUNG
# ================================================================================

print("\n--- ANNAHMEN-ÜBERPRÜFUNG ---")

# 1. Normalitätstest
print("\n1. NORMALITÄTSTEST")

# Shapiro-Wilk Test (für kleinere Stichproben pro Gruppe)
normality_by_group <- comprehensive_data %>%
  group_by(communication_condition) %>%
  summarise(
    n = n(),
    shapiro_w = ifelse(n >= 3 & n <= 5000, shapiro.test(flow_score)$statistic, NA),
    shapiro_p = ifelse(n >= 3 & n <= 5000, shapiro.test(flow_score)$p.value, NA),
    .groups = "drop"
  )

print("Normalitätstest pro Communication Condition:")
print(normality_by_group)

# Anderson-Darling Test für größere Gruppen
print("\nAnderson-Darling Normalitätstest (gesamt):")
ad_test <- nortest::ad.test(comprehensive_data$flow_score)
print(paste("A =", round(ad_test$statistic, 4), ", p =", round(ad_test$p.value, 4)))

# QQ-Plot Daten für visuelle Inspektion vorbereiten
qq_data <- comprehensive_data %>%
  group_by(communication_condition) %>%
  arrange(flow_score) %>%
  mutate(
    theoretical_quantile = qnorm(ppoints(n())),
    sample_quantile = flow_score
  ) %>%
  ungroup()

print("QQ-Plot Daten bereit für visuelle Inspektion")

# 2. Homogenität der Varianzen
print("\n2. HOMOGENITÄT DER VARIANZEN")

# Levene-Test
levene_test <- leveneTest(flow_score ~ communication_condition, data = comprehensive_data)
print("Levene-Test für Varianzhomogenität:")
print(levene_test)

# Bartlett-Test (sensitiver für Normalitätsabweichungen)
bartlett_test <- bartlett.test(flow_score ~ communication_condition, data = comprehensive_data)
print("Bartlett-Test für Varianzhomogenität:")
print(paste("Chi-squared =", round(bartlett_test$statistic, 4), 
           ", df =", bartlett_test$parameter, 
           ", p =", round(bartlett_test$p.value, 4)))

# Deskriptive Statistiken pro Gruppe
variance_stats <- comprehensive_data %>%
  group_by(communication_condition) %>%
  summarise(
    n = n(),
    mean = mean(flow_score, na.rm = TRUE),
    sd = sd(flow_score, na.rm = TRUE),
    variance = var(flow_score, na.rm = TRUE),
    min_val = min(flow_score, na.rm = TRUE),
    max_val = max(flow_score, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(across(where(is.numeric) & !matches("n"), ~ round(.x, 3)))

print("\nDeskriptive Statistiken pro Communication Condition:")
print(variance_stats)

# Varianzenverhältnis prüfen (Faustregel: größte/kleinste Varianz < 4)
variance_ratio <- max(variance_stats$variance) / min(variance_stats$variance)
print(paste("Varianzenverhältnis (max/min):", round(variance_ratio, 3)))
print(paste("Homogenitätsannahme erfüllt (< 4):", variance_ratio < 4))

# ================================================================================
# SCHRITT 2: EINFAKTORIELLE ANOVA - COMMUNICATION CONDITION
# ================================================================================

print("\n--- EINFAKTORIELLE ANOVA: COMMUNICATION CONDITION ---")

# Standard ANOVA
anova_comm <- aov(flow_score ~ communication_condition, data = comprehensive_data)
anova_comm_summary <- summary(anova_comm)
print("Einfaktorielle ANOVA - Communication Condition:")
print(anova_comm_summary)

# Typ II ANOVA (robuster bei unbalancierten Designs)
anova_comm_type2 <- Anova(anova_comm, type = "II")
print("\nTyp II ANOVA - Communication Condition:")
print(anova_comm_type2)

# Effektgrößen
eta_squared_comm <- eta_squared(anova_comm, partial = TRUE)
omega_squared_comm <- omega_squared(anova_comm, partial = TRUE)

print("\nEffektgrößen - Communication Condition:")
print("Eta-squared (partiell):")
print(eta_squared_comm)
print("Omega-squared (partiell):")
print(omega_squared_comm)

# ================================================================================
# SCHRITT 3: ZWEIFAKTORIELLE ANOVA - COMMUNICATION × DIFFICULTY  
# ================================================================================

print("\n--- ZWEIFAKTORIELLE ANOVA: COMMUNICATION × DIFFICULTY ---")

# Überprüfe Zellenbesetzung
cell_counts <- table(comprehensive_data$communication_condition, comprehensive_data$difficulty)
print("Zellenbesetzung (Communication × Difficulty):")
print(cell_counts)

# Identifiziere leere Zellen
empty_cells <- which(cell_counts == 0, arr.ind = TRUE)
if(nrow(empty_cells) > 0) {
  print("WARNUNG: Leere Zellen gefunden!")
  print(empty_cells)
} else {
  print("Keine leeren Zellen - Design ist balanciert genug für ANOVA")
}

# Zweifaktorielle ANOVA
anova_full <- aov(flow_score ~ communication_condition * difficulty, data = comprehensive_data)
anova_full_summary <- summary(anova_full)
print("\nZweifaktorielle ANOVA (Communication × Difficulty):")
print(anova_full_summary)

# Typ III ANOVA (für unbalancierte Designs mit Interaktionen)
anova_full_type3 <- Anova(anova_full, type = "III")
print("\nTyp III ANOVA - Communication × Difficulty:")
print(anova_full_type3)

# Effektgrößen für alle Faktoren
eta_squared_full <- eta_squared(anova_full, partial = TRUE)
omega_squared_full <- omega_squared(anova_full, partial = TRUE)

print("\nEffektgrößen - Vollständiges Modell:")
print("Eta-squared (partiell):")
print(eta_squared_full)
print("Omega-squared (partiell):")
print(omega_squared_full)

# ================================================================================
# SCHRITT 4: ANCOVA - MIT FLOW PRONENESS
# ================================================================================

print("\n--- ANCOVA: COMMUNICATION × DIFFICULTY + FLOW PRONENESS ---")

# ANCOVA mit Flow Proneness als Kovariate
ancova_model <- aov(flow_score ~ communication_condition * difficulty + fp_total, 
                   data = comprehensive_data)
ancova_summary <- summary(ancova_model)
print("ANCOVA mit Flow Proneness:")
print(ancova_summary)

# Typ III ANCOVA
ancova_type3 <- Anova(ancova_model, type = "III")
print("\nTyp III ANCOVA:")
print(ancova_type3)

# Effektgrößen ANCOVA
eta_squared_ancova <- eta_squared(ancova_model, partial = TRUE)
print("\nEffektgrößen ANCOVA:")
print(eta_squared_ancova)

# ================================================================================
# SCHRITT 5: POST-HOC TESTS
# ================================================================================

print("\n--- POST-HOC TESTS ---")

# Tukey HSD für Communication Condition
print("\n1. TUKEY HSD - COMMUNICATION CONDITION")
tukey_comm <- TukeyHSD(anova_comm)
print("Tukey HSD Ergebnisse:")
print(tukey_comm)

# Tukey HSD für vollständiges Modell (falls Interaktion signifikant)
if(anova_full_summary[[1]][3, 5] < 0.05) {  # Interaktion p-Wert
  print("\n2. TUKEY HSD - COMMUNICATION × DIFFICULTY (da Interaktion signifikant)")
  tukey_full <- TukeyHSD(anova_full, which = "communication_condition:difficulty")
  print(tukey_full)
} else {
  print("\n2. Interaktion nicht signifikant - keine Post-hoc Tests für Interaktion nötig")
}

# Bonferroni-korrigierte paarweise t-Tests als Alternative
print("\n3. BONFERRONI-KORRIGIERTE PAARWEISE T-TESTS")
pairwise_t <- pairwise.t.test(comprehensive_data$flow_score, 
                             comprehensive_data$communication_condition, 
                             p.adjust.method = "bonferroni")
print("Bonferroni-korrigierte paarweise t-Tests:")
print(pairwise_t)

# ================================================================================
# SCHRITT 6: ROBUSTHEITSANALYSEN
# ================================================================================

print("\n--- ROBUSTHEITSANALYSEN ---")

# Welch-ANOVA (robust gegen Varianzheteroskedastizität)
print("\n1. WELCH-ANOVA (robust gegen ungleiche Varianzen)")
welch_anova <- oneway.test(flow_score ~ communication_condition, 
                          data = comprehensive_data, 
                          var.equal = FALSE)
print(welch_anova)

# Kruskal-Wallis Test (nicht-parametrische Alternative)
print("\n2. KRUSKAL-WALLIS TEST (nicht-parametrisch)")
kruskal_test <- kruskal.test(flow_score ~ communication_condition, 
                           data = comprehensive_data)
print(kruskal_test)

# Bei signifikantem Kruskal-Wallis: Dunn-Test für Post-hoc
if(kruskal_test$p.value < 0.05) {
  print("\n3. DUNN-TEST (Post-hoc für Kruskal-Wallis)")
  # Vereinfachter paarweiser Wilcoxon-Test mit Bonferroni-Korrektur
  dunn_alternative <- pairwise.wilcox.test(comprehensive_data$flow_score, 
                                         comprehensive_data$communication_condition,
                                         p.adjust.method = "bonferroni")
  print("Paarweise Wilcoxon-Tests (Bonferroni-korrigiert):")
  print(dunn_alternative)
}

# ================================================================================
# SCHRITT 7: MODELLVERGLEICH UND ZUSAMMENFASSUNG
# ================================================================================

print("\n--- MODELLVERGLEICH: ANOVA vs MIXED-EFFECTS ---")

# AIC/BIC Vergleich zwischen ANOVA und Mixed-Effects Modellen
anova_lm <- lm(flow_score ~ communication_condition * difficulty + fp_total, 
               data = comprehensive_data)

print("Modellvergleich (AIC/BIC):")
print("Mixed-Effects Modell (aus vorheriger Analyse):")
print(paste("AIC:", round(AIC(model_comprehensive_4), 2)))
print(paste("BIC:", round(BIC(model_comprehensive_4), 2)))

print("Standard lineares Modell (ANCOVA):")
print(paste("AIC:", round(AIC(anova_lm), 2)))
print(paste("BIC:", round(BIC(anova_lm), 2)))

# R-squared für ANOVA-Modelle
r_squared_comm <- summary(lm(flow_score ~ communication_condition, data = comprehensive_data))$r.squared
r_squared_full <- summary(lm(flow_score ~ communication_condition * difficulty, data = comprehensive_data))$r.squared  
r_squared_ancova <- summary(anova_lm)$r.squared

print("\nErklärte Varianz (R²):")
print(paste("Nur Communication Condition:", round(r_squared_comm, 4)))
print(paste("Communication × Difficulty:", round(r_squared_full, 4)))
print(paste("ANCOVA (+ Flow Proneness):", round(r_squared_ancova, 4)))

# ================================================================================
# SCHRITT 8: ZUSAMMENFASSUNG DER ANOVA-BEFUNDE
# ================================================================================

print("\n=== ZUSAMMENFASSUNG DER ANOVA-BEFUNDE ===")

# Hauptbefunde extrahieren
comm_f_stat <- anova_comm_summary[[1]][1, 4]  # F-Statistik
comm_p_val <- anova_comm_summary[[1]][1, 5]   # p-Wert
comm_eta_sq <- eta_squared_comm$Eta2_partial[1] # Eta-squared

print("HAUPTBEFUNDE:")
print(sprintf("1. Communication Condition Haupteffekt: F(%d,%d) = %.3f, p = %.4f, η²p = %.3f",
              anova_comm_summary[[1]][1, 1],  # df1
              anova_comm_summary[[1]][2, 1],  # df2  
              comm_f_stat, comm_p_val, comm_eta_sq))

if(comm_p_val < 0.001) {
  significance_level <- "hoch signifikant (p < .001)"
} else if(comm_p_val < 0.01) {
  significance_level <- "sehr signifikant (p < .01)"
} else if(comm_p_val < 0.05) {
  significance_level <- "signifikant (p < .05)"
} else {
  significance_level <- "nicht signifikant (p ≥ .05)"
}

effect_size_interpretation <- case_when(
  comm_eta_sq < 0.01 ~ "vernachlässigbar",
  comm_eta_sq < 0.06 ~ "klein", 
  comm_eta_sq < 0.14 ~ "mittel",
  TRUE ~ "groß"
)

print(sprintf("   Interpretation: %s mit %s Effekt", significance_level, effect_size_interpretation))

# Annahmen-Check Zusammenfassung
assumptions_met <- TRUE
assumption_violations <- c()

if(levene_test$`Pr(>F)`[1] < 0.05) {
  assumptions_met <- FALSE
  assumption_violations <- c(assumption_violations, "Varianzhomogenität verletzt")
}

if(ad_test$p.value < 0.05) {
  assumptions_met <- FALSE  
  assumption_violations <- c(assumption_violations, "Normalität verletzt")
}

print("\n2. ANNAHMEN-ÜBERPRÜFUNG:")
if(assumptions_met) {
  print("   Alle ANOVA-Annahmen erfüllt ✓")
  print("   → Parametrische ANOVA-Ergebnisse sind vertrauenswürdig")
} else {
  print(paste("   Verletzungen:", paste(assumption_violations, collapse = ", ")))
  print("   → Robustheitsanalysen (Welch-ANOVA, Kruskal-Wallis) beachten!")
}

# Konsistenz mit Mixed-Effects Check
print("\n3. KONSISTENZ MIT MIXED-EFFECTS MODELL:")
print("   (Detaillierte Vergleiche siehe vorherige Emmeans-Analyse)")
print("   → Bei ähnlichen p-Werten: Befunde robust über Analysemethoden hinweg")
print("   → Bei unterschiedlichen Ergebnissen: Mixed-Effects bevorzugen (berücksichtigt Datenstruktur)")

print("\n=== ENDE DER ANOVA-ANALYSE ===")
```

Shared flow calculation via Intra-class coefficient (univariate and multivariate)

```{r}
# ================================================================================
# TEIL 1: UNIVARIATE ICCs (nach Task UND Kommunikationsmedium getrennt)
# ================================================================================

print("\n--- UNIVARIATE ICCs ---")

# Daten aufteilen nach Task und Kommunikationsmedium
math_jitsi_data <- model_data_math %>% filter(comm == "Jitsi")
math_chat_data <- model_data_math %>% filter(comm == "Chat")
hp_jitsi_data <- model_data_hp %>% filter(comm == "Jitsi")
hp_chat_data <- model_data_hp %>% filter(comm == "Chat")

# Funktion für Univariate ICC (orientiert an deiner ursprünglichen Version)
compute_univariate_icc_simple <- function(data, name) {
  cat(sprintf("\nUnivariate ICC (%s):\n", name))
  cat(sprintf("Sample: %d Beobachtungen, %d Teams\n", nrow(data), length(unique(data$team_id))))
  
  # Verwende nur team_id als Random Effect (wie in deiner compute_icc Funktion)
  model <- lmer(flow_score ~ 1 + (1 | team_id), data = data)
  icc_result <- icc(model)
  
  print(icc_result)
  return(list(name = name, model = model, icc = icc_result))
}

# Berechne Univariate ICCs für alle Kombinationen
icc_math_jitsi <- compute_univariate_icc_simple(math_jitsi_data, "Math-Jitsi")
icc_math_chat <- compute_univariate_icc_simple(math_chat_data, "Math-Chat")
icc_hp_jitsi <- compute_univariate_icc_simple(hp_jitsi_data, "HP-Jitsi")
icc_hp_chat <- compute_univariate_icc_simple(hp_chat_data, "HP-Chat")

# ================================================================================
# TEIL 2: MULTIVARIATE ICCs nach Difficulty
# ================================================================================

print("\n--- MULTIVARIATE ICCs nach Difficulty separiert---")

# Funktion für Multivariate ICC (wie deine ursprüngliche compute_icc Funktion)
compute_multivariate_icc_simple <- function(data, base_name) {
  cat(sprintf("\nMultivariate ICCs für %s (Separate ICC pro Difficulty):\n", base_name))
  
  # Verfügbare Difficulty Levels
  difficulties <- unique(data$difficulty)
  cat(sprintf("Difficulty Levels: %s\n", paste(difficulties, collapse = ", ")))
  
  results <- list()
  
  for (level in difficulties) {
    subset_data <- data %>% filter(difficulty == level)
    
    if (nrow(subset_data) > 0 && length(unique(subset_data$team_id)) > 1) {
      cat(sprintf("\n%s - %s:\n", base_name, level))
      cat(sprintf("Sample: %d Beobachtungen, %d Teams\n", 
                  nrow(subset_data), length(unique(subset_data$team_id))))
      
      model <- lmer(flow_score ~ 1 + (1 | team_id), data = subset_data)
      icc_result <- icc(model)
      print(icc_result)
      
      results[[level]] <- list(
        difficulty = level,
        model = model,
        icc = icc_result,
        n_obs = nrow(subset_data),
        n_teams = length(unique(subset_data$team_id))
      )
    } else {
      cat(sprintf("\n%s - %s: Nicht genügend Daten\n", base_name, level))
      results[[level]] <- NULL
    }
  }
  
  return(results)
}

# ================================================================================
# TEIL 2b: ECHTER MULTIVARIATE ICC
# ================================================================================

print("\n--- ECHTER MULTIVARIATE ICC ---")

# Funktion für echten Multivariate ICC mit multilevel::mult.icc
compute_true_multivariate_icc <- function(data, base_name) {
  cat(sprintf("\nEchter Multivariate ICC für %s:\n", base_name))
  
  # Prüfe verfügbare Daten
  cat(sprintf("Gesamt Sample: %d Beobachtungen, %d Teams\n", 
              nrow(data), length(unique(data$team_id))))
  
  difficulties <- unique(data$difficulty)
  cat(sprintf("Difficulty Levels: %s\n", paste(difficulties, collapse = ", ")))
  
  # Bereite Daten für mult.icc vor
  icc_data <- data %>%
    select(flow_score, difficulty, team_id) %>%
    # Verwende explizite dplyr::rename
    dplyr::rename(val = flow_score, Condition = difficulty, SessionID = team_id) %>%
    filter(!is.na(val), !is.na(Condition), !is.na(SessionID))
  
  cat(sprintf("Nach Filterung: %d gültige Beobachtungen\n", nrow(icc_data)))
  
  tryCatch({
    # Multivariate ICC
    multivariate_result <- icc_data %>%
      group_by(Condition) %>%
      mutate(dummy = 1:n()) %>%  # Dummy Variable
      # Prüfe ob genug Daten pro Condition
      filter(n() >= 3, length(unique(SessionID)) >= 2) %>%
      do(multilevel::mult.icc(x = as.data.frame(.[, c("val", "dummy")]), .$SessionID)) %>%
      ungroup() %>%
      select(-ICC2) %>%  # Nur ICC1 behalten
      spread(Condition, ICC1) %>%  # Von Long zu Wide
      filter(Variable != "dummy")  # Dummy-Zeile entfernen
    
    # Nur zeigen wenn Ergebnisse vorhanden
    if (!is.null(multivariate_result) && nrow(multivariate_result) > 0) {
      cat("Multivariate ICC Ergebnisse (ICC1 nach Difficulty):\n")
      print(multivariate_result)
    } else {
      cat("Keine gültigen Multivariate ICC Ergebnisse\n")
    }
    
    return(list(
      name = base_name,
      result = multivariate_result,
      raw_data = icc_data
    ))
    
  }, error = function(e) {
    cat(sprintf("Fehler bei Multivariate ICC Berechnung: %s\n", e$message))
    cat("Mögliche Gründe: Zu wenig Daten pro Condition oder zu wenig Teams\n")
    return(NULL)
  })
}

# Berechne separate univariate ICCs pro Difficulty
cat("\n=== SEPARATE ICCs PRO DIFFICULTY LEVEL ===")
multivariate_math_jitsi <- compute_multivariate_icc_simple(math_jitsi_data, "Math-Jitsi")
multivariate_math_chat <- compute_multivariate_icc_simple(math_chat_data, "Math-Chat")
multivariate_hp_jitsi <- compute_multivariate_icc_simple(hp_jitsi_data, "HP-Jitsi")
multivariate_hp_chat <- compute_multivariate_icc_simple(hp_chat_data, "HP-Chat")

# Berechne echte Multivariate ICCs
cat("\n=== ECHTER MULTIVARIATE ICC ===")
true_multi_math_jitsi <- compute_true_multivariate_icc(math_jitsi_data, "Math-Jitsi")
true_multi_math_chat <- compute_true_multivariate_icc(math_chat_data, "Math-Chat")
true_multi_hp_jitsi <- compute_true_multivariate_icc(hp_jitsi_data, "HP-Jitsi")
true_multi_hp_chat <- compute_true_multivariate_icc(hp_chat_data, "HP-Chat")

# ================================================================================
# TEIL 3: ZUSAMMENFASSUNG UND VERGLEICHE
# ================================================================================

print("\n\n=== ZUSAMMENFASSUNG UND VERGLEICHE ===")

# Extrahiere ICC-Werte für Vergleich (robust gegen verschiedene icc() Output-Formate)
extract_icc_value <- function(icc_result) {
  if (is.list(icc_result)) {
    # Versuche verschiedene Feldnamen
    if ("ICC_adjusted" %in% names(icc_result)) {
      return(icc_result$ICC_adjusted)
    } else if ("ICC_conditional" %in% names(icc_result)) {
      return(icc_result$ICC_conditional)  
    } else if (length(icc_result) > 0) {
      return(icc_result[[1]]) # Nimm den ersten Wert
    }
  } else if (is.numeric(icc_result)) {
    return(icc_result[1]) # Falls es ein numerischer Vektor ist
  }
  return(NA)
}

# Erstelle Vergleichstabelle für Univariate ICCs
univariate_comparison <- data.frame(
  Task_Communication = c("Math-Jitsi", "Math-Chat", "HP-Jitsi", "HP-Chat"),
  ICC_Value = c(
    extract_icc_value(icc_math_jitsi$icc),
    extract_icc_value(icc_math_chat$icc),
    extract_icc_value(icc_hp_jitsi$icc),
    extract_icc_value(icc_hp_chat$icc)
  ),
  N_Observations = c(
    nrow(math_jitsi_data),
    nrow(math_chat_data),
    nrow(hp_jitsi_data),
    nrow(hp_chat_data)
  ),
  N_Teams = c(
    length(unique(math_jitsi_data$team_id)),
    length(unique(math_chat_data$team_id)),
    length(unique(hp_jitsi_data$team_id)),
    length(unique(hp_chat_data$team_id))
  )
)

cat("\nUnivariate ICC Vergleich:\n")
print(univariate_comparison)

# Analysiere Unterschiede zwischen Kommunikationsmedien
cat("\nVergleich zwischen Kommunikationsmedien:\n")

# Math Task Vergleich
math_jitsi_value <- univariate_comparison$ICC_Value[univariate_comparison$Task_Communication == "Math-Jitsi"]
math_chat_value <- univariate_comparison$ICC_Value[univariate_comparison$Task_Communication == "Math-Chat"]

if (!is.na(math_jitsi_value) && !is.na(math_chat_value)) {
  cat(sprintf("Math Task: Jitsi (%.4f) vs Chat (%.4f) - Differenz: %.4f\n", 
              math_jitsi_value, math_chat_value, math_jitsi_value - math_chat_value))
}

# HP Task Vergleich  
hp_jitsi_value <- univariate_comparison$ICC_Value[univariate_comparison$Task_Communication == "HP-Jitsi"]
hp_chat_value <- univariate_comparison$ICC_Value[univariate_comparison$Task_Communication == "HP-Chat"]

if (!is.na(hp_jitsi_value) && !is.na(hp_chat_value)) {
  cat(sprintf("HP Task: Jitsi (%.4f) vs Chat (%.4f) - Differenz: %.4f\n", 
              hp_jitsi_value, hp_chat_value, hp_jitsi_value - hp_chat_value))
}

# Erstelle Multivariate Zusammenfassung (beide Methoden)
create_multivariate_summary <- function(results, name) {
  cat(sprintf("\n%s - Separate ICCs nach Difficulty:\n", name))
  
  for (difficulty in names(results)) {
    result <- results[[difficulty]]
    if (!is.null(result)) {
      icc_val <- extract_icc_value(result$icc)
      if (!is.na(icc_val)) {
        cat(sprintf("  %s: ICC = %.4f (%d obs, %d teams)\n", 
                    difficulty, icc_val, result$n_obs, result$n_teams))
      }
    }
  }
}

create_multivariate_summary(multivariate_math_jitsi, "Math-Jitsi")
create_multivariate_summary(multivariate_math_chat, "Math-Chat")
create_multivariate_summary(multivariate_hp_jitsi, "HP-Jitsi")
create_multivariate_summary(multivariate_hp_chat, "HP-Chat")

cat("\n=== ECHTE MULTIVARIATE ICC ERGEBNISSE ===\n")

show_true_multivariate <- function(result, name) {
  cat(sprintf("\n%s - Echter Multivariate ICC:\n", name))
  
  if (!is.null(result) && !is.null(result$result)) {
    if (nrow(result$result) > 0) {
      # Extrahiere und zeige die ICC-Werte ohne die Tabelle nochmal zu printen
      numeric_cols <- sapply(result$result, is.numeric)
      if (any(numeric_cols)) {
        cat("ICC1-Werte nach Difficulty:\n")
        for (col in names(result$result)[numeric_cols]) {
          val <- result$result[[col]][1]
          if (!is.na(val)) {
            cat(sprintf("  %s: ICC1 = %.4f\n", col, val))
          }
        }
      }
    } else {
      cat("Keine gültigen Ergebnisse verfügbar\n")
    }
  } else {
    cat("Multivariate ICC konnte nicht berechnet werden\n")
  }
}

if (!is.null(true_multi_math_jitsi)) show_true_multivariate(true_multi_math_jitsi, "Math-Jitsi")
if (!is.null(true_multi_math_chat)) show_true_multivariate(true_multi_math_chat, "Math-Chat")
if (!is.null(true_multi_hp_jitsi)) show_true_multivariate(true_multi_hp_jitsi, "HP-Jitsi")
if (!is.null(true_multi_hp_chat)) show_true_multivariate(true_multi_hp_chat, "HP-Chat")
```

Correlation of flow with anticipated mediators

```{r}
# Master Thesis Analysis: Flow Mediation Analysis - Revised Version
# Getrennte Analysen für Math und Hidden Profile Tasks

library(dplyr)
library(tidyr)
library(lme4)
library(lmerTest)
library(rmcorr)
library(ggplot2)
library(psych)

# Konflikt zwischen plyr und dplyr lösen
summarise <- dplyr::summarise
mutate <- dplyr::mutate
select <- dplyr::select

# ================================================================================
# TEIL 1: DATENAUFBEREITUNG - NEUE MEDIATOREN
# ================================================================================

# Funktion zur Extraktion rundenweiser Mediatoren
extract_round_based_mediators <- function(data, var_pattern, var_name) {
  # Extrahiere alle relevanten Spalten
  med_data <- data %>%
    select(participant.code, contains("player")) %>%
    select(participant.code, matches(paste0("(mathJitsi|mathChat|HiddenProfile_Jitsi|HiddenProfile_Chat).*\\.", var_pattern, "$")))
  
  # Umstrukturierung in Long-Format
  med_long <- med_data %>%
    pivot_longer(cols = -participant.code, 
                 names_to = "variable", 
                 values_to = "value") %>%
    filter(!is.na(value)) %>%
    mutate(
      task = case_when(
        grepl("^math", variable) ~ "Math",
        grepl("^HiddenProfile", variable) ~ "HP"
      ),
      comm = case_when(
        grepl("Jitsi", variable) ~ "Jitsi",
        grepl("Chat", variable) ~ "Chat"
      ),
      round_raw = as.numeric(gsub(".*\\.(\\d+)\\.player.*", "\\1", variable)),
      # Standardisiere Rundennummern: Math 3-6 wird zu 1-4, HP 1-3 bleibt 1-3
      round = case_when(
        task == "Math" ~ round_raw - 2,  # Math: 3-6 wird zu 1-4
        task == "HP" ~ round_raw          # HP: 1-3 bleibt 1-3
      ),
      mediator = var_name
    ) %>%
    # Filtere nur die relevanten Runden
    filter((task == "Math" & round_raw >= 3 & round_raw <= 6) | 
           (task == "HP" & round_raw >= 1 & round_raw <= 3)) %>%
    select(participant.code, task, comm, round, mediator, value)
  
  return(med_long)
}

# Funktion zur Extraktion von Mediatoren mit mehreren Items
extract_multi_item_mediators <- function(data, items, var_name, round_based = TRUE) {
  if(round_based) {
    # Für rundenweise Mediatoren
    all_data <- data.frame()
    
    for(item in items) {
      item_data <- extract_round_based_mediators(data, item, paste0(var_name, "_", item))
      all_data <- bind_rows(all_data, item_data)
    }
    
    # Aggregiere über Items
    aggregated <- all_data %>%
      mutate(base_mediator = var_name) %>%
      group_by(participant.code, task, comm, round, base_mediator) %>%
      summarise(value = mean(value, na.rm = TRUE), .groups = "drop") %>%
      dplyr::rename(mediator = base_mediator)
    
    return(aggregated)
  } else {
    # Für einmalige Mediatoren (nur letzte Runde)
    med_data <- data %>%
      select(participant.code, contains("player"))
    
    # Erstelle Pattern für letzte Runde
    patterns <- c()
    for(item in items) {
      # Math: Runde 6, HP: Runde 3
      patterns <- c(patterns, 
                    paste0("mathJitsi\\.6\\.player\\.", item, "$"),
                    paste0("mathChat\\.6\\.player\\.", item, "$"),
                    paste0("HiddenProfile_Jitsi\\.3\\.player\\.", item, "$"),
                    paste0("HiddenProfile_Chat\\.3\\.player\\.", item, "$"))
    }
    
    # Kombiniere alle Patterns
    combined_pattern <- paste(patterns, collapse = "|")
    
    # Wähle relevante Spalten
    med_data <- med_data %>%
      select(participant.code, matches(combined_pattern))
    
    # Umstrukturierung
    med_long <- med_data %>%
      pivot_longer(cols = -participant.code, 
                   names_to = "variable", 
                   values_to = "value") %>%
      filter(!is.na(value)) %>%
      mutate(
        task = case_when(
          grepl("^math", variable) ~ "Math",
          grepl("^HiddenProfile", variable) ~ "HP"
        ),
        comm = case_when(
          grepl("Jitsi", variable) ~ "Jitsi",
          grepl("Chat", variable) ~ "Chat"
        ),
        item = gsub(".*\\.(\\w+)$", "\\1", variable)
      )
    
    # Aggregiere über Items
    aggregated <- med_long %>%
      group_by(participant.code, task, comm) %>%
      summarise(value = mean(value, na.rm = TRUE), .groups = "drop")
    
    return(aggregated)
  }
}

# ================================================================================
# STRUKTURELLE INTEGRATION
# ================================================================================

# Team Composition (nur einmal pro Aufgabe erhoben - letzte Runde)
tc_aggregated <- extract_multi_item_mediators(data, 
                                              c("tsz1", "tsz2", "tsz3", "td1", "td2", "td3", "tsc1", "tsc2", "tsc3"), 
                                              "team_composition", 
                                              round_based = FALSE)

# ================================================================================
# FUNKTIONALE INTEGRATION
# ================================================================================

# Information Sharing (info1, info2) - rundenweise
is_long <- extract_multi_item_mediators(data, c("info1", "info2"), "information_sharing", round_based = TRUE)

# Synchronization (ec1) - rundenweise
sync_long <- extract_round_based_mediators(data, "ec1", "synchronization")

# ================================================================================
# MOTIVATIONALE INTEGRATION
# ================================================================================

# Stress (is1-is5) - rundenweise
stress_long <- extract_multi_item_mediators(data, c("is1", "is2", "is3", "is4", "is5"), "stress", round_based = TRUE)

# Arousal - rundenweise
arousal_long <- extract_round_based_mediators(data, "arousal", "arousal")

# Valence (pleasure) - rundenweise
valence_long <- extract_round_based_mediators(data, "pleasure", "valence")

# Individual Motivation (tm1-tm3) - rundenweise
ind_motiv_long <- extract_multi_item_mediators(data, c("tm1", "tm2", "tm3"), "individual_motivation", round_based = TRUE)

# Team Motivation (te1-te3, nur einmal pro Aufgabe - letzte Runde)
team_motiv_aggregated <- extract_multi_item_mediators(data, c("te1", "te2", "te3"), "team_motivation", round_based = FALSE)

# ================================================================================
# FLOW SCORES VORBEREITEN
# ================================================================================

# Flow Scores aggregieren (falls noch nicht geschehen)
flow_aggregated <- flow_scores %>%
  group_by(participant.code, task, comm) %>%
  summarise(mean_flow_score = mean(flow_score, na.rm = TRUE), .groups = "drop")

# ================================================================================
# TEIL 2: REPEATED MEASURES KORRELATIONEN
# ================================================================================

# Funktion für rmcorr Analyse
perform_rmcorr <- function(data, mediator_name, task_filter) {
  # Filtere nach Task
  task_data <- data %>%
    filter(task == task_filter)
  
  # Flow Scores vorbereiten mit korrekten Rundennummern
  if(task_filter == "Math") {
    # Für Math: Wir brauchen nur die Runden, wo rundenweise Mediatoren gemessen wurden
    flow_round <- flow_scores %>%
      filter(task == task_filter) %>%
      group_by(participant.code, comm) %>%
      arrange(participant.code, comm) %>%
      mutate(round_raw = row_number()) %>%
      # Behalte nur Runden 3-6 (entspricht round 1-4 nach Transformation)
      filter(round_raw >= 3) %>%
      mutate(round = round_raw - 2) %>%
      ungroup()
  } else {
    # Für HP: Runden 1-3
    flow_round <- flow_scores %>%
      filter(task == task_filter) %>%
      group_by(participant.code, comm) %>%
      arrange(participant.code, comm) %>%
      mutate(round = row_number()) %>%
      filter(round <= 3) %>%
      ungroup()
  }
  
  # Merge mit Mediator-Daten
  merged_data <- task_data %>%
    left_join(flow_round %>% select(participant.code, comm, round, flow_score), 
              by = c("participant.code", "comm", "round")) %>%
    filter(!is.na(flow_score) & !is.na(value))
  
  # Berechne rmcorr für jede Kommunikationsbedingung
  results <- list()
  
  for(comm_type in c("Jitsi", "Chat")) {
    comm_data <- merged_data %>% filter(comm == comm_type)
    
    if(nrow(comm_data) > 10) {  # Genug Datenpunkte
      rmcorr_result <- rmcorr(participant = participant.code, 
                              measure1 = value, 
                              measure2 = flow_score, 
                              dataset = comm_data)
      
      results[[comm_type]] <- list(
        r = rmcorr_result$r,
        p = rmcorr_result$p,
        df = rmcorr_result$df,
        CI = rmcorr_result$CI
      )
    }
  }
  
  return(results)
}

# Führe rmcorr für alle rundenweisen Mediatoren durch
print("=== REPEATED MEASURES CORRELATIONS ===\n")

# Liste aller rundenweisen Mediatoren
round_based_mediators <- list(
  "information_sharing" = is_long,
  "synchronization" = sync_long,
  "stress" = stress_long,
  "arousal" = arousal_long,
  "valence" = valence_long,
  "individual_motivation" = ind_motiv_long
)

# Durchführung für Math Task
print("--- MATH TASK ---")
rmcorr_results_math <- list()
for(med_name in names(round_based_mediators)) {
  cat("\n", med_name, ":\n", sep = "")
  result <- perform_rmcorr(round_based_mediators[[med_name]], med_name, "Math")
  rmcorr_results_math[[med_name]] <- result
  
  # Ausgabe
  for(comm in names(result)) {
    cat("  ", comm, ": r = ", round(result[[comm]]$r, 3), 
        ", p = ", round(result[[comm]]$p, 3), 
        ", 95% CI [", round(result[[comm]]$CI[1], 3), ", ", 
        round(result[[comm]]$CI[2], 3), "]\n", sep = "")
  }
}

# Durchführung für HP Task
print("\n--- HIDDEN PROFILE TASK ---")
rmcorr_results_hp <- list()
for(med_name in names(round_based_mediators)) {
  cat("\n", med_name, ":\n", sep = "")
  result <- perform_rmcorr(round_based_mediators[[med_name]], med_name, "HP")
  rmcorr_results_hp[[med_name]] <- result
  
  # Ausgabe
  for(comm in names(result)) {
    cat("  ", comm, ": r = ", round(result[[comm]]$r, 3), 
        ", p = ", round(result[[comm]]$p, 3), 
        ", 95% CI [", round(result[[comm]]$CI[1], 3), ", ", 
        round(result[[comm]]$CI[2], 3), "]\n", sep = "")
  }
}

# ================================================================================
# TEIL 3: LINEAR MIXED MODELS
# ================================================================================

# Funktion für LMM Analysen
perform_lmm_analysis <- function(mediator_data, mediator_name, task_filter, is_round_based = TRUE) {
  
  # Filtere nach Task
  task_data <- mediator_data %>%
    filter(task == task_filter)
  
  if(is_round_based) {
    # Für rundenweise Mediatoren
    # Modell 1: Kommunikationsmedium -> Mediator
    model1 <- lmer(value ~ comm + (1|participant.code) + (1|round), 
                   data = task_data)
    
    # Flow Scores mit korrekten Rundennummern vorbereiten
    if(task_filter == "Math") {
      flow_round <- flow_scores %>%
        filter(task == task_filter) %>%
        group_by(participant.code, comm) %>%
        arrange(participant.code, comm) %>%
        mutate(round_raw = row_number()) %>%
        filter(round_raw >= 3) %>%
        mutate(round = round_raw - 2) %>%
        ungroup()
    } else {
      flow_round <- flow_scores %>%
        filter(task == task_filter) %>%
        group_by(participant.code, comm) %>%
        arrange(participant.code, comm) %>%
        mutate(round = row_number()) %>%
        filter(round <= 3) %>%
        ungroup()
    }
    
    merged_data <- task_data %>%
      left_join(flow_round %>% select(participant.code, comm, round, flow_score), 
                by = c("participant.code", "comm", "round"))
    
    # Modell 2: Mediator -> Flow
    model2 <- lmer(flow_score ~ value + (1|participant.code) + (1|round), 
                   data = merged_data)
    
  } else {
    # Für nicht-rundenweise Mediatoren (rename value column für consistency)
    if("team_composition_score" %in% names(task_data)) {
      task_data <- task_data %>% rename(value = team_composition_score)
    }
    if("team_motivation_score" %in% names(task_data)) {
      task_data <- task_data %>% rename(value = team_motivation_score)
    }
    
    # Modell 1: Kommunikationsmedium -> Mediator
    model1 <- lm(value ~ comm, data = task_data)
    
    # Merge mit aggregierten Flow Scores
    merged_data <- task_data %>%
      left_join(flow_aggregated, by = c("participant.code", "task", "comm"))
    
    # Modell 2: Mediator -> Flow
    model2 <- lm(mean_flow_score ~ value, data = merged_data)
  }
  
  return(list(
    comm_to_mediator = model1,
    mediator_to_flow = model2
  ))
}

# ================================================================================
# LMM ANALYSEN FÜR MATH TASK
# ================================================================================

print("\n\n=== LINEAR MIXED MODELS - MATH TASK ===\n")

# Strukturelle Integration
print("--- STRUKTURELLE INTEGRATION ---")
print("\nTeam Composition:")
tc_models_math <- perform_lmm_analysis(tc_aggregated, "team_composition", "Math", FALSE)
print("Kommunikation -> Team Composition:")
print(summary(tc_models_math$comm_to_mediator))
print("\nTeam Composition -> Flow:")
print(summary(tc_models_math$mediator_to_flow))

# Funktionale Integration
print("\n--- FUNKTIONALE INTEGRATION ---")

print("\nInformation Sharing:")
is_models_math <- perform_lmm_analysis(is_long, "information_sharing", "Math", TRUE)
print("Kommunikation -> Information Sharing:")
print(summary(is_models_math$comm_to_mediator))
print("\nInformation Sharing -> Flow:")
print(summary(is_models_math$mediator_to_flow))

print("\nSynchronization:")
sync_models_math <- perform_lmm_analysis(sync_long, "synchronization", "Math", TRUE)
print("Kommunikation -> Synchronization:")
print(summary(sync_models_math$comm_to_mediator))
print("\nSynchronization -> Flow:")
print(summary(sync_models_math$mediator_to_flow))

# Motivationale Integration
print("\n--- MOTIVATIONALE INTEGRATION ---")

print("\nStress:")
stress_models_math <- perform_lmm_analysis(stress_long, "stress", "Math", TRUE)
print("Kommunikation -> Stress:")
print(summary(stress_models_math$comm_to_mediator))
print("\nStress -> Flow:")
print(summary(stress_models_math$mediator_to_flow))

print("\nArousal:")
arousal_models_math <- perform_lmm_analysis(arousal_long, "arousal", "Math", TRUE)
print("Kommunikation -> Arousal:")
print(summary(arousal_models_math$comm_to_mediator))
print("\nArousal -> Flow:")
print(summary(arousal_models_math$mediator_to_flow))

print("\nValence:")
valence_models_math <- perform_lmm_analysis(valence_long, "valence", "Math", TRUE)
print("Kommunikation -> Valence:")
print(summary(valence_models_math$comm_to_mediator))
print("\nValence -> Flow:")
print(summary(valence_models_math$mediator_to_flow))

print("\nIndividual Motivation:")
ind_motiv_models_math <- perform_lmm_analysis(ind_motiv_long, "individual_motivation", "Math", TRUE)
print("Kommunikation -> Individual Motivation:")
print(summary(ind_motiv_models_math$comm_to_mediator))
print("\nIndividual Motivation -> Flow:")
print(summary(ind_motiv_models_math$mediator_to_flow))

print("\nTeam Motivation:")
tm_models_math <- perform_lmm_analysis(team_motiv_aggregated, "team_motivation", "Math", FALSE)
print("Kommunikation -> Team Motivation:")
print(summary(tm_models_math$comm_to_mediator))
print("\nTeam Motivation -> Flow:")
print(summary(tm_models_math$mediator_to_flow))

# ================================================================================
# LMM ANALYSEN FÜR HP TASK
# ================================================================================

print("\n\n=== LINEAR MIXED MODELS - HIDDEN PROFILE TASK ===\n")

# Strukturelle Integration
print("--- STRUKTURELLE INTEGRATION ---")
print("\nTeam Composition:")
tc_models_hp <- perform_lmm_analysis(tc_aggregated, "team_composition", "HP", FALSE)
print("Kommunikation -> Team Composition:")
print(summary(tc_models_hp$comm_to_mediator))
print("\nTeam Composition -> Flow:")
print(summary(tc_models_hp$mediator_to_flow))

# Funktionale Integration
print("\n--- FUNKTIONALE INTEGRATION ---")

print("\nInformation Sharing:")
is_models_hp <- perform_lmm_analysis(is_long, "information_sharing", "HP", TRUE)
print("Kommunikation -> Information Sharing:")
print(summary(is_models_hp$comm_to_mediator))
print("\nInformation Sharing -> Flow:")
print(summary(is_models_hp$mediator_to_flow))

print("\nSynchronization:")
sync_models_hp <- perform_lmm_analysis(sync_long, "synchronization", "HP", TRUE)
print("Kommunikation -> Synchronization:")
print(summary(sync_models_hp$comm_to_mediator))
print("\nSynchronization -> Flow:")
print(summary(sync_models_hp$mediator_to_flow))

# Motivationale Integration
print("\n--- MOTIVATIONALE INTEGRATION ---")

print("\nStress:")
stress_models_hp <- perform_lmm_analysis(stress_long, "stress", "HP", TRUE)
print("Kommunikation -> Stress:")
print(summary(stress_models_hp$comm_to_mediator))
print("\nStress -> Flow:")
print(summary(stress_models_hp$mediator_to_flow))

print("\nArousal:")
arousal_models_hp <- perform_lmm_analysis(arousal_long, "arousal", "HP", TRUE)
print("Kommunikation -> Arousal:")
print(summary(arousal_models_hp$comm_to_mediator))
print("\nArousal -> Flow:")
print(summary(arousal_models_hp$mediator_to_flow))

print("\nValence:")
valence_models_hp <- perform_lmm_analysis(valence_long, "valence", "HP", TRUE)
print("Kommunikation -> Valence:")
print(summary(valence_models_hp$comm_to_mediator))
print("\nValence -> Flow:")
print(summary(valence_models_hp$mediator_to_flow))

print("\nIndividual Motivation:")
ind_motiv_models_hp <- perform_lmm_analysis(ind_motiv_long, "individual_motivation", "HP", TRUE)
print("Kommunikation -> Individual Motivation:")
print(summary(ind_motiv_models_hp$comm_to_mediator))
print("\nIndividual Motivation -> Flow:")
print(summary(ind_motiv_models_hp$mediator_to_flow))

print("\nTeam Motivation:")
tm_models_hp <- perform_lmm_analysis(team_motiv_aggregated, "team_motivation", "HP", FALSE)
print("Kommunikation -> Team Motivation:")
print(summary(tm_models_hp$comm_to_mediator))
print("\nTeam Motivation -> Flow:")
print(summary(tm_models_hp$mediator_to_flow))

# ================================================================================
# TEIL 4: ZUSAMMENFASSENDE TABELLEN
# ================================================================================

# Funktion zur Extraktion der Koeffizienten
extract_coefficients <- function(model, is_lmm = TRUE) {
  if(is_lmm) {
    coef_summary <- summary(model)$coefficients
    # Extrahiere Koeffizient für commJitsi
    if("commJitsi" %in% rownames(coef_summary)) {
      return(c(
        estimate = coef_summary["commJitsi", "Estimate"],
        se = coef_summary["commJitsi", "Std. Error"],
        t = coef_summary["commJitsi", "t value"],
        p = coef_summary["commJitsi", "Pr(>|t|)"]
      ))
    }
  } else {
    coef_summary <- summary(model)$coefficients
    if("commJitsi" %in% rownames(coef_summary)) {
      return(c(
        estimate = coef_summary["commJitsi", "Estimate"],
        se = coef_summary["commJitsi", "Std. Error"],
        t = coef_summary["commJitsi", "t value"],
        p = coef_summary["commJitsi", "Pr(>|t|)"]
      ))
    }
  }
  
  # Falls value der Prädiktor ist
  if("value" %in% rownames(coef_summary)) {
    return(c(
      estimate = coef_summary["value", "Estimate"],
      se = coef_summary["value", "Std. Error"],
      t = coef_summary["value", "t value"],
      p = coef_summary["value", "Pr(>|t|)"]
    ))
  }
  
  return(c(estimate = NA, se = NA, t = NA, p = NA))
}

# Erstelle Zusammenfassungstabelle für Math Task
create_summary_table <- function(models_list, task_name) {
  summary_data <- data.frame()
  
  for(med_name in names(models_list)) {
    if(!is.null(models_list[[med_name]])) {
      # Kommunikation -> Mediator
      comm_coef <- extract_coefficients(models_list[[med_name]]$comm_to_mediator)
      
      # Mediator -> Flow
      flow_coef <- extract_coefficients(models_list[[med_name]]$mediator_to_flow)
      
      row_data <- data.frame(
        Mediator = med_name,
        Comm_to_Med_Est = round(comm_coef["estimate"], 3),
        Comm_to_Med_p = round(comm_coef["p"], 3),
        Med_to_Flow_Est = round(flow_coef["estimate"], 3),
        Med_to_Flow_p = round(flow_coef["p"], 3)
      )
      
      summary_data <- rbind(summary_data, row_data)
    }
  }
  
  return(summary_data)
}

# Sammle alle Modelle
all_models_math <- list(
  "Team Composition" = tc_models_math,
  "Information Sharing" = is_models_math,
  "Synchronization" = sync_models_math,
  "Stress" = stress_models_math,
  "Arousal" = arousal_models_math,
  "Valence" = valence_models_math,
  "Individual Motivation" = ind_motiv_models_math,
  "Team Motivation" = tm_models_math
)

all_models_hp <- list(
  "Team Composition" = tc_models_hp,
  "Information Sharing" = is_models_hp,
  "Synchronization" = sync_models_hp,
  "Stress" = stress_models_hp,
  "Arousal" = arousal_models_hp,
  "Valence" = valence_models_hp,
  "Individual Motivation" = ind_motiv_models_hp,
  "Team Motivation" = tm_models_hp
)

# Erstelle Zusammenfassungstabellen
print("\n\n=== ZUSAMMENFASSUNGSTABELLEN ===\n")
print("Math Task - Übersicht der Effekte:")
summary_math <- create_summary_table(all_models_math, "Math")
print(summary_math)

print("\n\nHP Task - Übersicht der Effekte:")
summary_hp <- create_summary_table(all_models_hp, "HP")
print(summary_hp)

# ================================================================================
# TEIL 5: VISUALISIERUNGEN
# ================================================================================

# Funktion für Mediator-Plots
create_mediator_plot <- function(mediator_data, mediator_name, task_filter) {
  # Aggregiere Daten für Visualisierung
  plot_data <- mediator_data %>%
    filter(task == task_filter) %>%
    group_by(comm) %>%
    summarise(
      mean_value = mean(value, na.rm = TRUE),
      se_value = sd(value, na.rm = TRUE) / sqrt(n()),
      .groups = "drop"
    )
  
  p <- ggplot(plot_data, aes(x = comm, y = mean_value, fill = comm)) +
    geom_bar(stat = "identity", position = "dodge") +
    geom_errorbar(aes(ymin = mean_value - se_value, ymax = mean_value + se_value),
                  width = 0.2, position = position_dodge(0.9)) +
    labs(title = paste(task_filter, "Task -", mediator_name),
         x = "Communication Medium",
         y = paste("Mean", mediator_name),
         fill = "Communication") +
    theme_minimal() +
    scale_fill_brewer(palette = "Set2")
  
  return(p)
}

# Erstelle Plots für ausgewählte Mediatoren
print("\n=== VISUALISIERUNGEN ===")
print("Erstelle Plots für Mediatoren...")

# Beispiel-Plots für Math Task
p1 <- create_mediator_plot(is_long, "Information Sharing", "Math")
p2 <- create_mediator_plot(sync_long, "Synchronization", "Math")
p3 <- create_mediator_plot(stress_long, "Stress", "Math")

# Zeige Plots
print(p1)
print(p2)
print(p3)

# ================================================================================
# SCHLUSSNOTIZEN
# ================================================================================

print("\n\n=== ANALYSEN ABGESCHLOSSEN ===")
print("1. Repeated Measures Korrelationen für alle rundenweisen Mediatoren berechnet")
print("2. Linear Mixed Models für Kommunikation -> Mediator erstellt")
print("3. Linear Mixed Models für Mediator -> Flow erstellt")
print("4. Getrennte Analysen für Math und HP Tasks durchgeführt")
print("5. Zusammenfassungstabellen mit allen Effekten erstellt")
print("6. Visualisierungen der Mediator-Unterschiede zwischen Kommunikationsmedien erstellt")
```

Mediation analysis in comparison with data from experiment 1

```{r}
# Erweiterte Mediationsanalyse: Integration alter und neuer Datensätze
# Vergleich von SP, MP (ohne Kommunikation), MP+Chat und MP+Jitsi

library(dplyr)
library(tidyr)
library(lme4)
library(lmerTest)
library(rmcorr)
library(ggplot2)
library(psych)

# ================================================================================
# TEIL 1: DATENINTEGRATION - ALTE DATENSÄTZE VORBEREITEN
# ================================================================================

# 1.1 Alte rundenweise Daten vorbereiten
old_rounds_prepared <- data_old_rounds %>%
  mutate(
    # Einheitliche Spaltennamen - KORRIGIERT
    participant.code = SubjectID,
    team_id = SessionID,
    # Kommunikationsmedium kodieren
    comm = case_when(
      Treatment == "MP" ~ "Together_None",
      Treatment == "SP" ~ "Alone",
      TRUE ~ NA_character_
    ),
    # Difficulty mapping
    difficulty = case_when(
      Condition == "Overload" ~ "Hard",
      Condition == "Boredom" ~ "Easy",
      Condition == "Autonomy" ~ "Optimal_Selected",
      Condition == "Flow" ~ "Optimal_Calibrated",
      TRUE ~ NA_character_
    ),
    # Order als Character belassen - KORRIGIERT
    order = as.character(ConditionOrder),
    # Round basierend auf order - KORRIGIERT
    round = as.numeric(ConditionOrder),
    # Flow Score
    flow_score = flowFKS_9,
    # Task ist immer Math
    task = "Math"
  ) %>%
  # Mediatoren umbenennen
  dplyr::rename(
    stress_value = stress,
    individual_motivation_value = motivation,
    valence_value = valence,
    arousal_value = arousal,
    information_sharing_value = infSharing,
    synchronization_value = teamSynch
  ) %>%
  select(participant.code, team_id, comm, task, difficulty, order, round,
         flow_score, stress_value, individual_motivation_value, 
         valence_value, arousal_value, information_sharing_value, 
         synchronization_value)

# 1.2 Alte finale Daten vorbereiten (nur PHASE Condition)
old_final_prepared <- data_old_final %>%
  filter(Condition == "PHASE") %>%
  mutate(
    # Einheitliche Spaltennamen - KORRIGIERT
    participant.code = SubjectID,
    team_id = SessionID,
    # Kommunikationsmedium kodieren
    comm = case_when(
      Treatment == "MP" ~ "Together_None",
      Treatment == "SP" ~ "Alone",
      TRUE ~ NA_character_
    ),
    task = "Math",
    # Team Motivation
    team_motivation_value = grpEffort,
    # Team Composition berechnen
    team_size_score = rowMeans(cbind(
      8 - grpSizeTooLarge,  # Reverse
      8 - grpSizeTooSmall,  # Reverse
      grpSizeJustRight
    ), na.rm = TRUE),
    team_composition_value = rowMeans(cbind(
      team_size_score,
      grpDivers,
      grpSkill
    ), na.rm = TRUE)
  ) %>%
  select(participant.code, team_id, comm, task, 
         team_motivation_value, team_composition_value)

# ================================================================================
# TEIL 2: NEUE DATEN VORBEREITEN
# ================================================================================

# 2.1 Order-Variable aus aktuellem Datensatz extrahieren - NEU HINZUGEFÜGT
order_mapping <- data %>%
  select(participant.code, participant.condition_order) %>%
  distinct() %>%
  dplyr::rename(
    participant.code = participant.code,
    order = participant.condition_order
  ) %>%
  mutate(order = as.character(order))  # Als Character für Konsistenz

# 2.2 Rundenweise Mediatoren aus aktuellem Datensatz extrahieren
# (Verwende die Funktionen aus der vorherigen Analyse)

# Funktion zur Extraktion rundenweiser Mediatoren (angepasst)
extract_round_mediator_current <- function(data, var_pattern, var_name) {
  med_data <- data %>%
    select(participant.code, contains("player")) %>%
    select(participant.code, matches(paste0("(mathJitsi|mathChat).*\\.", var_pattern, "$")))
  
  med_long <- med_data %>%
    pivot_longer(cols = -participant.code, 
                 names_to = "variable", 
                 values_to = "value") %>%
    filter(!is.na(value)) %>%
    mutate(
      comm_raw = case_when(
        grepl("Jitsi", variable) ~ "Jitsi",
        grepl("Chat", variable) ~ "Chat"
      ),
      round_raw = as.numeric(gsub(".*\\.(\\d+)\\.player.*", "\\1", variable)),
      # Standardisiere Rundennummern: 3-6 wird zu 1-4
      round = round_raw - 2
    ) %>%
    filter(round_raw >= 3 & round_raw <= 6) %>%
    mutate(!!paste0(var_name, "_value") := value) %>%
    # Spaltenname korrigiert - KORRIGIERT
    dplyr::rename(participant.code = participant.code) %>%
    select(participant.code, comm_raw, round, !!paste0(var_name, "_value"))
  
  return(med_long)
}

# Extrahiere alle rundenweisen Mediatoren
current_stress <- extract_round_mediator_current(data, "is1|is2|is3|is4|is5", "stress")
current_arousal <- extract_round_mediator_current(data, "arousal", "arousal")
current_valence <- extract_round_mediator_current(data, "pleasure", "valence")
current_ind_motiv <- extract_round_mediator_current(data, "tm1|tm2|tm3", "individual_motivation")
current_info_sharing <- extract_round_mediator_current(data, "info1|info2", "information_sharing")
current_sync <- extract_round_mediator_current(data, "ec1", "synchronization")

# Aggregiere Items wo nötig
aggregate_mediator_items <- function(df, mediator_name) {
  df %>%
    group_by(participant.code, comm_raw, round) %>%
    summarise(!!paste0(mediator_name, "_value") := mean(get(paste0(mediator_name, "_value")), na.rm = TRUE),
              .groups = "drop")
}

current_stress <- aggregate_mediator_items(current_stress, "stress")
current_ind_motiv <- aggregate_mediator_items(current_ind_motiv, "individual_motivation")
current_info_sharing <- aggregate_mediator_items(current_info_sharing, "information_sharing")

# 2.3 Einmalige Mediatoren extrahieren
# Team Composition
tc_current <- data %>%
  select(participant.code, 
         matches("(mathJitsi|mathChat)\\.6\\.player\\.(tsz1|tsz2|tsz3|td1|td2|td3|tsc1|tsc2|tsc3)$")) %>%
  pivot_longer(cols = -participant.code, names_to = "variable", values_to = "value") %>%
  filter(!is.na(value)) %>%
  mutate(
    comm_raw = case_when(
      grepl("Jitsi", variable) ~ "Jitsi",
      grepl("Chat", variable) ~ "Chat"
    )
  ) %>%
  # Spaltenname korrigiert - KORRIGIERT
  dplyr::rename(participant.code = participant.code) %>%
  group_by(participant.code, comm_raw) %>%
  summarise(team_composition_value = mean(value, na.rm = TRUE), .groups = "drop")

# Team Motivation
tm_current <- data %>%
  select(participant.code, 
         matches("(mathJitsi|mathChat)\\.6\\.player\\.(te1|te2|te3)$")) %>%
  pivot_longer(cols = -participant.code, names_to = "variable", values_to = "value") %>%
  filter(!is.na(value)) %>%
  mutate(
    comm_raw = case_when(
      grepl("Jitsi", variable) ~ "Jitsi",
      grepl("Chat", variable) ~ "Chat"
    )
  ) %>%
  # Spaltenname korrigiert - KORRIGIERT
  dplyr::rename(participant.code = participant.code) %>%
  group_by(participant.code, comm_raw) %>%
  summarise(team_motivation_value = mean(value, na.rm = TRUE), .groups = "drop")

# 2.4 Kombiniere mit Flow Scores und erstelle finalen aktuellen Datensatz
current_rounds_prepared <- flow_scores %>%
  filter(task == "Math") %>%
  # Füge Order-Mapping hinzu - NEU HINZUGEFÜGT
  left_join(order_mapping, by = "participant.code") %>%
  group_by(participant.code, comm) %>%
  arrange(participant.code, comm) %>%
  mutate(
    round_raw = row_number(),
    round = round_raw - 2
  ) %>%
  filter(round_raw >= 3) %>%
  ungroup() %>%
  mutate(
    # Erweitere comm Variable
    comm = case_when(
      comm == "Jitsi" ~ "Together_Jitsi",
      comm == "Chat" ~ "Together_Chat",
      TRUE ~ comm
    ),
    comm_raw = case_when(
      comm == "Together_Jitsi" ~ "Jitsi",
      comm == "Together_Chat" ~ "Chat",
      TRUE ~ NA_character_
    )
  ) %>%
  # Füge rundenweise Mediatoren hinzu
  left_join(current_stress, by = c("participant.code", "comm_raw", "round")) %>%
  left_join(current_arousal, by = c("participant.code", "comm_raw", "round")) %>%
  left_join(current_valence, by = c("participant.code", "comm_raw", "round")) %>%
  left_join(current_ind_motiv, by = c("participant.code", "comm_raw", "round")) %>%
  left_join(current_info_sharing, by = c("participant.code", "comm_raw", "round")) %>%
  left_join(current_sync, by = c("participant.code", "comm_raw", "round")) %>%
  select(participant.code, team_id, comm, task, difficulty, round,
         flow_score, stress_value, individual_motivation_value, 
         valence_value, arousal_value, information_sharing_value, 
         synchronization_value)

# Einmalige Mediatoren für aktuellen Datensatz
current_final_prepared <- tc_current %>%
  left_join(tm_current, by = c("participant.code", "comm_raw")) %>%
  mutate(
    comm = case_when(
      comm_raw == "Jitsi" ~ "Together_Jitsi",
      comm_raw == "Chat" ~ "Together_Chat",
      TRUE ~ NA_character_
    ),
    task = "Math"
  ) %>%
  select(participant.code, comm, task, team_composition_value, team_motivation_value)

# Team ID hinzufügen
current_final_prepared <- current_final_prepared %>%
  left_join(
    flow_scores %>% 
      filter(task == "Math") %>%
      select(participant.code, team_id) %>%
      distinct(),
    by = "participant.code"
  )

# ================================================================================
# TEIL 3: DATENSÄTZE KOMBINIEREN
# ================================================================================

# 3.1 Rundenweise Daten kombinieren
combined_rounds <- bind_rows(
  old_rounds_prepared,
  current_rounds_prepared
) %>%
  mutate(
    # Erstelle Faktor mit sinnvoller Reihenfolge
    comm = factor(comm, levels = c("Alone", "Together_None", "Together_Chat", "Together_Jitsi"))
  )

# 3.2 Einmalige Daten kombinieren
combined_final <- bind_rows(
  old_final_prepared,
  current_final_prepared
) %>%
  mutate(
    comm = factor(comm, levels = c("Alone", "Together_None", "Together_Chat", "Together_Jitsi"))
  )

# 3.3 Finale Integration
# Füge einmalige Mediatoren zu den rundenweisen Daten hinzu - KORRIGIERT
integrated_data <- combined_rounds %>%
  left_join(
    combined_final %>% select(participant.code, comm, team_composition_value, team_motivation_value),
    by = c("participant.code", "comm")
  )

# ================================================================================
# TEIL 4: DESKRIPTIVE STATISTIKEN
# ================================================================================

print("=== DESKRIPTIVE STATISTIKEN DES INTEGRIERTEN DATENSATZES ===\n")

# Übersicht über Stichprobengrößen
sample_overview <- integrated_data %>%
  group_by(comm) %>%
  summarise(
    n_participants = n_distinct(participant.code),
    n_teams = n_distinct(team_id),
    n_observations = n(),
    .groups = "drop"
  )

print("Stichprobengrößen nach Kommunikationsbedingung:")
print(sample_overview)

# Flow Scores nach Bedingung
flow_by_comm <- integrated_data %>%
  group_by(comm) %>%
  summarise(
    flow_mean = mean(flow_score, na.rm = TRUE),
    flow_sd = sd(flow_score, na.rm = TRUE),
    .groups = "drop"
  )

print("\nFlow Scores nach Kommunikationsbedingung:")
print(flow_by_comm)

# ================================================================================
# TEIL 5: REPEATED MEASURES KORRELATIONEN
# ================================================================================

print("\n\n=== REPEATED MEASURES CORRELATIONS ===\n")

# Liste der rundenweisen Mediatoren
round_mediators <- c("stress_value", "individual_motivation_value", 
                    "valence_value", "arousal_value", 
                    "information_sharing_value", "synchronization_value")

# Funktion für rmcorr mit mehreren Gruppen
perform_rmcorr_extended <- function(data, mediator_name) {
  results <- list()
  
  for(comm_type in levels(data$comm)) {
    comm_data <- data %>% 
      filter(comm == comm_type) %>%
      filter(!is.na(get(mediator_name)) & !is.na(flow_score))
    
    if(nrow(comm_data) > 10 & n_distinct(comm_data$participant.code) > 3) {
      rmcorr_result <- rmcorr(
        participant = participant.code,
        measure1 = get(mediator_name),
        measure2 = flow_score,
        dataset = comm_data
      )
      
      results[[comm_type]] <- list(
        r = rmcorr_result$r,
        p = rmcorr_result$p,
        df = rmcorr_result$df,
        CI = rmcorr_result$CI,
        n = nrow(comm_data)
      )
    }
  }
  
  return(results)
}

# Durchführung für alle Mediatoren
rmcorr_results <- list()

for(mediator in round_mediators) {
  cat("\n", gsub("_value", "", mediator), ":\n", sep = "")
  results <- perform_rmcorr_extended(integrated_data, mediator)
  rmcorr_results[[mediator]] <- results
  
  for(comm_type in names(results)) {
    if(!is.null(results[[comm_type]])) {
      cat("  ", comm_type, ": r = ", round(results[[comm_type]]$r, 3),
          ", p = ", format.pval(results[[comm_type]]$p, digits = 3),
          ", n = ", results[[comm_type]]$n,
          ", 95% CI [", round(results[[comm_type]]$CI[1], 3), 
          ", ", round(results[[comm_type]]$CI[2], 3), "]\n", sep = "")
    }
  }
}

# ================================================================================
# TEIL 6: LINEAR MIXED MODELS
# ================================================================================

print("\n\n=== LINEAR MIXED MODELS ===\n")

# 6.1 Modelle für rundenweise Mediatoren
print("--- RUNDENWEISE MEDIATOREN ---\n")

for(mediator in round_mediators) {
  mediator_name <- gsub("_value", "", mediator)
  cat("\n", toupper(mediator_name), ":\n", sep = "")
  
  # Modell 1: Kommunikation -> Mediator
  formula1 <- as.formula(paste(mediator, "~ comm + (1|participant.code) + (1|round)"))
  model1 <- lmer(formula1, data = integrated_data)
  
  cat("Kommunikation -> ", mediator_name, ":\n", sep = "")
  print(summary(model1)$coefficients)
  
  # Modell 2: Mediator -> Flow
  formula2 <- as.formula(paste("flow_score ~", mediator, "+ (1|participant.code) + (1|round)"))
  model2 <- lmer(formula2, data = integrated_data)
  
  cat("\n", mediator_name, " -> Flow:\n", sep = "")
  print(summary(model2)$coefficients[2,])
  cat("\n")
}

# 6.2 Modelle für einmalige Mediatoren
print("\n--- EINMALIGE MEDIATOREN ---\n")

# Team Composition
print("\nTEAM COMPOSITION:")
tc_data <- integrated_data %>%
  select(participant.code, comm, team_composition_value, flow_score) %>%
  distinct() %>%
  filter(!is.na(team_composition_value))

model_tc1 <- lm(team_composition_value ~ comm, data = tc_data)
print("Kommunikation -> Team Composition:")
print(summary(model_tc1)$coefficients)

# Aggregiere Flow für Mediator -> Flow Analyse
tc_flow <- integrated_data %>%
  group_by(participant.code, comm) %>%
  summarise(
    mean_flow = mean(flow_score, na.rm = TRUE),
    team_composition_value = first(team_composition_value),
    .groups = "drop"
  ) %>%
  filter(!is.na(team_composition_value))

model_tc2 <- lm(mean_flow ~ team_composition_value, data = tc_flow)
print("\nTeam Composition -> Flow:")
print(summary(model_tc2)$coefficients[2,])

# Team Motivation
print("\n\nTEAM MOTIVATION:")
tm_data <- integrated_data %>%
  select(participant.code, comm, team_motivation_value, flow_score) %>%
  distinct() %>%
  filter(!is.na(team_motivation_value))

model_tm1 <- lm(team_motivation_value ~ comm, data = tm_data)
print("Kommunikation -> Team Motivation:")
print(summary(model_tm1)$coefficients)

tm_flow <- integrated_data %>%
  group_by(participant.code, comm) %>%
  summarise(
    mean_flow = mean(flow_score, na.rm = TRUE),
    team_motivation_value = first(team_motivation_value),
    .groups = "drop"
  ) %>%
  filter(!is.na(team_motivation_value))

model_tm2 <- lm(mean_flow ~ team_motivation_value, data = tm_flow)
print("\nTeam Motivation -> Flow:")
print(summary(model2)$coefficients[2,])

# ================================================================================
# TEIL 7: ZUSAMMENFASSENDE TABELLEN UND VISUALISIERUNGEN
# ================================================================================

# 7.1 Erstelle Übersichtstabelle der Effekte
create_summary_table_extended <- function() {
  # Platzhalter für erweiterte Zusammenfassung
  # Hier könnten die Koeffizienten aus allen Modellen extrahiert werden
  return(NULL)
}

# 7.2 Visualisierungen
print("\n\n=== VISUALISIERUNGEN ===\n")

# Flow nach Kommunikationsbedingung
p1 <- ggplot(integrated_data, aes(x = comm, y = flow_score, fill = comm)) +
  geom_boxplot(alpha = 0.7) +
  scale_fill_manual(values = c("Alone" = "#E69F00", 
                              "Together_None" = "#56B4E9", 
                              "Together_Chat" = "#009E73", 
                              "Together_Jitsi" = "#F0E442")) +
  labs(title = "Flow Scores by Communication Condition",
       x = "Communication Condition",
       y = "Flow Score",
       fill = "Condition") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p1)

# Mediator-Profile nach Bedingung
mediator_profiles <- integrated_data %>%
  group_by(comm) %>%
  summarise(
    Stress = mean(stress_value, na.rm = TRUE),
    `Individual Motivation` = mean(individual_motivation_value, na.rm = TRUE),
    Valence = mean(valence_value, na.rm = TRUE),
    Arousal = mean(arousal_value, na.rm = TRUE),
    `Information Sharing` = mean(information_sharing_value, na.rm = TRUE),
    Synchronization = mean(synchronization_value, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  pivot_longer(cols = -comm, names_to = "Mediator", values_to = "Score")

p2 <- ggplot(mediator_profiles, aes(x = Mediator, y = Score, fill = comm)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = c("Alone" = "#E69F00", 
                              "Together_None" = "#56B4E9", 
                              "Together_Chat" = "#009E73", 
                              "Together_Jitsi" = "#F0E442")) +
  labs(title = "Mediator Profiles by Communication Condition",
       x = "Mediator",
       y = "Mean Score",
       fill = "Condition") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p2)

# ================================================================================
# TEIL 8: POST-HOC ANALYSEN
# ================================================================================

print("\n\n=== POST-HOC ANALYSEN ===\n")

# Vergleiche spezifische Kontraste
print("Geplante Kontraste:")

# Alone vs. alle Together Bedingungen
integrated_data_contrast <- integrated_data %>%
  mutate(
    alone_vs_together = ifelse(comm == "Alone", 0, 1),
    no_comm_vs_comm = case_when(
      comm %in% c("Alone", "Together_None") ~ 0,
      comm %in% c("Together_Chat", "Together_Jitsi") ~ 1
    ),
    chat_vs_jitsi = case_when(
      comm == "Together_Chat" ~ 0,
      comm == "Together_Jitsi" ~ 1,
      TRUE ~ NA_real_
    )
  )

# Kontrast 1: Alone vs. Together
contrast1 <- lmer(flow_score ~ alone_vs_together + (1|participant.code) + (1|round), 
                 data = integrated_data_contrast)
print("\n1. Alone vs. Together (alle):")
print(summary(contrast1)$coefficients)

# Kontrast 2: Keine Kommunikation vs. Kommunikation
contrast2 <- lmer(flow_score ~ no_comm_vs_comm + (1|participant.code) + (1|round), 
                 data = integrated_data_contrast)
print("\n2. Keine Kommunikation vs. Kommunikation:")
print(summary(contrast2)$coefficients)

# Kontrast 3: Chat vs. Jitsi
contrast3 <- lmer(flow_score ~ chat_vs_jitsi + (1|participant.code) + (1|round), 
                 data = integrated_data_contrast %>% 
                   filter(comm %in% c("Together_Chat", "Together_Jitsi")))
print("\n3. Chat vs. Jitsi:")
print(summary(contrast3)$coefficients)

# ================================================================================
# SCHLUSSNOTIZEN
# ================================================================================

print("\n\n=== ANALYSEN ABGESCHLOSSEN ===")
print("1. Datensätze erfolgreich integriert")
print("2. Repeated Measures Korrelationen für alle Bedingungen berechnet")
print("3. Linear Mixed Models für erweiterten Datensatz erstellt")
print("4. Visualisierungen der Unterschiede zwischen allen vier Bedingungen")
print("5. Post-hoc Kontraste zur Untersuchung spezifischer Hypothesen")

# Speichere integrierten Datensatz für weitere Analysen
# write.csv(integrated_data, "integrated_mediation_data.csv", row.names = FALSE)
```


Regression of flow on familiarity scores

```{r}
# --- 1. Korrelationsanalysen für Familiarity-Variablen ----------------------

# Math Jitsi
math_jitsi_fam <- data %>%
  select(starts_with("mathJitsi.6.player.fam")) %>%
  select(contains("lightcoral"), contains("lightgreen"), contains("lightblue"))

cor_math_jitsi <- cor(math_jitsi_fam, use = "pairwise.complete.obs")
print("Correlation Matrix - Math Jitsi:")
print(round(cor_math_jitsi, 3))

# Math Chat
math_chat_fam <- data %>%
  select(starts_with("mathChat.6.player.fam")) %>%
  select(contains("lightcoral"), contains("lightgreen"), contains("lightblue"))

cor_math_chat <- cor(math_chat_fam, use = "pairwise.complete.obs")
print("\nCorrelation Matrix - Math Chat:")
print(round(cor_math_chat, 3))

# HP Jitsi
hp_jitsi_fam <- data %>%
  select(starts_with("HiddenProfile_Jitsi.3.player.fam")) %>%
  select(contains("lightcoral"), contains("lightgreen"), contains("lightblue"))

cor_hp_jitsi <- cor(hp_jitsi_fam, use = "pairwise.complete.obs")
print("\nCorrelation Matrix - HP Jitsi:")
print(round(cor_hp_jitsi, 3))

# HP Chat
hp_chat_fam <- data %>%
  select(starts_with("HiddenProfile_Chat.3.player.fam")) %>%
  select(contains("lightcoral"), contains("lightgreen"), contains("lightblue"))

cor_hp_chat <- cor(hp_chat_fam, use = "pairwise.complete.obs")
print("\nCorrelation Matrix - HP Chat:")
print(round(cor_hp_chat, 3))

# --- 2. Familiarity: Aggregation pro Farbe (fam1 & fam2 mitteln) ----------------------

data <- data %>%
  mutate(
    # Math – Jitsi
    fam_mathJitsi_coral = rowMeans(select(., mathJitsi.6.player.fam1_lightcoral, mathJitsi.6.player.fam2_lightcoral), na.rm = TRUE),
    fam_mathJitsi_green = rowMeans(select(., mathJitsi.6.player.fam1_lightgreen, mathJitsi.6.player.fam2_lightgreen), na.rm = TRUE),
    fam_mathJitsi_blue  = rowMeans(select(., mathJitsi.6.player.fam1_lightblue,  mathJitsi.6.player.fam2_lightblue),  na.rm = TRUE),
    
    # Math – Chat
    fam_mathChat_coral = rowMeans(select(., mathChat.6.player.fam1_lightcoral, mathChat.6.player.fam2_lightcoral), na.rm = TRUE),
    fam_mathChat_green = rowMeans(select(., mathChat.6.player.fam1_lightgreen, mathChat.6.player.fam2_lightgreen), na.rm = TRUE),
    fam_mathChat_blue  = rowMeans(select(., mathChat.6.player.fam1_lightblue,  mathChat.6.player.fam2_lightblue),  na.rm = TRUE),

    # HP – Jitsi
    fam_hpJitsi_coral = rowMeans(select(., HiddenProfile_Jitsi.3.player.fam1_lightcoral, HiddenProfile_Jitsi.3.player.fam2_lightcoral), na.rm = TRUE),
    fam_hpJitsi_green = rowMeans(select(., HiddenProfile_Jitsi.3.player.fam1_lightgreen, HiddenProfile_Jitsi.3.player.fam2_lightgreen), na.rm = TRUE),
    fam_hpJitsi_blue  = rowMeans(select(., HiddenProfile_Jitsi.3.player.fam1_lightblue,  HiddenProfile_Jitsi.3.player.fam2_lightblue),  na.rm = TRUE),

    # HP – Chat
    fam_hpChat_coral = rowMeans(select(., HiddenProfile_Chat.3.player.fam1_lightcoral, HiddenProfile_Chat.3.player.fam2_lightcoral), na.rm = TRUE),
    fam_hpChat_green = rowMeans(select(., HiddenProfile_Chat.3.player.fam1_lightgreen, HiddenProfile_Chat.3.player.fam2_lightgreen), na.rm = TRUE),
    fam_hpChat_blue  = rowMeans(select(., HiddenProfile_Chat.3.player.fam1_lightblue,  HiddenProfile_Chat.3.player.fam2_lightblue),  na.rm = TRUE)
  )

# --- 3. Familiarity: Aggregation pro Bedingung (über die zwei befüllten Farben) -------

data <- data %>%
  mutate(
    fam_mathJitsi = rowMeans(select(., fam_mathJitsi_coral, fam_mathJitsi_green, fam_mathJitsi_blue), na.rm = TRUE),
    fam_mathChat  = rowMeans(select(., fam_mathChat_coral, fam_mathChat_green, fam_mathChat_blue), na.rm = TRUE),
    fam_hpJitsi   = rowMeans(select(., fam_hpJitsi_coral, fam_hpJitsi_green, fam_hpJitsi_blue), na.rm = TRUE),
    fam_hpChat    = rowMeans(select(., fam_hpChat_coral, fam_hpChat_green, fam_hpChat_blue), na.rm = TRUE)
  )

# --- 4. Recognition: Kategorisierung für Analyse ---------------

data <- data %>%
  mutate(
    # Individual recognition scores
    rec_coral = Outro.1.player.rec_lightcoral,
    rec_green = Outro.1.player.rec_lightgreen,
    rec_blue  = Outro.1.player.rec_lightblue
  ) %>%
  mutate(
    # Mean recognition across teammates
    rec_mean = rowMeans(select(., rec_coral, rec_green, rec_blue), na.rm = TRUE),
    
    # Count how many teammates were known (>4 on 7-point scale)
    rec_count = rowSums(select(., rec_coral, rec_green, rec_blue) > 4, na.rm = TRUE),
    
    # Categorical variable
    rec_category = case_when(
      rec_count == 0 ~ "Nobody known",
      rec_count == 1 ~ "One person known",
      rec_count == 2 ~ "Both known",
      TRUE ~ NA_character_
    )
  )

# --- 5. Deskriptive Statistiken ---------------

# Recognition categories
print("\n--- Recognition Categories ---")
table(data$rec_category)

# Familiarity means by condition
print("\n--- Mean Familiarity by Condition ---")
data %>%
  summarise(
    MathJitsi = mean(fam_mathJitsi, na.rm = TRUE),
    MathChat = mean(fam_mathChat, na.rm = TRUE),
    HPJitsi = mean(fam_hpJitsi, na.rm = TRUE),
    HPChat = mean(fam_hpChat, na.rm = TRUE)
  ) %>%
  print()

# --- 6. Long format for regression analyses ---------------

# Da die Flow-Scores in einem separaten Dataframe sind, müssen wir anders vorgehen
# Erst aggregieren wir die Flow-Scores pro Bedingung (über alle Schwierigkeiten)

print("\n--- Aggregating flow scores ---")
flow_aggregated <- flow_scores %>%
  group_by(participant.code, task, comm) %>%
  summarise(
    flow_score = mean(flow_score, na.rm = TRUE),
    n_difficulties = n(),  # Anzahl der Schwierigkeitsstufen
    .groups = 'drop'
  )

print("Sample of aggregated flow scores:")
print(head(flow_aggregated))

# Erstelle das Long-Format für Familiarity
familiarity_long <- data %>%
  select(participant.code,
         fam_mathJitsi, fam_mathChat, fam_hpJitsi, fam_hpChat,
         rec_mean, rec_count, rec_category) %>%
  pivot_longer(
    cols = starts_with("fam_"),
    names_to = "condition",
    values_to = "familiarity"
  ) %>%
  mutate(
    task = case_when(
      str_detect(condition, "math") ~ "Math", 
      TRUE ~ "HP"  # HP wie in flow_scores
    ),
    comm = case_when(
      str_detect(condition, "Chat") ~ "Chat", 
      TRUE ~ "Jitsi"
    )
  )

# Merge mit Flow-Scores
familiarity_long <- familiarity_long %>%
  left_join(
    flow_aggregated %>% select(participant.code, task, comm, flow_score),
    by = c("participant.code", "task", "comm")
  ) %>%
  drop_na(flow_score)

# Check wie viele Zeilen wir haben
print(paste("\nRows in familiarity_long:", nrow(familiarity_long)))
print(paste("Unique participants:", n_distinct(familiarity_long$participant.code)))

# Überprüfe die Verteilung
print("\nDistribution by condition:")
print(table(familiarity_long$task, familiarity_long$comm))

# --- 7. Regression Models ---------------

print("\n--- Model A: Basic (Familiarity + Recognition) ---")
model_a <- lm(flow_score ~ familiarity + rec_mean, data = familiarity_long)
summary(model_a)

print("\n--- Model B: With Task and Communication ---")
model_b <- lm(flow_score ~ familiarity + rec_mean + task + comm, data = familiarity_long)
summary(model_b)

print("\n--- Model C: With Interactions ---")
model_c <- lm(flow_score ~ familiarity * comm + rec_mean * comm + task, data = familiarity_long)
summary(model_c)

print("\n--- Model D: Recognition Categories ---")
model_d <- lm(flow_score ~ familiarity + rec_category + task + comm, data = familiarity_long)
summary(model_d)

print("\n--- Model E: Non-linear Recognition Effect ---")
model_e <- lm(flow_score ~ familiarity + rec_count + I(rec_count^2) + task + comm, data = familiarity_long)
summary(model_e)

# --- 8. Visualizations ---------------

# Plot 1: Familiarity vs Flow by Communication Type
p1 <- ggplot(familiarity_long, aes(x = familiarity, y = flow_score, color = comm)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = TRUE) +
  facet_wrap(~ task) +
  labs(
    title = "Familiarity vs. Flow by Communication Type and Task",
    x = "Familiarity with Teammates (during task)",
    y = "Flow Score",
    color = "Communication"
  ) +
  theme_minimal()

print(p1)

# Plot 2: Recognition Categories and Flow
p2 <- ggplot(familiarity_long, aes(x = rec_category, y = flow_score, fill = rec_category)) +
  geom_boxplot(alpha = 0.7) +
  geom_jitter(width = 0.2, alpha = 0.3) +
  facet_grid(task ~ comm) +
  labs(
    title = "Flow by Prior Recognition of Teammates",
    x = "Prior Recognition",
    y = "Flow Score"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p2)

# Plot 3: Recognition Mean vs Flow
p3 <- ggplot(familiarity_long, aes(x = rec_mean, y = flow_score)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = TRUE) +
  facet_grid(task ~ comm) +
  labs(
    title = "Mean Prior Recognition vs. Flow",
    x = "Mean Recognition Score",
    y = "Flow Score"
  ) +
  theme_minimal()

print(p3)

# --- 9. Additional Analyses ---------------

# Check if recognition effect differs by communication type
print("\n--- Model F: Recognition × Communication Interaction ---")
model_f <- lm(flow_score ~ familiarity + rec_mean * comm + task, data = familiarity_long)
summary(model_f)

# Correlation between familiarity and recognition
print("\n--- Correlation: Familiarity vs Recognition ---")
cor.test(familiarity_long$familiarity, familiarity_long$rec_mean)

# Mean flow by recognition categories
print("\n--- Mean Flow by Recognition Category ---")
familiarity_long %>%
  group_by(rec_category) %>%
  summarise(
    mean_flow = mean(flow_score, na.rm = TRUE),
    sd_flow = sd(flow_score, na.rm = TRUE),
    n = n()
  ) %>%
  print()

```

Regression of flow on gender distribution

```{r}
# --- Team Gender Composition Analysis ---

# 1. Team-ID ist bereits in data vorhanden (erstellt mit session.code + group.custom_group_id)
print("Checking for team_id in data:")
print("team_id" %in% names(data))

# 2. Teamzusammensetzung berechnen
team_gender_composition <- data %>%
  select(participant.code, team_id, gender = Intro.1.player.gender) %>%
  filter(!is.na(gender), !is.na(team_id)) %>%
  group_by(team_id) %>%
  summarise(
    n_members = n(),
    n_male = sum(gender == "Male", na.rm = TRUE),
    n_female = sum(gender == "Female", na.rm = TRUE),
    n_other = sum(!gender %in% c("Male", "Female"), na.rm = TRUE),
    unique_genders = n_distinct(gender),
    .groups = "drop"
  ) %>%
  mutate(
    gender_comp = case_when(
      n_male == n_members ~ "all_male",
      n_female == n_members ~ "all_female",
      n_male > 0 & n_female > 0 ~ "mixed",
      TRUE ~ "other"
    )
  )

# 3. Check der Verteilung
print("\nTeam Gender Composition Distribution:")
table(team_gender_composition$gender_comp)

# 4. Gender composition zu flow_scores hinzufügen
flow_scores_gender <- flow_scores %>%
  left_join(team_gender_composition %>% select(team_id, gender_comp), 
            by = "team_id")

# 5. Visualisierung: Boxplot
p1 <- ggplot(flow_scores_gender, aes(x = gender_comp, y = flow_score, fill = gender_comp)) +
  geom_boxplot(alpha = 0.7) +
  geom_jitter(width = 0.2, alpha = 0.3) +
  labs(
    title = "Flow Scores by Team Gender Composition",
    x = "Team Gender Composition",
    y = "Flow Score",
    fill = "Gender Composition"
  ) +
  theme_minimal() +
  scale_fill_brewer(palette = "Set2")

print(p1)

# 6. Visualisierung nach Task und Communication
p2 <- ggplot(flow_scores_gender, aes(x = gender_comp, y = flow_score, fill = gender_comp)) +
  geom_boxplot(alpha = 0.7) +
  facet_grid(task ~ comm) +
  labs(
    title = "Flow Scores by Gender Composition, Task and Communication",
    x = "Team Gender Composition",
    y = "Flow Score",
    fill = "Gender Composition"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_brewer(palette = "Set2")

print(p2)

# 7. Statistische Modelle

# Basis-Modell
print("\n--- Model 1: Gender Composition Only ---")
model_gender <- lm(flow_score ~ gender_comp, data = flow_scores_gender)
summary(model_gender)

# Mit Task und Communication
print("\n--- Model 2: Gender Comp + Task + Communication ---")
model_gender_task <- lm(flow_score ~ gender_comp + task + comm, data = flow_scores_gender)
summary(model_gender_task)

# Mit Schwierigkeit
print("\n--- Model 3: Gender Comp + Difficulty ---")
model_gender_diff <- lm(flow_score ~ gender_comp + difficulty, data = flow_scores_gender)
summary(model_gender_diff)

# Volles Modell mit Interaktionen
print("\n--- Model 4: Full Model with Interactions ---")
model_gender_full <- lm(flow_score ~ gender_comp * task + comm + difficulty, 
                       data = flow_scores_gender)
summary(model_gender_full)

# 8. Deskriptive Statistiken
print("\n--- Mean Flow by Gender Composition ---")
flow_scores_gender %>%
  group_by(gender_comp) %>%
  summarise(
    mean_flow = mean(flow_score, na.rm = TRUE),
    sd_flow = sd(flow_score, na.rm = TRUE),
    n = n(),
    n_teams = n_distinct(team_id)
  ) %>%
  print()

# 9. ANOVA für Gruppenunterschiede
print("\n--- ANOVA: Gender Composition ---")
anova_gender <- aov(flow_score ~ gender_comp, data = flow_scores_gender)
summary(anova_gender)

# Post-hoc Test falls signifikant
if(summary(anova_gender)[[1]][["Pr(>F)"]][1] < 0.05) {
  print("\n--- Post-hoc Test (Tukey HSD) ---")
  TukeyHSD(anova_gender)
}

# 10. Zusätzlich: Gender composition zu familiarity_long hinzufügen
familiarity_long_gender <- familiarity_long %>%
  left_join(flow_scores %>% select(participant.code, team_id) %>% distinct(), 
            by = "participant.code") %>%
  left_join(team_gender_composition %>% select(team_id, gender_comp), 
            by = "team_id")

# Model mit Familiarity und Gender Composition
print("\n--- Model 5: Familiarity + Gender Composition ---")
model_fam_gender <- lm(flow_score ~ familiarity + rec_mean + gender_comp + task + comm, 
                      data = familiarity_long_gender)
summary(model_fam_gender)

# Visualisierung: Familiarity vs Flow nach Gender Composition
p3 <- ggplot(familiarity_long_gender, aes(x = familiarity, y = flow_score, color = gender_comp)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = TRUE) +
  facet_wrap(~ task) +
  labs(
    title = "Familiarity vs. Flow by Gender Composition",
    x = "Familiarity with Teammates",
    y = "Flow Score",
    color = "Gender Composition"
  ) +
  theme_minimal() +
  scale_color_brewer(palette = "Set2")

print(p3)

# Erweiterte Analyse: Gender Composition Effekte mit Mediatoren und Familiarity

# ================================================================================
# TEIL 1: DATEN VORBEREITEN
# ================================================================================

# Gender Composition zu mediation_data hinzufügen
mediation_data_gender <- mediation_data %>%
  left_join(
    data %>% select(participant.code, team_id) %>% distinct(),
    by = "participant.code"
  ) %>%
  left_join(
    team_gender_composition %>% select(team_id, gender_comp),
    by = "team_id" 
  )

# Familiarity Daten hinzufügen (aggregiert über Farben)
familiarity_summary <- data %>%
  mutate(
    fam_mathJitsi = rowMeans(select(., fam_mathJitsi_coral, fam_mathJitsi_green, fam_mathJitsi_blue), na.rm = TRUE),
    fam_mathChat = rowMeans(select(., fam_mathChat_coral, fam_mathChat_green, fam_mathChat_blue), na.rm = TRUE),
    fam_hpJitsi = rowMeans(select(., fam_hpJitsi_coral, fam_hpJitsi_green, fam_hpJitsi_blue), na.rm = TRUE),
    fam_hpChat = rowMeans(select(., fam_hpChat_coral, fam_hpChat_green, fam_hpChat_blue), na.rm = TRUE)
  ) %>%
  select(participant.code, starts_with("fam_")) %>%
  pivot_longer(
    cols = starts_with("fam_"),
    names_to = "condition",
    values_to = "familiarity"
  ) %>%
  mutate(
    task = case_when(
      str_detect(condition, "math") ~ "Math",
      TRUE ~ "HP"
    ),
    comm = case_when(
      str_detect(condition, "Chat") ~ "Chat",
      TRUE ~ "Jitsi"
    )
  )

# Alles zusammenführen
analysis_data <- mediation_data_gender %>%
  left_join(
    familiarity_summary %>% select(participant.code, task, comm, familiarity),
    by = c("participant.code", "task", "comm")
  ) %>%
  # Recognition hinzufügen
  left_join(
    data %>% select(participant.code, rec_mean, rec_count, rec_category),
    by = "participant.code"
  )

# ================================================================================
# TEIL 2: GENDER COMPOSITION EFFEKT AUFSCHLÜSSELN
# ================================================================================

print("=== GENDER COMPOSITION ANALYSE ===\n")

# 2.1 Deskriptive Statistiken für alle Variablen nach Gender Composition
gender_descriptives <- analysis_data %>%
  group_by(gender_comp) %>%
  summarise(
    n = n(),
    # Flow
    flow_mean = mean(mean_flow_score, na.rm = TRUE),
    flow_sd = sd(mean_flow_score, na.rm = TRUE),
    # Mediatoren
    tc_mean = mean(team_composition_score, na.rm = TRUE),
    is_mean = mean(information_sharing_score, na.rm = TRUE),
    es_mean = mean(emotional_synchrony_score, na.rm = TRUE),
    # Familiarity
    fam_mean = mean(familiarity, na.rm = TRUE),
    rec_mean_avg = mean(rec_mean, na.rm = TRUE),
    .groups = "drop"
  )

print("Deskriptive Statistiken nach Gender Composition:")
print(gender_descriptives)

# 2.2 Unterschiede zwischen Tasks
gender_task_descriptives <- analysis_data %>%
  group_by(gender_comp, task) %>%
  summarise(
    n = n(),
    flow_mean = mean(mean_flow_score, na.rm = TRUE),
    flow_sd = sd(mean_flow_score, na.rm = TRUE),
    .groups = "drop"
  )

print("\nFlow nach Gender Composition und Task:")
print(gender_task_descriptives)

# ================================================================================
# TEIL 3: MEDIATIONSANALYSE - WARUM HABEN ALL-MALE TEAMS HÖHEREN FLOW?
# ================================================================================

print("\n=== MEDIATIONSANALYSE: GENDER COMPOSITION -> MEDIATOREN -> FLOW ===\n")

# 3.1 Prüfe ob Gender Composition die Mediatoren beeinflusst
mediator_models <- list(
  tc = lm(team_composition_score ~ gender_comp, data = analysis_data),
  is = lm(information_sharing_score ~ gender_comp, data = analysis_data),
  es = lm(emotional_synchrony_score ~ gender_comp, data = analysis_data),
  fam = lm(familiarity ~ gender_comp, data = analysis_data)
)

print("Gender Composition -> Mediatoren:")
for(mediator in names(mediator_models)) {
  print(paste("\n", mediator, ":"))
  print(summary(mediator_models[[mediator]])$coefficients)
}

# 3.2 Schrittweise Modelle mit Mediatoren
model_base <- lm(mean_flow_score ~ gender_comp, data = analysis_data)
model_tc <- lm(mean_flow_score ~ gender_comp + team_composition_score, data = analysis_data)
model_is <- lm(mean_flow_score ~ gender_comp + information_sharing_score, data = analysis_data)
model_es <- lm(mean_flow_score ~ gender_comp + emotional_synchrony_score, data = analysis_data)
model_fam <- lm(mean_flow_score ~ gender_comp + familiarity + rec_mean, data = analysis_data)
model_full <- lm(mean_flow_score ~ gender_comp + team_composition_score + 
                 information_sharing_score + emotional_synchrony_score + 
                 familiarity + rec_mean, data = analysis_data)

print("\nModellvergleich - Wie verändert sich der Gender Effect?")
print("Base Model (nur Gender):")
print(summary(model_base)$coefficients)
print("\nMit allen Mediatoren:")
print(summary(model_full)$coefficients)

# ================================================================================
# TEIL 4: INTERAKTIONSEFFEKTE
# ================================================================================

print("\n=== INTERAKTIONSANALYSEN ===\n")

# 4.1 Gender × Task Interaktion
model_gender_task <- lm(mean_flow_score ~ gender_comp * task, data = analysis_data)
print("Gender × Task Interaktion:")
print(summary(model_gender_task))

# 4.2 Gender × Communication Interaktion
model_gender_comm <- lm(mean_flow_score ~ gender_comp * comm, data = analysis_data)
print("\nGender × Communication Interaktion:")
print(summary(model_gender_comm))

# 4.3 Gender × Familiarity Interaktion
model_gender_fam <- lm(mean_flow_score ~ gender_comp * familiarity, data = analysis_data)
print("\nGender × Familiarity Interaktion:")
print(summary(model_gender_fam))

# ================================================================================
# TEIL 5: TASK-SPEZIFISCHE ANALYSEN
# ================================================================================

print("\n=== TASK-SPEZIFISCHE GENDER EFFEKTE ===\n")

# Math Task
math_gender_data <- analysis_data %>% filter(task == "Math")
math_gender_model <- lm(mean_flow_score ~ gender_comp + team_composition_score + 
                       information_sharing_score + emotional_synchrony_score + 
                       familiarity, data = math_gender_data)
print("MATH TASK - Gender Effect mit Mediatoren:")
print(summary(math_gender_model))

# HP Task
hp_gender_data <- analysis_data %>% filter(task == "HP")
hp_gender_model <- lm(mean_flow_score ~ gender_comp + team_composition_score + 
                     information_sharing_score + emotional_synchrony_score + 
                     familiarity, data = hp_gender_data)
print("\nHP TASK - Gender Effect mit Mediatoren:")
print(summary(hp_gender_model))

# ================================================================================
# TEIL 6: VISUALISIERUNGEN
# ================================================================================

# 6.1 Mediator-Profile nach Gender Composition
mediator_profile <- analysis_data %>%
  group_by(gender_comp) %>%
  summarise(
    `Team Composition` = mean(team_composition_score, na.rm = TRUE),
    `Information Sharing` = mean(information_sharing_score, na.rm = TRUE),
    `Emotional Synchrony` = mean(emotional_synchrony_score, na.rm = TRUE),
    `Familiarity` = mean(familiarity, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  pivot_longer(cols = -gender_comp, names_to = "Mediator", values_to = "Score")

p_profile <- ggplot(mediator_profile, aes(x = Mediator, y = Score, fill = gender_comp)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Mediator Profile by Gender Composition",
    x = "Mediator",
    y = "Mean Score",
    fill = "Gender Composition"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p_profile)

# 6.2 Flow-Mediator Beziehungen nach Gender
p_mediators <- analysis_data %>%
  pivot_longer(
    cols = c(team_composition_score, information_sharing_score, 
             emotional_synchrony_score, familiarity),
    names_to = "mediator",
    values_to = "mediator_value"
  ) %>%
  mutate(
    mediator = case_when(
      mediator == "team_composition_score" ~ "Team Composition",
      mediator == "information_sharing_score" ~ "Information Sharing",
      mediator == "emotional_synchrony_score" ~ "Emotional Synchrony",
      mediator == "familiarity" ~ "Familiarity"
    )
  ) %>%
  ggplot(aes(x = mediator_value, y = mean_flow_score, color = gender_comp)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~ mediator, scales = "free_x") +
  labs(
    title = "Flow-Mediator Relationships by Gender Composition",
    x = "Mediator Score",
    y = "Flow Score",
    color = "Gender Composition"
  ) +
  theme_minimal()

print(p_mediators)

# ================================================================================
# TEIL 7: PFADANALYSE ZUSAMMENFASSUNG
# ================================================================================

print("\n=== PFADANALYSE ZUSAMMENFASSUNG ===\n")

# Berechne indirekte Effekte für all-male vs. mixed teams
for(mediator in c("team_composition_score", "information_sharing_score", 
                  "emotional_synchrony_score", "familiarity")) {
  
  # a-Pfad: Gender -> Mediator
  a_model <- lm(as.formula(paste(mediator, "~ gender_comp")), data = analysis_data)
  a_coef <- coef(a_model)["gender_compmixed"]
  
  # b-Pfad: Mediator -> Flow (kontrolliert für Gender)
  b_model <- lm(as.formula(paste("mean_flow_score ~ gender_comp +", mediator)), 
                data = analysis_data)
  b_coef <- coef(b_model)[mediator]
  
  # Indirekter Effekt
  indirect <- a_coef * b_coef
  
  print(paste(mediator, ":"))
  print(paste("  a-Pfad (all-male -> mixed):", round(a_coef, 3)))
  print(paste("  b-Pfad (Mediator -> Flow):", round(b_coef, 3)))
  print(paste("  Indirekter Effekt:", round(indirect, 3)))
  print("")
}

# ================================================================================
# TEIL 8: EMPFEHLUNGEN FÜR WEITERE ANALYSEN
# ================================================================================

print("\n=== EMPFEHLUNGEN ===\n")
print("1. Team Composition Score zeigt möglicherweise Unterschiede - all-male Teams")
print("   könnten sich als besser aufeinander abgestimmt wahrnehmen")
print("2. Emotional Synchrony könnte in homogenen Teams höher sein")
print("3. Prüfe ob der Effekt taskspezifisch ist (Math vs. HP)")
print("4. Untersuche ob Familiarity den Gender-Effekt moderiert")
print("5. Betrachte individuelle Gender-Effekte (männliche vs. weibliche Teilnehmer)")

# Zusätzlich: Prüfe individuelle Geschlechtseffekte
individual_gender <- data %>%
  select(participant.code, individual_gender = Intro.1.player.gender) %>%
  right_join(analysis_data, by = "participant.code")

model_individual <- lm(mean_flow_score ~ individual_gender + gender_comp, 
                      data = individual_gender)
print("\nIndividuelles Geschlecht + Team Composition:")
print(summary(model_individual))

```

Regression of flow on emoji count

```{r}
# Emoji-Analyse in Chat-Logs und Zusammenhang mit Flow

library(stringr)

# ================================================================================
# TEIL 1: EMOJI-EXTRAKTION AUS CHAT-LOGS
# ================================================================================

# Definiere die Text-Emojis, die wir suchen
emoji_patterns <- c(
  # Positive Emojis
  ":\\)", ";\\)", ":D", ";D", "[xX]D", "XD", "<3", ":P", ":p",
  # Negative/Neutrale Emojis  
  ":/", ":\\(", ":\\|", ":o", ":O",
  # Varianten mit Bindestrich (falls vorhanden)
  ":-\\)", ";-\\)", ":-D", ";-D", ":-/", ":-\\(", ":-\\|", ":-[oO]", ":-[pP]"
)

# Erstelle ein einzelnes Pattern für alle Emojis
emoji_regex <- paste0("(", paste(emoji_patterns, collapse = "|"), ")")

# Funktion zum Zählen von Emojis in einem Text
count_emojis <- function(text) {
  if(is.na(text) || text == "") return(0)
  matches <- str_extract_all(text, emoji_regex)[[1]]
  return(length(matches))
}

# Funktion zum Extrahieren spezifischer Emoji-Typen
extract_emoji_types <- function(text) {
  if(is.na(text) || text == "") {
    return(list(
      positive = 0,
      negative = 0,
      neutral = 0,
      total = 0
    ))
  }
  
  # Positive Emojis
  positive_pattern <- "(:\\)|;\\)|:D|;D|[xX]D|XD|<3|:P|:p|:-\\)|;-\\)|:-D|;-D|:-[pP])"
  positive_count <- length(str_extract_all(text, positive_pattern)[[1]])
  
  # Negative Emojis
  negative_pattern <- "(:\\(|:-\\()"
  negative_count <- length(str_extract_all(text, negative_pattern)[[1]])
  
  # Neutrale Emojis
  neutral_pattern <- "(:/|:\\||:o|:O|:-/|:-\\||:-[oO])"
  neutral_count <- length(str_extract_all(text, neutral_pattern)[[1]])
  
  return(list(
    positive = positive_count,
    negative = negative_count,
    neutral = neutral_count,
    total = positive_count + negative_count + neutral_count
  ))
}

# ================================================================================
# TEIL 2: CHAT-LOG DATEN EXTRAHIEREN UND ANALYSIEREN
# ================================================================================

# Extrahiere Math Chat Logs
math_chat_logs <- data %>%
  select(participant.code, team_id, starts_with("mathChat.") & ends_with(".player.chat_log"))

# Zähle Emojis für jede Runde
math_emoji_counts <- data.frame()

for(round in 1:6) {
  col_name <- paste0("mathChat.", round, ".player.chat_log")
  
  if(col_name %in% names(math_chat_logs)) {
    round_data <- math_chat_logs %>%
      select(participant.code, team_id, chat_log = all_of(col_name)) %>%
      mutate(
        round = round,
        emoji_data = map(chat_log, extract_emoji_types)
      ) %>%
      mutate(
        emoji_total = map_dbl(emoji_data, ~ .x$total),
        emoji_positive = map_dbl(emoji_data, ~ .x$positive),
        emoji_negative = map_dbl(emoji_data, ~ .x$negative),
        emoji_neutral = map_dbl(emoji_data, ~ .x$neutral),
        chat_length = nchar(chat_log)
      ) %>%
      select(-emoji_data)
    
    math_emoji_counts <- bind_rows(math_emoji_counts, round_data)
  }
}

# Aggregiere über alle Runden pro Teilnehmer
math_emoji_summary <- math_emoji_counts %>%
  group_by(participant.code, team_id) %>%
  summarise(
    total_emojis = sum(emoji_total, na.rm = TRUE),
    positive_emojis = sum(emoji_positive, na.rm = TRUE),
    negative_emojis = sum(emoji_negative, na.rm = TRUE),
    neutral_emojis = sum(emoji_neutral, na.rm = TRUE),
    total_chat_length = sum(chat_length, na.rm = TRUE),
    emoji_density = ifelse(total_chat_length > 0, total_emojis / total_chat_length * 100, 0),
    task = "Math",
    comm = "Chat",
    .groups = "drop"
  )

# Dasselbe für Hidden Profile Chat Logs
hp_chat_logs <- data %>%
  select(participant.code, team_id, starts_with("HiddenProfile_Chat.") & ends_with(".player.chat_log"))

hp_emoji_counts <- data.frame()

for(round in 1:3) {
  col_name <- paste0("HiddenProfile_Chat.", round, ".player.chat_log")
  
  if(col_name %in% names(hp_chat_logs)) {
    round_data <- hp_chat_logs %>%
      select(participant.code, team_id, chat_log = all_of(col_name)) %>%
      mutate(
        round = round,
        emoji_data = map(chat_log, extract_emoji_types)
      ) %>%
      mutate(
        emoji_total = map_dbl(emoji_data, ~ .x$total),
        emoji_positive = map_dbl(emoji_data, ~ .x$positive),
        emoji_negative = map_dbl(emoji_data, ~ .x$negative),
        emoji_neutral = map_dbl(emoji_data, ~ .x$neutral),
        chat_length = nchar(chat_log)
      ) %>%
      select(-emoji_data)
    
    hp_emoji_counts <- bind_rows(hp_emoji_counts, round_data)
  }
}

# Aggregiere HP Emojis
hp_emoji_summary <- hp_emoji_counts %>%
  group_by(participant.code, team_id) %>%
  summarise(
    total_emojis = sum(emoji_total, na.rm = TRUE),
    positive_emojis = sum(emoji_positive, na.rm = TRUE),
    negative_emojis = sum(emoji_negative, na.rm = TRUE),
    neutral_emojis = sum(emoji_neutral, na.rm = TRUE),
    total_chat_length = sum(chat_length, na.rm = TRUE),
    emoji_density = ifelse(total_chat_length > 0, total_emojis / total_chat_length * 100, 0),
    task = "HP",
    comm = "Chat",
    .groups = "drop"
  )

# Kombiniere Math und HP Emoji-Daten
all_emoji_summary <- bind_rows(math_emoji_summary, hp_emoji_summary)

# ================================================================================
# TEIL 3: TEAM-LEVEL EMOJI ANALYSE
# ================================================================================

# Aggregiere auf Team-Ebene
team_emoji_summary <- all_emoji_summary %>%
  group_by(team_id, task) %>%
  summarise(
    team_total_emojis = sum(total_emojis),
    team_positive_emojis = sum(positive_emojis),
    team_negative_emojis = sum(negative_emojis),
    team_emoji_density = mean(emoji_density),
    emoji_users = sum(total_emojis > 0),
    team_size = n(),
    emoji_user_ratio = emoji_users / team_size,
    .groups = "drop"
  )

# ================================================================================
# TEIL 4: EMOJI-DATEN MIT FLOW UND MEDIATOREN VERBINDEN
# ================================================================================

# Verbinde mit Flow-Scores
emoji_flow_data <- all_emoji_summary %>%
  left_join(
    flow_scores %>%
      filter(comm == "Chat") %>%
      group_by(participant.code, task) %>%
      summarise(mean_flow_score = mean(flow_score, na.rm = TRUE), .groups = "drop"),
    by = c("participant.code", "task")
  )

# Verbinde mit Mediatoren (aus mediation_data)
emoji_mediation_data <- emoji_flow_data %>%
  left_join(
    mediation_data %>%
      filter(comm == "Chat") %>%
      select(participant.code, task, team_composition_score, 
             information_sharing_score, emotional_synchrony_score),
    by = c("participant.code", "task")
  )

# ================================================================================
# TEIL 5: STATISTISCHE ANALYSEN
# ================================================================================

print("=== EMOJI-NUTZUNG DESKRIPTIVE STATISTIKEN ===\n")

# Gesamtübersicht
emoji_overview <- all_emoji_summary %>%
  group_by(task) %>%
  summarise(
    n_participants = n(),
    emoji_users = sum(total_emojis > 0),
    emoji_user_percentage = emoji_users / n_participants * 100,
    mean_emojis = mean(total_emojis),
    sd_emojis = sd(total_emojis),
    mean_positive = mean(positive_emojis),
    mean_negative = mean(negative_emojis),
    mean_neutral = mean(neutral_emojis),
    .groups = "drop"
  )

print("Emoji-Nutzung nach Task:")
print(emoji_overview)

# ================================================================================
# TEIL 6: EMOJI-FLOW ZUSAMMENHÄNGE
# ================================================================================

print("\n=== EMOJI-FLOW ZUSAMMENHÄNGE ===\n")

# Individuelle Ebene
emoji_flow_models <- list(
  math_total = lm(mean_flow_score ~ total_emojis, 
                  data = emoji_mediation_data %>% filter(task == "Math")),
  hp_total = lm(mean_flow_score ~ total_emojis, 
                data = emoji_mediation_data %>% filter(task == "HP")),
  math_types = lm(mean_flow_score ~ positive_emojis + negative_emojis + neutral_emojis, 
                  data = emoji_mediation_data %>% filter(task == "Math")),
  hp_types = lm(mean_flow_score ~ positive_emojis + negative_emojis + neutral_emojis, 
                data = emoji_mediation_data %>% filter(task == "HP"))
)

print("Math Task - Emoji Total -> Flow:")
print(summary(emoji_flow_models$math_total))

print("\nHP Task - Emoji Total -> Flow:")
print(summary(emoji_flow_models$hp_total))

print("\nMath Task - Emoji Types -> Flow:")
print(summary(emoji_flow_models$math_types))

# Team-Ebene Analyse
team_flow_data <- team_emoji_summary %>%
  left_join(
    emoji_flow_data %>%
      group_by(team_id, task) %>%
      summarise(team_mean_flow = mean(mean_flow_score, na.rm = TRUE), .groups = "drop"),
    by = c("team_id", "task")
  )

team_emoji_model <- lm(team_mean_flow ~ team_emoji_density + emoji_user_ratio + task, 
                      data = team_flow_data)
print("\nTeam-Level: Emoji Density & User Ratio -> Flow:")
print(summary(team_emoji_model))

# ================================================================================
# TEIL 7: MEDIATIONSANALYSE - EMOJIS -> EMOTIONAL SYNCHRONY -> FLOW
# ================================================================================

print("\n=== MEDIATIONSANALYSE: EMOJIS -> EMOTIONAL SYNCHRONY -> FLOW ===\n")

# Pfad a: Emojis -> Emotional Synchrony
path_a_model <- lm(emotional_synchrony_score ~ total_emojis, data = emoji_mediation_data)
print("Pfad a (Emojis -> Emotional Synchrony):")
print(summary(path_a_model))

# Pfad c: Emojis -> Flow (Total Effect)
path_c_model <- lm(mean_flow_score ~ total_emojis, data = emoji_mediation_data)
print("\nPfad c (Emojis -> Flow, Total Effect):")
print(summary(path_c_model))

# Pfad c': Emojis -> Flow (kontrolliert für Emotional Synchrony)
path_c_prime_model <- lm(mean_flow_score ~ total_emojis + emotional_synchrony_score, 
                         data = emoji_mediation_data)
print("\nPfad c' (Emojis -> Flow, kontrolliert für ES):")
print(summary(path_c_prime_model))

# Berechne indirekten Effekt
a_coef <- coef(path_a_model)["total_emojis"]
b_coef <- coef(path_c_prime_model)["emotional_synchrony_score"]
indirect_effect <- a_coef * b_coef

print(paste("\nIndirekter Effekt (a × b):", round(indirect_effect, 4)))

# ================================================================================
# TEIL 8: VISUALISIERUNGEN
# ================================================================================

# 8.1 Emoji-Nutzung vs Flow
p1 <- ggplot(emoji_flow_data, aes(x = total_emojis, y = mean_flow_score)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = TRUE) +
  facet_wrap(~ task) +
  labs(
    title = "Emoji Usage vs. Flow Score",
    x = "Total Emojis Used",
    y = "Mean Flow Score"
  ) +
  theme_minimal()

print(p1)

# 8.2 Emoji-Typen Verteilung
emoji_type_data <- emoji_mediation_data %>%
  select(participant.code, task, positive_emojis, negative_emojis, neutral_emojis) %>%
  pivot_longer(cols = c(positive_emojis, negative_emojis, neutral_emojis),
               names_to = "emoji_type", values_to = "count") %>%
  mutate(emoji_type = gsub("_emojis", "", emoji_type))

p2 <- ggplot(emoji_type_data %>% filter(count > 0), 
             aes(x = emoji_type, y = count, fill = emoji_type)) +
  geom_boxplot() +
  facet_wrap(~ task) +
  labs(
    title = "Distribution of Emoji Types",
    x = "Emoji Type",
    y = "Count"
  ) +
  theme_minimal() +
  scale_fill_brewer(palette = "Set2")

print(p2)

# 8.3 Team Emoji Density vs Team Flow
p3 <- ggplot(team_flow_data, aes(x = team_emoji_density, y = team_mean_flow)) +
  geom_point(aes(size = emoji_user_ratio), alpha = 0.7) +
  geom_smooth(method = "lm", se = TRUE) +
  facet_wrap(~ task) +
  labs(
    title = "Team Emoji Density vs. Team Flow",
    x = "Team Emoji Density (%)",
    y = "Team Mean Flow Score",
    size = "Emoji User Ratio"
  ) +
  theme_minimal()

print(p3)

# 8.4 Mediationsvisualisierung
p4 <- ggplot(emoji_mediation_data, aes(x = total_emojis, y = emotional_synchrony_score)) +
  geom_point(aes(color = mean_flow_score), alpha = 0.6) +
  geom_smooth(method = "lm", se = TRUE) +
  scale_color_gradient(low = "blue", high = "red") +
  labs(
    title = "Emojis -> Emotional Synchrony (colored by Flow)",
    x = "Total Emojis",
    y = "Emotional Synchrony Score",
    color = "Flow Score"
  ) +
  theme_minimal()

print(p4)

# ================================================================================
# TEIL 9: BEISPIEL-CHATLOGS MIT VIELEN EMOJIS
# ================================================================================

print("\n=== BEISPIELE FÜR EMOJI-REICHE CHATS ===\n")

# Finde Top 5 emoji-reichste Nachrichten
top_emoji_examples <- bind_rows(
  math_emoji_counts %>% mutate(task = "Math"),
  hp_emoji_counts %>% mutate(task = "HP")
) %>%
  filter(emoji_total > 0) %>%
  arrange(desc(emoji_total)) %>%
  slice_head(n = 5) %>%
  select(participant.code, task, round, emoji_total, chat_log)

print("Top 5 Nachrichten mit den meisten Emojis:")
for(i in 1:nrow(top_emoji_examples)) {
  cat(paste0("\n", i, ". Participant: ", top_emoji_examples$participant.code[i], 
             " (", top_emoji_examples$task[i], " Round ", top_emoji_examples$round[i], 
             ", ", top_emoji_examples$emoji_total[i], " emojis)\n"))
  cat(paste0("   \"", substr(top_emoji_examples$chat_log[i], 1, 100), "...\"\n"))
}

# ================================================================================
# TEIL 10: ZUSAMMENFASSUNG
# ================================================================================

print("\n=== ZUSAMMENFASSUNG DER EMOJI-ANALYSE ===\n")

# Berechne Korrelationen
cor_emoji_flow <- cor(emoji_flow_data$total_emojis, emoji_flow_data$mean_flow_score, 
                     use = "complete.obs")
cor_emoji_es <- cor(emoji_mediation_data$total_emojis, 
                   emoji_mediation_data$emotional_synchrony_score, 
                   use = "complete.obs")

print(paste("Korrelation Emojis-Flow:", round(cor_emoji_flow, 3)))
print(paste("Korrelation Emojis-Emotional Synchrony:", round(cor_emoji_es, 3)))
print(paste("Prozent der Teilnehmer die Emojis nutzen:", 
            round(sum(all_emoji_summary$total_emojis > 0) / nrow(all_emoji_summary) * 100, 1), "%"))

```



